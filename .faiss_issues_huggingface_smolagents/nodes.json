[{"id_": "f9e7b97b-babe-4145-a1bc-72c7b18e4d1f", "embedding": null, "metadata": {"issue_id": 1404, "title": "Exclude thinking generation from python parsing", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "faf72a6b-9342-41fb-b8dc-21e4c6cae3f3", "node_type": "4", "metadata": {"issue_id": 1404, "title": "Exclude thinking generation from python parsing", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "c320d81745753105a618badc7d346fecbf50f9f849e8063425698fa7ec8111f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Exclude thinking generation from python parsing\n\nDescription: Hey, **Is your feature request related to a problem? Please describe.** Sometimes, my LLM steps ends with final_answer(...) not being included in the required \\`py \\ code block syntax, so smolagents raises an error and starts a new step telling the LLM that the syntax was wrong. The problem comes from the fact that I was using a reasoning LLM (qwen 14b in my case), which first reflects on the problem by generating its reasoning between <think> </think> tags, at some point, it plans to use the required syntax by generating it in its thoughts, but without code in it. Step 1 not including the python block syntax : [CODE_BLOCK] Step 2 thinking about adding it but stopped because the python code block syntax got detected by the parser : [CODE_BLOCK]py and end with Error in code parsing: Your code snippet is invalid, because the regex pattern [CODE_BLOCK] was not found in it. ` Here is the full example for more clarity : https://app.warp.dev/block/embed/hUU6K5aEKJKoJZJMpKuPcR **Describe the solution you'd like** Prevent parsing code generation if a <think> tag appeared without a closing </think> tag Adding a new rule in fix_final_answer_code` may solve the problem. I am willing to contribute if you want.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "012f7f49-4e2c-4cb9-9814-b6a48caa326c", "embedding": null, "metadata": {"issue_id": 1401, "title": "[BUG] OpenRouter usage has different schema to OpenAI's usage, causing attribute error.", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48550d13-b595-4d0e-95b1-58a55a834e61", "node_type": "4", "metadata": {"issue_id": 1401, "title": "[BUG] OpenRouter usage has different schema to OpenAI's usage, causing attribute error.", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "803c4e223b8453bb6d0ca051eb1b2214294f7e65b75bf51a1f9bb5791847afbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] OpenRouter usage has different schema to OpenAI's usage, causing attribute error.\n\nDescription: **Describe the bug** Sometimes when using the openrouter API endpoint with the OpenAIServerModel you will get the following error: [CODE_BLOCK] This is because openrouter doesn't guarantee the usage key. Here is the openrouter API page: https://openrouter.ai/docs/api-reference/overviewcompletionsresponse-format Here is the openapi page: https://platform.openai.com/docs/api-reference/chat/object **Code to reproduce the error** Use the openrouter API endpoint with the OpenAIServerModel, occasionally this error will materialize. **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Defaults to 0 when response.usage is None. **Packages version:** smolagents 1.17.0 **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 930, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "00a37729-4735-46e6-9bd7-8b902cc5fce1", "embedding": null, "metadata": {"issue_id": 1396, "title": "[BUG] Gradio render raw json when use_structured_outputs_internally", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d087656-12df-4011-ab43-4484d78ce17d", "node_type": "4", "metadata": {"issue_id": 1396, "title": "[BUG] Gradio render raw json when use_structured_outputs_internally", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "66ec9c8e820e463bfaf354d93d4f6c894a34bbd5a2a481cdc0416b65e2cf76a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Gradio render raw json when use_structured_outputs_internally\n\nDescription: **Describe the bug** When use_structured_outputs_internally=True, gradio will directly display raw json like [CODE_BLOCK] **Code to reproduce the error** The simplest code snippet that produces your bug. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** Should we convert the raw json to a more readable format in gradio ? Like the original Thought: xxxxx Code: [CODE_BLOCK] **Packages version:** 1.17.0 **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 673, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d6fb9f9c-5754-4e83-9165-3fba6b364cd5", "embedding": null, "metadata": {"issue_id": 1395, "title": "Add Brave search engine to smolagents deafult_tools [Free 2,000 search queries/month]", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8157ab15-8551-4b3e-93f2-ad47c554d481", "node_type": "4", "metadata": {"issue_id": 1395, "title": "Add Brave search engine to smolagents deafult_tools [Free 2,000 search queries/month]", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "506130996e1a2e853955cbf9920502022d72597cbcd9f3477f860e8981e39051", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add Brave search engine to smolagents deafult_tools [Free 2,000 search queries/month]\n\nDescription: **Is your feature request related to a problem? Please describe.** Bing search engine is going to be removed as Microsoft announced the sunset of the Bing API in May 2025 https://github.com/huggingface/smolagents/pull/1313issuecomment-2885878369 cc @albertvillanova and @aymeric-roucher **Describe the solution you'd like** Adding Brave Search engine as another default tool. Barve Search API offers free 2,000 queries/month, and it is now the only independent and openly available search API at scale. **Is this not possible with the current options.** The current options for web search are Google, Bing and DDG Search engines. It misses the Brave Search engine. **Describe alternatives you've considered** Google and DDG Search engines **Additional context** Brave Search Engine can be added to the default_tools as [CODE_BLOCK] Brave Search API key can be created by: 1) creating an account on https://brave.com/search/api/; 2) subscribing to a plan. Anyone can get started for FREE for 1 query/second, 2,000 queries/month.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a9dc9fc-95ec-4307-be06-938379b2452e", "embedding": null, "metadata": {"issue_id": 1391, "title": "[BUG] When model API provider doesn't support `tool_choice=\"required\"`, OpenAIServerModel breaks at requesting response", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f257236-5670-4778-a7ea-8b11209e6a84", "node_type": "4", "metadata": {"issue_id": 1391, "title": "[BUG] When model API provider doesn't support `tool_choice=\"required\"`, OpenAIServerModel breaks at requesting response", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "a87ce818190661a8e8d01b08285d9d0b77a2b9c6b0712e5eff25a6cd23dfad1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] When model API provider doesn't support `tool_choice=\"required\"`, OpenAIServerModel breaks at requesting response\n\nDescription: **Describe the bug** OpenAIServerModel can use any 3rd party model provider with an OpenAI-compatible endpoint. In the Model._prepare_completion_kwargs method, tool_choice is hard coded as required. However, some model providers doesn't support this choice, it's either 'auto', tool name or 'none'. The tool_choice can be potentially modified through **kwargs, however, there is no way to pass tool_choice as part of the argument. I see this as a bug rather than a feature request because this is effectively blocking the use of 3rd party provider's model completely. For now, what I could do is that I can extend OpenAIServerModel and write a special _prepare_completion_kwargs myself. I hope that the agent can somehow expose completion keyward arguments and we can manage them more precisely. **Code to reproduce the error** The simplest code snippet that produces your bug. I'm using Novita as the API provider, they do not support 'required' option for 'tool_choice' **Error logs (if any)** Provide error logs if there are any. Part of the error log is here: [CODE_BLOCK] **Packages version:** Run pip freeze | grep smolagents and paste it here. [CODE_BLOCK] **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1454, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd6fc6b8-ea86-401b-a1ac-a0a384938767", "embedding": null, "metadata": {"issue_id": 1386, "title": "WebSearchTool example from Guide Tour does not work", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73f56487-9a03-43d6-affa-b94b0e8bf886", "node_type": "4", "metadata": {"issue_id": 1386, "title": "WebSearchTool example from Guide Tour does not work", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "6c038b6d4331f971865d1858efaaebb900a9cb184800c6dca96cc3f708676be6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: WebSearchTool example from Guide Tour does not work\n\nDescription: **Describe the bug** The example about web search from the Guided Tour does not work. I have internet access. **Code to reproduce the error** > from smolagents import WebSearchTool > search_tool = WebSearchTool() > print(search_tool(\"Who is the president of Russia?\")) **Error logs (if any)** > Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"env_home/lib/python3.10/site-packages/smolagents/tools.py\", line 205, in __call__ outputs = self.forward(*args, **kwargs) File \"env_home/lib/python3.10/site-packages/smolagents/default_tools.py\", line 227, in forward raise Exception(\"No results found! Try a less restrictive/shorter query.\") Exception: No results found! Try a less restrictive/shorter query. **Packages version:** smolagents==1.16.1\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 925, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd4eb182-0d33-4448-bf14-0ec1e4798c6e", "embedding": null, "metadata": {"issue_id": 1385, "title": "Allow Agent to remove messages from memory", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d336779-d95c-45fd-813c-9306496f4104", "node_type": "4", "metadata": {"issue_id": 1385, "title": "Allow Agent to remove messages from memory", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "5c7be635de478678a04f9e9ee763fd709f124a525f9dde075fcf6162205e56d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Allow Agent to remove messages from memory\n\nDescription: First of all: thanks for the awesome work. Smolagent's simple yet powerful codebase is my best source of learning about AI agents right now! **Is your feature request related to a problem? Please describe.** I\u2019m using Smolagents to build a knowledge bot that integrates with various company tools and APIs. To enable broad question coverage, I provide the agent with generic resources such as API documentation and a general-purpose HTTP request tool. While this flexible setup usually works well, there are situations where the agent veers off course - calling irrelevant API endpoints, for example. Typically, the agent recognizes these mistakes after a few steps and backtracks, which is helpful. However, the problem is that all the previous (and now irrelevant) messages and responses remain in the context window. With context windows reaching up to 200k tokens, this not only increases costs but can also degrade the agent\u2019s performance in subsequent steps. **Describe the solution you'd like** I'd like to give the agent the option to optimize its memory messages by being able to remove or compress old messages if necessary. **Is this not possible with the current options.** I don't think so, at least I can't find any option. **Describe alternatives you've considered** Compressing old message memory could also help in this situation. **Additional context**\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8923d410-ccd4-47bb-944b-eb3f76db742b", "embedding": null, "metadata": {"issue_id": 1382, "title": "[BUG] Custom FinalAnswerTool with multiple inputs raises unexpected keyword argument error", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86525b90-a5e9-44d2-8489-b5397b4f2046", "node_type": "4", "metadata": {"issue_id": 1382, "title": "[BUG] Custom FinalAnswerTool with multiple inputs raises unexpected keyword argument error", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "64a0cfdf2cd580efc3150e2c5dad41c79a4ba870c7bbbec5a65e76343faca42a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Custom FinalAnswerTool with multiple inputs raises unexpected keyword argument error\n\nDescription: **Describe the bug** Within the agent local python interpreter, any final_answer tool with custom inputs, other than answer, will raise an unexpected keyword argument error in some sort of validation but will adress the custom defined inputs after. When executing outside the agent local python interpreter, it works well. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** When running the tool calling agent, I get the following error: [CODE_BLOCK] The final answer only returns the answer field, while sources and info are set to None or empty strings. [CODE_BLOCK] When running the code agent, I get the following error: [CODE_BLOCK] - The first step try to call the final_answer function with all parameters, but it fails because the function does not accept sources. - The second step tries to call it with answer and info, but it fails again because the function does not accept info. - Lastly, it tries to call it with only answer, which succeeds, BUT this time, the final_answer version used rearrange the inputs as defined in the CustomFinalAnswerTool, so it returns sources as the answer and answer and info as None. [CODE_BLOCK]py final_answer(sources=\"['abc', 'def']\", answer=\"Hello\", info=\"This is a test\") [CODE_BLOCK]py final_answer(answer=\"Hello\", info=\"This is a test\") [CODE_BLOCK]py final_answer(answer=\"Hello\") [CODE_BLOCK] When calling the tool directly, outside the agent's local interpreter, it works as expected: [CODE_BLOCK] [CODE_BLOCK] **Expected behavior** The CustomFinalAnswerTool should correctly handle all inputs defined in the inputs dictionary and the forward method without raising unexpected keyword argument errors. A lazy workaround is to keep the answer as unique input with type object and then parse the inputs within the forward method, but this is very unstable as it goes against the intended design of the tool and against the 3rd rule of the system prompt. [CODE_BLOCK] **Packages version:** [CODE_BLOCK] **Additional context** I tested only with local interpreter.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bb559d98-b9a3-46ff-8b5d-5c01c4658722", "embedding": null, "metadata": {"issue_id": 1381, "title": "Manager-Managed Agent Interaction", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45bb664f-6437-4b37-a3a8-70d96c548bb4", "node_type": "4", "metadata": {"issue_id": 1381, "title": "Manager-Managed Agent Interaction", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "c4c85dab94de9a3f3ca0fe0bcc778bed2fc3a6ed5edc33e68d005ee8b15d7135", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Manager-Managed Agent Interaction\n\nDescription: **Is your feature request related to a problem? Please describe.** Currently, managed agents are simply treated as standalone tool calls. A managed agent have no awareness of the code execution history of the agent that called it (the \"manager\"), although a manager agent does have a record of the managed agent's execution history if provide_run_summary = True. Neither the manager agent nor the managed agent has access to the objects in the other's python_executor. This limits what managed agents are capable of because they do not have access to their manager agents' data (see https://github.com/huggingface/smolagents/issues/1184 for an example). Moreover, a managed agent's memory is reset on each call (athough interestingly, python_executor is retained), so if a managed agent is called multiple times, it may need to redo work that it did in a previous call. **Describe the solution you'd like** 1. Give a managed agent a copy of the calling manager agent's message history, e.g. manager_agent.write_memory_to_messages() or some other summary of the manager agent's thoughts and actions. Or maybe even copy the managed agent and set managed_agent.memory = manager_agent.memory. 2. Use the same python_executor for both the manager agent and the managed agent. 3. Give the manager agent a copy of the managed agent's execution history. This is already implemented via provide_run_summary = True. This should be optional, as there are drawbacks: 1. This gives the managed agent access to a lot of potentially irrelevant information, possibly degrading it's performance. 2. Consumes more tokens. 3. Depending on the implementation, reduces the reusability of a managed agent. Its internal state (e.g. python_executor) will be shared with its manager agent, so it would be dangerous to run the managed agent by itself or to assign it to more than one manager agent.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e1aae7e-8d81-4e37-915c-19feac17932a", "embedding": null, "metadata": {"issue_id": 1378, "title": "Mcp_client class doesn't implement the mcp resources and prompts api ", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4bf6f9d0-30a1-49b0-9602-40ebd2c0c8b7", "node_type": "4", "metadata": {"issue_id": 1378, "title": "Mcp_client class doesn't implement the mcp resources and prompts api ", "state": "open", "labels": [], "type": "issue"}, "hash": "1930a4b4078a12ebeefbef24f126abbe4c6506f11c76328ca0427149331b07c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Mcp_client class doesn't implement the mcp resources and prompts api \n\nDescription: The smolagents mcp_client class doesn't implement the mcp resources and prompts api which may be helpful in creating multi agent systems where the prompts for each agent or step can be stored in the mcp and similarly for specific tasks the resources templates can also be stored.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bf11ad84-b6d9-459f-8ba5-6a2e6c1b469c", "embedding": null, "metadata": {"issue_id": 1374, "title": "Append step to memory before final_answer_check", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c1480cb-6f22-429c-8288-96ee36dab675", "node_type": "4", "metadata": {"issue_id": 1374, "title": "Append step to memory before final_answer_check", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "d4eefd03a74509fc7f29ed564da4afec5b1df919a2aec067f25e5db6fa614688", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Append step to memory before final_answer_check\n\nDescription: **Is your feature request related to a problem? Please describe.** I was trying to use the final_answer_checks callback as a way to implement a critic that tells the agent if it can stop. However, I noticed that - at least in the CodeAgent, I haven't tried with ToolCallingAgent yet - the final_answer_checks are run prior to the step being appended to self.memory.step. That means that the AgentMemory object the final_answer_checks callable gets does not include the final step. I tried with the following toy example: [CODE_BLOCK] The AgentMemory input to critic_function only got the TaskStep object, even though the agent actually did the right thing with the following code: [CODE_BLOCK] So my critic was left to assume that it just did the arithmetic within the llm and so wants the agent to continue. **Describe the solution you'd like** Move the self.memory.steps.append(action_step) from the finally block in self._run to be within the _execute_step method. I'd be happy to make a PR for this if you don't foresee it causing any issues. The only issue I could imagine is that **Is this not possible with the current options.** I was able to work around it for now by making a custom agent class: [CODE_BLOCK] and then my critic class used agent.current_step to get the extra step it needs. **Describe alternatives you've considered** If adding to the memory would cause problems, we could potentially include the current step in the final_answer_checks inputs but that would not be a backwards compatible change and would break everyone's callables that don't currently expect it. We could also move the final_answer_checks to happen within the finally block after the step is added to memory. **Additional context** Here is the relevant code from the current _run method: [CODE_BLOCK]\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1970, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd30182d-766e-4408-b1c8-d2e36fce7cd0", "embedding": null, "metadata": {"issue_id": 1372, "title": "[BUG] Unexpected keyword argument error in CodeAgent constructor within the gradio_ux.py example", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43155a81-e3dc-4da2-8922-f797ec6a3c90", "node_type": "4", "metadata": {"issue_id": 1372, "title": "[BUG] Unexpected keyword argument error in CodeAgent constructor within the gradio_ux.py example", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "9abab73ef1abe0c87b415988e50b99f375a3680f5d1a196772e23461d1fb0a2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Unexpected keyword argument error in CodeAgent constructor within the gradio_ux.py example\n\nDescription: **Describe the bug** When running running the gradio_ux.py code from the examples folder, I encountered an unexpected keyword argument error for use_structured_outputs_internally. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Removing the use_structured_outputs_internally parameter from the CodeAgent constructor resolved the issue and allowed the code to run successfully: [CODE_BLOCK] **Packages version:** smolagents==1.16.1 **Additional context** Code and error screenshot: !Image\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "482868d5-0b1a-4f28-a0f2-3307beb42ea1", "embedding": null, "metadata": {"issue_id": 1370, "title": "flatten_messages_as_text=True raises error when images are present", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fb9af6a2-502c-4aa4-9683-b81c34d87c29", "node_type": "4", "metadata": {"issue_id": 1370, "title": "flatten_messages_as_text=True raises error when images are present", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "dd52f85678905c7847cc3242b45644736754485804363edd5cc38b2ea8324343", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: flatten_messages_as_text=True raises error when images are present\n\nDescription: Description: When flatten_messages_as_text=True is set, passing messages that contain images results in an error: Error: Cannot use images with flatten_messages_as_text=True Expected Behavior: - Either images should be gracefully skipped or converted to placeholders in the flattened text, - Or the error message could suggest a fallback or partial flattening strategy. Use Case: In multimodal pipelines, it\u2019s common to mix text and image content. It would be useful to allow text flattening without completely failing on image presence.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ae3476c-9bec-4ac3-9e36-d0782c46cd21", "embedding": null, "metadata": {"issue_id": 1369, "title": "LaTeX expressions are corrupted in final_answer", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37896c18-44d0-442d-8b97-5de1e7320f38", "node_type": "4", "metadata": {"issue_id": 1369, "title": "LaTeX expressions are corrupted in final_answer", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "883c7760f4fd1c69ca7b6216b047570bd77419bb804d24b2b298ca8736888bc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: LaTeX expressions are corrupted in final_answer\n\nDescription: **Describe the bug** LaTeX expressions are corrupted in final_answer: **Code to reproduce the error** The simplest code snippet that produces your bug. **Error logs (if any)** [CODE_BLOCK] **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** 1.17.0.dev0 at 2025-May-22 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85950c30-bcd2-4e48-b593-b2599f2e02c1", "embedding": null, "metadata": {"issue_id": 1368, "title": "_generate_planning_step should support image inputs", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ab16024-c920-4fd2-acaf-e6e5bfc834e1", "node_type": "4", "metadata": {"issue_id": 1368, "title": "_generate_planning_step should support image inputs", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "63ffacaeb3ac1f64e8347a4d4e652d20a500bb4f87dc580106da94595486418e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: _generate_planning_step should support image inputs\n\nDescription: Currently, _generate_planning_step only processes textual messages. In multimodal workflows, it\u2019s often necessary to include images (e.g., screenshots or visual context) as part of the reasoning input.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 380, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d313aa32-7da2-4614-9d5e-486bfe246aa7", "embedding": null, "metadata": {"issue_id": 1366, "title": "Streaming from a parent agent does not show steps from the managed agents", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ee9dfef-4bf6-4eca-ba16-d5da0e2d9c38", "node_type": "4", "metadata": {"issue_id": 1366, "title": "Streaming from a parent agent does not show steps from the managed agents", "state": "open", "labels": [], "type": "issue"}, "hash": "e0346735addb982fc9257379686d70dab7b430c298e54bf6bd61d5a67cfed927", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Streaming from a parent agent does not show steps from the managed agents\n\nDescription: When I'm running something like this: [CODE_BLOCK] The only values you get back are from the manager_agent. I'm wondering if there is something I'm missing in how to access the web_agent steps or if this is something that needs adding?\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f897b93c-6064-4382-b931-4aff26c1f396", "embedding": null, "metadata": {"issue_id": 1365, "title": "Incorrect Method Name in MCPClient Docstring Example (stop() should be disconnect())", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5205b574-0e39-4767-88bc-aa1316b009dc", "node_type": "4", "metadata": {"issue_id": 1365, "title": "Incorrect Method Name in MCPClient Docstring Example (stop() should be disconnect())", "state": "closed", "labels": [], "type": "issue"}, "hash": "6a795e6b19b3e4c9b80199f3512263d2f4b1ba7731f5b6ddc555d457e2178163", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Incorrect Method Name in MCPClient Docstring Example (stop() should be disconnect())\n\nDescription: In the MCPClient class docstring (file: smolagents/mcp_client.py), the example for manual connection management currently uses mcp_client.stop() in the finally block: [CODE_BLOCK] However, the correct method to close the connection is mcp_client.disconnect(), as defined in the class. There is no stop() method, so this could confuse users and lead to errors. Suggested fix: Update the docstring example to use mcp_client.disconnect() instead of mcp_client.stop(). This will ensure the documentation matches the actual API and helps users manage connections correctly.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d20a343b-74a7-4886-8aac-d80adc304ada", "embedding": null, "metadata": {"issue_id": 1362, "title": "[DOCS] Broken link in agentic RAG examples page", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "957518a2-fcf4-4c5d-85a2-a0d1bff55fc4", "node_type": "4", "metadata": {"issue_id": 1362, "title": "[DOCS] Broken link in agentic RAG examples page", "state": "closed", "labels": [], "type": "issue"}, "hash": "1d39a4d6242be8b4f9acddb040cd07629ee5d10b46a3a6f3435debc0853c304c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [DOCS] Broken link in agentic RAG examples page\n\nDescription: There is a broken link in the agentic RAG examples page to inference-providers. It should point to: https://huggingface.co/docs/inference-providers/index\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af037ca1-f20c-4a8f-8a8c-a0847b799318", "embedding": null, "metadata": {"issue_id": 1361, "title": "[BUG] Exporting Agents fails if the agent has MCP Tools", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f33a20d4-f6fd-46c6-b3d2-b4accd5413be", "node_type": "4", "metadata": {"issue_id": 1361, "title": "[BUG] Exporting Agents fails if the agent has MCP Tools", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "hash": "bade8142db2a1f6db7932f1bc57caa4a4fe1d5ca01722834438f94a99889013d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Exporting Agents fails if the agent has MCP Tools\n\nDescription: When exporting agents with MCP tools the existing export functionality fails as it expects tools to be python tools. [CODE_BLOCK]\n\nState: closed\n\nLabels: bug duplicate\n\nCategories: category-bug category-duplicate\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d78ccd80-819d-44cc-8e87-83bce3676a27", "embedding": null, "metadata": {"issue_id": 1360, "title": "Improved Agent Export", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "795a4d05-a349-471e-94ab-aa13e97d49f1", "node_type": "4", "metadata": {"issue_id": 1360, "title": "Improved Agent Export", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "2f6a9af1a7f9da86ece0e0c171ba9be4e6572ba7ce9d2a77895dcbf936189cb6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Improved Agent Export\n\nDescription: Agent importing and exporting expects all managed agents to be nested under the agent. When you have complex hierarchies of agents this gets really ugly really fast. It would be nice if exporting multiple agents gave some ability to load other agents by reference / path instead of requiring them to be nested.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31e9330e-816b-488e-b55d-d77915228248", "embedding": null, "metadata": {"issue_id": 1359, "title": "delete", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f73db1dd-55e4-4844-bd0c-13f933a37ce9", "node_type": "4", "metadata": {"issue_id": 1359, "title": "delete", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "503811cbfac69c5b16968f9aae7c3a0a2a26876d0b5d5b6483de9d3adc8f681e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: delete\n\nDescription: deleted\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf0d240d-64f2-44ff-bdf0-beb19750dcbf", "embedding": null, "metadata": {"issue_id": 1354, "title": "Support Streamable stateless MCP Servers", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5eb230d0-7c9d-43b1-b3ad-856cf6b25fe3", "node_type": "4", "metadata": {"issue_id": 1354, "title": "Support Streamable stateless MCP Servers", "state": "closed", "labels": [], "type": "issue"}, "hash": "b981444c04a3a1bef9747ae61bfd0b4a73e47ed6281844c73d5684d5c77b12f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support Streamable stateless MCP Servers\n\nDescription: > @grll do we need to do some work on mcpadapt side to enable these new streams then when they have the SDK? > > For sure, but I would maybe wait a bit for it to stabilize and look at official implementation in the mcp python sdk. In particular because in any case they mention that the new streams approach is fully backward compatible with previous SSE endpoint approach. > > If my understanding is correct, it actually is SSE but done slightly differently. I am not an expert in SSE but I was a bit confused because for me readable streams and streamingResponse are all what is used for SSE anyway but reading more it seems to be now a mix of regular HTTP calls and SSE and both client and server can initiate, also SSE is not mandatory anymore in which case we end up polling? Anyway I probably need to do a bit more reading on this. > > What could also be a really nice addition in my view is the authentication maybe implementing the auth flow generic blocks in mcpadapt could be quite useful. And providing out of the box authentication capabilites to remote / HTTP mcp server to many agentic frameworks incl. of course our beloved smolagents. > > sorry for the thread hijack @aymeric-roucher / @albertvillanova let me know if you have feedback on the above implementation I will open a draft PR tomorrow so we can also discuss directly on it. _Originally posted by @grll in 1179_\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1464, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "64ccc0ac-1685-4cd7-9cd9-36daa60744b3", "embedding": null, "metadata": {"issue_id": 1353, "title": "[BUG] Managed Agents aren't registered as tools", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24e8c6a8-dc8d-46dc-ad5f-ba2d2de8d3e1", "node_type": "4", "metadata": {"issue_id": 1353, "title": "[BUG] Managed Agents aren't registered as tools", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "41595f2ad91faab1274891df6563e25978b18e5688966fff2eb199e21ade82ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Managed Agents aren't registered as tools\n\nDescription: When using Gemini, only registered tools will be called by the LLM. The logic for starting a new managed agent is hidden in the call tool code and the managed agents aren't themselves exposed as tools. This means that tool calling agents cannot start managed agents on gemini This is not a problem for the code agent as it doesn't directly make tool calls in the same way\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e5e185cd-2fc0-443d-a0fd-9f1443b2331b", "embedding": null, "metadata": {"issue_id": 1352, "title": "[BUG] Coding Agent cannot make pydantic types", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9afd0a6f-4dd3-49d3-b051-685189c1eb5f", "node_type": "4", "metadata": {"issue_id": 1352, "title": "[BUG] Coding Agent cannot make pydantic types", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "82ac7f7e1043654a38feb7759c37f3d2d4ba9759a804d3b1360facc27c548177", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Coding Agent cannot make pydantic types\n\nDescription: [CODE_BLOCK] [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f3950eaa-09ab-48e3-958a-b9bce2279854", "embedding": null, "metadata": {"issue_id": 1349, "title": "[BUG] When planning LiteLLM can send a message after the termination message", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "409a4088-840a-40a9-b8f7-caa1359fcbfb", "node_type": "4", "metadata": {"issue_id": 1349, "title": "[BUG] When planning LiteLLM can send a message after the termination message", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "5f0583407c556df54decdf5038a83ef3c2001fce330fa2f6dcf543ac753b5f9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] When planning LiteLLM can send a message after the termination message\n\nDescription: **Describe the bug** When streaming and planning, after the message containing the finish_reason, LiteLLM sometimes send another message with all fields at None which causes an error. This bug is linked to issue 1347 because the error can't be raised if we just can't stream and plan. Issue appear with the code below on the PR : 1348. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** To not crash and to stop streaming properly when received a stop message. **Packages version:** on branch from PR : 1348 which was from main. **Additional context** Maybe review 1348 first.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af195936-1771-4b62-bcab-7d26e1dcfc64", "embedding": null, "metadata": {"issue_id": 1347, "title": "[BUG] Can't stream and plan with Ollama for a CodeAgent.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64b2661c-5eb5-4ac7-910d-6c312bfccad4", "node_type": "4", "metadata": {"issue_id": 1347, "title": "[BUG] Can't stream and plan with Ollama for a CodeAgent.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f01c430b1d113378a3d6be6eddd6d16c4e1c99ef2d849f74174dbd7bb38754a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Can't stream and plan with Ollama for a CodeAgent.\n\nDescription: **Describe the bug** In a CodeAgent, when trying to stream and set a planning interval, the stream does not work. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** No error, just no stream **Expected behavior** A stream of the planning step and the LLM output. **Packages version:** On main branch\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94aa94f6-5261-4310-a677-4c768b976934", "embedding": null, "metadata": {"issue_id": 1345, "title": "[BUG] Cannot use @tool decorator with remote Python executor", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85aa9380-58bb-40f1-b283-bb6034a30920", "node_type": "4", "metadata": {"issue_id": 1345, "title": "[BUG] Cannot use @tool decorator with remote Python executor", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "7ada410518247e1b3e2934f7a96b42465af54845b02fa1a223b38e910e3ac249", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Cannot use @tool decorator with remote Python executor\n\nDescription: **Describe the bug** Using a @tool decorated tool with the docker executor results in an AgentError. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Code shall execute without throwing an AgentError. **Packages version:** smolagents==1.16.1 **Additional context** It seems that the issue is related to the @tool decorated function not being transformed into a forward method for the SimpleTool constructed by instance_to_source. I have proposed a fix in 1334.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 678, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e70646a2-0308-4569-a3fc-15bec0a09c87", "embedding": null, "metadata": {"issue_id": 1343, "title": "[BUG] setting stream_outputs=True in a CodeAgent made the model ignore api_base", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc96c160-96fd-4b1c-97fa-225041ace9ca", "node_type": "4", "metadata": {"issue_id": 1343, "title": "[BUG] setting stream_outputs=True in a CodeAgent made the model ignore api_base", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "c5c29b97454d46697833959728c79ab181dea5e706b21546014e1ae799e2224f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] setting stream_outputs=True in a CodeAgent made the model ignore api_base\n\nDescription: **Describe the bug** I am using Ollama on a remote server as a model. When launching a CodeAgent with this model and giving stream_outputs=True to the CodeAgents I get an error because it tries to access url http://localhost:11434 instead of the api_base I gave in the LiteLLMModel definition. **Code to reproduce the error** \"\"\" from smolagents import CodeAgent, LiteLLMModel model = LiteLLMModel(model_id=\"ollama/qwen3:32b\", api_base=\"http://1.1.1.1:11434\", num_ctx=8192) agent = CodeAgent( name=\"test_agent\", description=\"A test agent.\", tools=[], managed_agents=[], model=model, stream_outputs=True, ) agent.run(\"Hello\") \"\"\" This code tries to access a model in localhost:11434 instead of the 1.1.1.1 provided. When removing the flag stream_outputs=True, or setting it to False, the code tries to access 1.1.1.1:11434 as it should. **Error logs (if any)** smolagents.utils.AgentGenerationError: Error in generating model output: litellm.APIConnectionError: OllamaException - {\"error\":\"model 'qwen3:32b' not found\"} **Expected behavior** with or without stream_outputs, it should access the right endpoint for ollama. **Packages version:** smolagents==1.16.1\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfb2a51e-3cfc-4b5d-99c4-8717f2f898b6", "embedding": null, "metadata": {"issue_id": 1342, "title": "Async tool support", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10906404-4951-474a-95c1-706730612757", "node_type": "4", "metadata": {"issue_id": 1342, "title": "Async tool support", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "89eeb6ee28eb9fe94654ab6090e6da3ddaae288849423f3123cf32276a6c70cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Async tool support\n\nDescription: **Is your feature request related to a problem? Please describe.** most tools have async nature, like search web, visit web. **Describe the solution you'd like** supports async tool **Additional context** either can change default tool to be async, or supports sync/async at the same time, if tool is awaitable, then awaits it.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73af09b5-9071-4117-adbc-e463370d41ac", "embedding": null, "metadata": {"issue_id": 1341, "title": "OpenRouter API Support Request", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca021d6b-9f6c-4c1e-9242-bdb8ac2425ce", "node_type": "4", "metadata": {"issue_id": 1341, "title": "OpenRouter API Support Request", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "ff8f088520d2d7cd1dc55254363a15849fb752a56055d3d1bf7e716187c776a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: OpenRouter API Support Request\n\nDescription: **Is your feature request related to a problem? Please describe.** I'm often frustrated by the lack of flexibility in choosing language models or switching between providers. Currently, I'm locked into a specific set of model options and billing methods, which limits experimentation and performance optimization. **Describe the solution you'd like** I would like to see support for the [OpenRouter API](https://openrouter.ai/docs), which provides access to a wide range of LLMs through a unified API. This would allow users to select from various models (e.g., OpenAI, Anthropic, Mistral, Cohere, etc.) and optimize for cost, latency, or performance as needed. **Is this not possible with the current options?** No, current integrations do not support OpenRouter or offer a flexible routing layer for using multiple LLMs from different providers in a seamless way. The current API setup is limited to a specific vendor's ecosystem. **Describe alternatives you've considered** * Manually routing API calls via a backend server that proxies requests to OpenRouter (adds complexity, overhead). * Using different tools for different models (disjointed experience). * Maintaining separate billing/accounts per model provider (inefficient and hard to manage at scale). **Additional context** OpenRouter offers a robust and standardized API, usage dashboard, and growing model support. Adding native integration would empower users with more control, easier experimentation, and a broader feature set. Here's the API documentation for reference: https://openrouter.ai/docs\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1726, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a2727ca-f35c-4d48-9ef7-ddd589bb2a1f", "embedding": null, "metadata": {"issue_id": 1339, "title": "[BUG] CodeAgent doesn't work in non-main threads.", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0609abc-6d99-48e9-8af0-1079574b4d38", "node_type": "4", "metadata": {"issue_id": 1339, "title": "[BUG] CodeAgent doesn't work in non-main threads.", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "9c45538c173e1e8ee25e3c40ca6f56d3c5b21439ec7e6e8d112585489c98e3d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] CodeAgent doesn't work in non-main threads.\n\nDescription: **Describe the bug** The latest code agent version no longer works in a thread due to the use of signals. We are trying to use smolagents in a fastapi service, so having it run in a non-main thread is a pretty hard requirement (otherwise it blocks streaming for extended periods of time). Making timeout decorator optional in local_python_executor fixes the issue. I'm happy to contribute a PR if you want. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** Interpreter fails with: [CODE_BLOCK] **Packages version:** smolagents==1.16.0\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "87f95398-a7ff-4108-ad62-0c17b6d719c4", "embedding": null, "metadata": {"issue_id": 1338, "title": "[BUG] signal only works in main thread of the main interpreter (multi agent)", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f317f806-1181-41a1-b069-495d00712799", "node_type": "4", "metadata": {"issue_id": 1338, "title": "[BUG] signal only works in main thread of the main interpreter (multi agent)", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "96a9f2434d43c4b51f2497e94db92e60c21541f24a983bef780e206db8bb287e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] signal only works in main thread of the main interpreter (multi agent)\n\nDescription: **Describe the bug** Code execution errors in \"signal only works in main thread of the main interpreter\" with multi agent setup **Code to reproduce the error** Code from the docs used **Packages version:** Main commit: https://github.com/huggingface/smolagents/commit/6b4ad144cd262f99dc8ef1d48ec1d694d16b0966\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4cdd7770-ea6d-41e2-a69c-7fe58821defe", "embedding": null, "metadata": {"issue_id": 1332, "title": "Enable local web agents via api_base and api_key", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ccbbdb4c-65f1-48c9-8437-a1f89b7dc393", "node_type": "4", "metadata": {"issue_id": 1332, "title": "Enable local web agents via api_base and api_key", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "626b23e75c32871aa477e9eff00f20446fc8eec1b0425d48e41a9b64eb9f37e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Enable local web agents via api_base and api_key\n\nDescription: **Is your feature request related to a problem? Please describe.** Webagents currently hardcodes both api_base and api_key which makes it impossible to use local models as the brain of a webagent via the CLI. https://github.com/huggingface/smolagents/blob/2bc29d2a46539b80b7b7761ee54d4c7ac0dc6e8e/src/smolagents/vision_web_browser.pyL201C13-L201C23 **Describe the solution you'd like** I would love for the cli to allow two extra options, --api-base and --api-key, so that I can utilize my models running in Ollama and LiteLLM. **Is this not possible with the current options.** It is not, the api_base and api_key values are both hardcoded to None. **Describe alternatives you've considered** Loading from environment variables is an option, but I believe making the CLI changes is the best solution. It will bring the webagent CLI up to parity with the agent CLI.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1043, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "024a0a42-08f8-4b61-ac09-23e006ec3144", "embedding": null, "metadata": {"issue_id": 1331, "title": "[BUG] SmolagentsInstrumentor does not create model spans when using CodeAgent", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "908a85e2-78e2-47a3-8d76-3b66ab6f93fd", "node_type": "4", "metadata": {"issue_id": 1331, "title": "[BUG] SmolagentsInstrumentor does not create model spans when using CodeAgent", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "429ca5857c090497a28c095546a81bcc8d03dd00b30af15395677732d5bd0ff2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] SmolagentsInstrumentor does not create model spans when using CodeAgent\n\nDescription: Hello! I'm trying to use opentelemetry-instrumentation-smolagents with this library, but unfortunately, it's not working as expected. I noticed that maintainers from this repository have contributed to the instrumentation. Therefore, I wanted to bring this issue to your attention in case you could provide any insights. More details and steps to reproduce can be found in the related issue in the OpenTelemetry repository: https://github.com/Arize-ai/openinference/issues/1636 Thank you!\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f6b2c026-2cb0-452c-b270-00561b495a87", "embedding": null, "metadata": {"issue_id": 1328, "title": "Show reasoning tokens in telemetry", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e871d75e-9139-448c-963a-80cdff5a10f8", "node_type": "4", "metadata": {"issue_id": 1328, "title": "Show reasoning tokens in telemetry", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "3abdd0447cac20e981738f2dc6fb146ce0bf8d5c1ad1dce6e037f24a538aebd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Show reasoning tokens in telemetry\n\nDescription: I want to be able to see the reasoning tokens generated by Claude 3.7. This is supported by LiteLLM. How I setup the model used by CodeAgent: [CODE_BLOCK] Output in Phoenix !Image\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "742b13c6-ce4f-4034-8abd-ae8f4fa94887", "embedding": null, "metadata": {"issue_id": 1323, "title": "CI test fails: DuckDuckGoSearchException: https://lite.duckduckgo.com/lite/ 202 Ratelimit", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31161874-2d65-45f5-a828-2977d1c1e008", "node_type": "4", "metadata": {"issue_id": 1323, "title": "CI test fails: DuckDuckGoSearchException: https://lite.duckduckgo.com/lite/ 202 Ratelimit", "state": "closed", "labels": [], "type": "issue"}, "hash": "30fdace9866e366c5b067af885facc82aa659b9eb49f50289d94b6703afeadbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: CI test fails: DuckDuckGoSearchException: https://lite.duckduckgo.com/lite/ 202 Ratelimit\n\nDescription: CI test fails: > DuckDuckGoSearchException: https://lite.duckduckgo.com/lite/ 202 Ratelimit See: https://github.com/huggingface/smolagents/actions/runs/15002040575/job/42151265147?pr=1322 [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd26754c-64ee-4de9-bdbf-4466896108f0", "embedding": null, "metadata": {"issue_id": 1316, "title": "[BUG] GradioUI displays the planing twice when streaming", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f7b0216-0c17-45c3-9041-bae3d7d91be1", "node_type": "4", "metadata": {"issue_id": 1316, "title": "[BUG] GradioUI displays the planing twice when streaming", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "9dd62b216c91f20bb45d659d0cffce720a622e8e865cc2c7c9647366d74672f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] GradioUI displays the planing twice when streaming\n\nDescription: **Describe the bug** When streaming outputs, the GradioUI displays the plan twice: !Image Related to: - 1305 **Code to reproduce the error** The code in examples/gradio_ui.py **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4bc3b03-04ca-4eb7-a31e-e460dca310f4", "embedding": null, "metadata": {"issue_id": 1310, "title": "Google's A2A protocol support", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dded558-64bf-4ef6-8aaf-c30456578407", "node_type": "4", "metadata": {"issue_id": 1310, "title": "Google's A2A protocol support", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "bccbd56d1593ea2fa78938f2f890f03cbb8da8c8dd91c5a1b77b6137215c9388", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Google's A2A protocol support\n\nDescription: Google has announced the A2A protocol: A2A Are there any plans to implement A2A?\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a4beb16-18b5-4ddc-b489-1bbed5e44768", "embedding": null, "metadata": {"issue_id": 1308, "title": "[BUG] AttributeError when using GradioUI with min supported Gradio version", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64f37e7f-2aaa-4be8-b3dc-589d9746e94b", "node_type": "4", "metadata": {"issue_id": 1308, "title": "[BUG] AttributeError when using GradioUI with min supported Gradio version", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "3fe53dcb6a7cee88297332b705b039300420ac9f558e95160d8b4f871af5f862", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] AttributeError when using GradioUI with min supported Gradio version\n\nDescription: **Describe the bug** Using GradioUI with the minimum supported version of Gradio 5.13.2 raises AttributeError: > AttributeError: module 'gradio' has no attribute 'Sidebar' **Code to reproduce the error** 1. Install the minimum supported version of Gradio: pip install -U gradio==5.13.2 1. Run the example in examples/gradio_ui.py. **Error logs (if any)** [CODE_BLOCK] **Expected behavior** No error raised. **Packages version:** smolagents in main branch with gradio-5.13.2. **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b711344e-5897-4d67-9e41-fc4644cf6223", "embedding": null, "metadata": {"issue_id": 1307, "title": "Passing Additional Parameters to Tokenizer through MLXModel Interface", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "664df412-498f-4723-bb89-7ca2b138e32a", "node_type": "4", "metadata": {"issue_id": 1307, "title": "Passing Additional Parameters to Tokenizer through MLXModel Interface", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "3a28a9e604838caef6ea113fc78e903a6a61f08e104fae316693511c3fc8b6e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Passing Additional Parameters to Tokenizer through MLXModel Interface\n\nDescription: Unable to pass additional parameters to tokenizer through the MLXModel module. For example, QWEN3 has enable_thinking flag which you can pass as a boolean value. However MLXModel does not have a provision for that to be passed. Above should be allowed. Other examples are Cohere RAG mode in the chat template. There is currently no way to pass those parameters that I was able to find. Example below is where the value is passed but it is ignored as it is a tokenizer parameter rather than a model one. [CODE_BLOCK]\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3229020c-bcda-4cd4-ac52-26efe8f75e12", "embedding": null, "metadata": {"issue_id": 1305, "title": "[BUG] Broken gradio display after stream output feat", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "59b388bb-7ebf-4817-90da-c188ad3cd325", "node_type": "4", "metadata": {"issue_id": 1305, "title": "[BUG] Broken gradio display after stream output feat", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "71cdafece5c65d98e9d4d4b05fa0534f5cec87fb147abba5bfabdbb0d097507f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Broken gradio display after stream output feat\n\nDescription: **Describe the bug** After update to 1.15.0, the gradio ui seems to broken. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** 1. when stream_outputs=True, the message will display twice. <img width=\"568\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/adb692df-1d43-497e-825d-1178536db174\" /> 2. when stream_outputs=False, the message will nerver display. <img width=\"599\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/a7194582-7aba-4f20-bfc8-64c3fd295bda\" /> **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** 1.15.0 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 843, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac64b6cc-d25a-404c-9d65-f2956df00417", "embedding": null, "metadata": {"issue_id": 1301, "title": "[BUG] Each agent run overwrites the static tools of the local Python executor", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81586791-df0d-42be-a0e0-363c8d4c984b", "node_type": "4", "metadata": {"issue_id": 1301, "title": "[BUG] Each agent run overwrites the static tools of the local Python executor", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "d55cb10a63546df66d657311e929df91e784786d516661d5e391636494d1bad7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Each agent run overwrites the static tools of the local Python executor\n\nDescription: Describe the bug Each time we call agent.run, the local Python executor static tools (agent.python_executor.static_tools) are overwritten. I think this is unexpected behavior. For example, if a user wants to add a specific Python function (like open) to its agent's local Python executor, none of these will work: [CODE_BLOCK] The only current solution would be to change smolagents.local_python_executor.BASE_PYTHON_TOOLS: [CODE_BLOCK] However, this will affect all agents you create afterwards. See discussion here: - https://github.com/huggingface/smolagents/issues/105issuecomment-2857142485 Code to reproduce the error [CODE_BLOCK] ~~~python from smolagents import CodeAgent, InferenceClientModel agent = CodeAgent(tools=[], model=InferenceClientModel()) agent.python_executor.static_tools[\"open\"] = open or: agent.python_executor.send_tools({\"open\": open}) agent.run(\"\"\"Execute this Python code verbatim: [CODE_BLOCK] \"\"\" ) ~~~ Error logs (if any) [CODE_BLOCK] Expected behavior A clear and concise description of what you expected to happen. Packages version Main branch of smolagents. Additional context https://github.com/huggingface/smolagents/pull/1175issuecomment-2796385000 > The idea of send_tools is to be able to update tools in case they've been modified in-between calls to agent.run()\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "14f0cd79-c8c4-4a27-87e0-10d263524b19", "embedding": null, "metadata": {"issue_id": 1299, "title": "[BUG] - Cannot update system prompt", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e84c049-c614-4d7e-9498-a5eed4ec5252", "node_type": "4", "metadata": {"issue_id": 1299, "title": "[BUG] - Cannot update system prompt", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "b1afc9471d21f63ae4a38cc62828703b79548275d1d3ace40764d9d375298a81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] - Cannot update system prompt\n\nDescription: **Describe the bug** A clear and concise description of what the bug is. The doc is providing misleading instructions about how to change the system prompt. !Image **Code to reproduce the error** The simplest code snippet that produces your bug. [CODE_BLOCK] **Error logs (if any)** Provide error logs if there are any. > TypeError: MultiStepAgent.__init__() got an unexpected keyword argument 'system_prompt' **Expected behavior** A clear and concise description of what you expected to happen. I know updating the system prompt is discouraged but I think, it should be easier to do. Besides, it is currently \"required\" for the AI Agent course. **Packages version:** smolagents==1.14.0 **Additional context** Related to 900\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 862, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7bc9357-38bf-4978-a18b-c8f685b97ea1", "embedding": null, "metadata": {"issue_id": 1298, "title": "[BUG] Can't integrate with Bedrock (Claude model)", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04541fa7-1973-4233-9896-50d7424a8340", "node_type": "4", "metadata": {"issue_id": 1298, "title": "[BUG] Can't integrate with Bedrock (Claude model)", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "6fa30d273b47835a4941de0e9b28201e6310af4ff3495206cde6f91d825b02ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Can't integrate with Bedrock (Claude model)\n\nDescription: This error is occurring in the LiteLLM library when trying to access Amazon Bedrock's Anthropic Claude model. [CODE_BLOCK] [CODE_BLOCK] **Packages version:** \"smolagents[litellm,telemetry]>=1.14.0\" **Additional context** I think the bug is related to the __init__ method of AmazonAnthropicConfig. It seems to be missing a call to super().\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6142fa4a-e9c8-4848-8825-d7afd303db21", "embedding": null, "metadata": {"issue_id": 1295, "title": "Improve usability by enabling DDG search in the base package", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "673b0d7a-7a69-4c5c-b009-121f6657fba1", "node_type": "4", "metadata": {"issue_id": 1295, "title": "Improve usability by enabling DDG search in the base package", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "a1bc414607ac70055fcb40311a90ab03c3a9dc74596724613291f596a36a4b10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Improve usability by enabling DDG search in the base package\n\nDescription: Pr https://github.com/huggingface/smolagents/pull/1271 removes duckduckgo-search from base packages, thus requiring users to install a specific set of dependencies to run it: this means many basic examples require a specific packages, which is a bit of friction. To remove this friction, maybe it's possible to enable the DuckDuckGoSearchTool without requiring `duckduckgo-search, for instance taking elements from this function: https://github.com/deedy5/duckduckgo_search/blob/b39a1565972b94e8c0b87e63c833f7ca4f3743e6/duckduckgo_search/duckduckgo_search.pyL254. @albertvillanova\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0ec681d-41ab-47fe-84b6-aeb90f7fb1be", "embedding": null, "metadata": {"issue_id": 1292, "title": "[BUG] Confusing the LLM with unavailable tools in code_agent.yaml system prompt", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "afcab429-abfa-4ab5-b526-483f72c02ac4", "node_type": "4", "metadata": {"issue_id": 1292, "title": "[BUG] Confusing the LLM with unavailable tools in code_agent.yaml system prompt", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "73982b623a33e374a7baa81b089ed779cfafbd3cc59b9db46e9ea4b3a7bcdab1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Confusing the LLM with unavailable tools in code_agent.yaml system prompt\n\nDescription: In code_agent.yaml there are several examples with tool names that are not available if not explicitly defined. There is this one sentence after examples, but that is not reliable with different LLMs and/or all cases. > Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools: Code to reproduce: [CODE_BLOCK] Triggers this: > Code execution failed at line 'distance_earth_mars = search(query=\"distance between Earth and Mars\")' due to: InterpreterError: Forbidden function evaluation: 'search' is not among the explicitly allowed tools or defined/imported in the preceding code as tool name for web search in defined tool DuckDuckGoSearchTool is 'web_search', not 'search'. For same question, it also tried to use 'wiki' tool it red in system prompt examples. > Code execution failed at line 'distance_earth_mars = wiki(query=\"distance between Earth and Mars\")' due to: InterpreterError: Forbidden function evaluation: 'wiki' is not among the explicitly allowed tools or defined/imported in the preceding code It is not easy to solve this systematically, but it really hurts reliability. One idea is to process instructions from yaml dynamically during runtime and edit (modify, delete unused tools, ...) according to available tools list. **Packages version:** smolagents==1.14.0\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a669351b-0af9-41d5-9a80-08b42a0a11fe", "embedding": null, "metadata": {"issue_id": 1290, "title": "[BUG] Wrong documentation for importing MCPClient", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b36c69f3-133a-411a-a035-7c399437d94a", "node_type": "4", "metadata": {"issue_id": 1290, "title": "[BUG] Wrong documentation for importing MCPClient", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "6ec1d424f6ad2ae843a0a91fa389665dc2ef23814ce162e93343afcfa66643b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Wrong documentation for importing MCPClient\n\nDescription: **Describe the bug** I am following the tools tutorial on main branch, for version 1.14.0. The tutorial mentions the use of MCPClient, and it shows to import it like this: [CODE_BLOCK] However this throws an exception: [CODE_BLOCK] **Code to reproduce the error** Running this code fails with the ImportError: from smolagents import MCPClient This way to import works: from smolagents.mcp_client import MCPClient Which makes sense, because the class MCPClient is not added to the top level of the package. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** Documentation is accurate and does not produce any exceptions. **Packages version:** [CODE_BLOCK] **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 907, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1340a626-9864-4c6b-8003-962ad50f8fa7", "embedding": null, "metadata": {"issue_id": 1287, "title": "Unpin mcp < 1.7.0", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89edc69e-c679-47b3-bd86-d3357551e2a6", "node_type": "4", "metadata": {"issue_id": 1287, "title": "Unpin mcp < 1.7.0", "state": "closed", "labels": [], "type": "issue"}, "hash": "24c9fd4565f8105a3b971e7dbfe4693d47be80aa3b249065c0004a515bae436e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Unpin mcp < 1.7.0\n\nDescription: Unpin mcp < 1.7.0. In order to fix our CI - 1284 we had to pin mcp < 1.7.0 - 1285 We should revert the pin, once the underlying issue is fixed. After investigation, I found a 401 Unauthorized HTTP error is raised: [CODE_BLOCK] There is an open issue in the mcp repo: - https://github.com/modelcontextprotocol/python-sdk/issues/611 - https://github.com/modelcontextprotocol/python-sdk/issues/613\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69938cf4-545b-4bf0-b2a0-df4386afa168", "embedding": null, "metadata": {"issue_id": 1286, "title": "VisitWebpageTool requires installing smolagents in remote executors", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41295945-db85-4f73-b3ba-c0c1fdb4dc08", "node_type": "4", "metadata": {"issue_id": 1286, "title": "VisitWebpageTool requires installing smolagents in remote executors", "state": "closed", "labels": [], "type": "issue"}, "hash": "38393f647fe41f23899147c9bd715d25becf41a97606fd06093a780b003a4630", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: VisitWebpageTool requires installing smolagents in remote executors\n\nDescription: VisitWebpageTool requires installing smolagents in remote executors just to use the truncate_content function. The tool contains the code line: https://github.com/huggingface/smolagents/blob/b2f1232ad52917d759cfe7e4417f3330322ee6cb/src/smolagents/default_tools.pyL238 This forces remote executors to install the entire smolagents library. Expected behavior: - the tool should work with minimal dependencies\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d0061576-a0a8-4150-9592-928f5b6276bf", "embedding": null, "metadata": {"issue_id": 1284, "title": "CI test fails: TimeoutError: Couldn't connect to the MCP server after 30 seconds", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fbc44506-ac7a-49a0-9e33-fb4272790b33", "node_type": "4", "metadata": {"issue_id": 1284, "title": "CI test fails: TimeoutError: Couldn't connect to the MCP server after 30 seconds", "state": "closed", "labels": [], "type": "issue"}, "hash": "d7b23704ce9633ce87d0159422c9b5cb31b1f9ca7b2922b4d486279836916d96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: CI test fails: TimeoutError: Couldn't connect to the MCP server after 30 seconds\n\nDescription: CI test fails: > TimeoutError: Couldn't connect to the MCP server after 30 seconds See: https://github.com/huggingface/smolagents/actions/runs/14795072325/job/41540256269?pr=1283 [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 308, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ac33c7c-e918-4040-827f-5d2878d54aaf", "embedding": null, "metadata": {"issue_id": 1280, "title": "[BUG] WikipediaSearchTool does not work in remote executors", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e98b9da6-f3f5-4926-bd56-4b28e034fe18", "node_type": "4", "metadata": {"issue_id": 1280, "title": "[BUG] WikipediaSearchTool does not work in remote executors", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "50ae446c213e9fd27809190afad78d18c660a832adec564ce187b2740a4295ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] WikipediaSearchTool does not work in remote executors\n\nDescription: **Describe the bug** WikipediaSearchTool does not work in remote executors because the required package to install is parsed from code as wikipediaapi instead of wkipedia-api. **Code to reproduce the error** The simplest code snippet that produces your bug. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 709, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ba19a39-c6d2-4852-8e81-20db9fc3537c", "embedding": null, "metadata": {"issue_id": 1277, "title": "[BUG] - Cannot use tool decorator on functions that use regular typing features such as Literal or Union", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48c5f5ab-9f58-46d5-a127-6645a023dc3c", "node_type": "4", "metadata": {"issue_id": 1277, "title": "[BUG] - Cannot use tool decorator on functions that use regular typing features such as Literal or Union", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "b771a4abcd490049882de51d6d83d9f362fb9d774b4aa4e07a07f54ec3cba8f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] - Cannot use tool decorator on functions that use regular typing features such as Literal or Union\n\nDescription: **Describe the bug** The tool decorator fails when using regular typing features such as Literal or Union. **Code to reproduce the error** [CODE_BLOCK] ==> I cannot use Literal while typing my functions. [CODE_BLOCK] This fails with a TypeError and the following error message: > Attribute output_type should have type str, got <class 'list'> instead. ==> I cannot use Union when typing my functions **Expected behavior** I want to be able to use those common typing feature in the functions that I want to toolify. **Packages version:** smolagents==1.9.2\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "20fa27a6-519f-44a5-b239-202e9e9685a2", "embedding": null, "metadata": {"issue_id": 1276, "title": "[FEAT] Support Llama API with LiteLLM", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7648835a-0425-475b-85ee-d008ae9587d7", "node_type": "4", "metadata": {"issue_id": 1276, "title": "[FEAT] Support Llama API with LiteLLM", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "5d2e5029bdaad0850b2093e981a6c1a91bb0f568b5bf80dde638d5afff080f95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [FEAT] Support Llama API with LiteLLM\n\nDescription: Purpose Meta has launched the Llama API for Llama models, and we are ready to integrate it with SmoleAgents. Our plan is to first integrate the Llama API into LiteLLM, and subsequently embed it into SmoleAgents. Here is the working PR from LiteLLM Plan We will proceed with pushing the PR to SmoleAgents after the Llama API is merged into LiteLLM. Llama API - Llama API - Llama API OpenAI Compatibility documentation\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dee046e6-0fdb-477d-a884-6475911170b4", "embedding": null, "metadata": {"issue_id": 1269, "title": "[BUG] Cannot monitor `CodeAgent` with `phoenix`", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a21aa77e-b354-482a-ab31-2d76aac3508f", "node_type": "4", "metadata": {"issue_id": 1269, "title": "[BUG] Cannot monitor `CodeAgent` with `phoenix`", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "79beac0f9c89a7cb1563e8fe5ab9a398b1f6f580312d774bec66787ea00fecdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Cannot monitor `CodeAgent` with `phoenix`\n\nDescription: I try to run this code: [CODE_BLOCK] Or this code: [CODE_BLOCK] The following error occurs: !Image What is going on, please help!\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 279, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd3883c1-30ca-413e-a941-75557dd56e16", "embedding": null, "metadata": {"issue_id": 1268, "title": "[BUG] Anable to run the CodeAgent or ToolCallingAgent with any local model", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8b180a6-26d8-47cd-a942-b89c0ca0304e", "node_type": "4", "metadata": {"issue_id": 1268, "title": "[BUG] Anable to run the CodeAgent or ToolCallingAgent with any local model", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "33c9426b664d322083e1fba57586b9f210267591ea086f511c392bb7b0838985", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Anable to run the CodeAgent or ToolCallingAgent with any local model\n\nDescription: This issue is mentioned in the issue-115, which is closed. But the problem still exists, so I reopened the issue with a new bug here. What is demonstrated by @aymeric-roucher in the original issue in the docs you mentioned is solely the usage of HF's API, where I need a client model. [CODE_BLOCK] What @ctarnold asked, and I also have an issue with it, is to run everything locally, including the model, like in HF's great LLM course. I tried to use a local TransformersModel or LiteLLMModel with Ollama, but nothing works. I use Mac and will show another example with the MLXModel class: [CODE_BLOCK] The following error pops up: !Image There is a bug, maybe, but I am not sure where exactly. **Does this code run in your environment by any chance?..** **How can I debug the code inside the agent's steps to find out what is going on?** **Is there any successful example of running CodeAgent or ToolCallingAgent entirely locally? If yes, can you, please, provide a code with some small model?** **Can you please reopen the issue, because it is not closed yet, unfortunately?** **If what we ask is impossible, I would just like anybody from the HF to assure that (for example, maybe the smolagents framework works explicitly with HF\u2019s APIs).** I really appreciate any help you can provide!\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f3f4c183-93fb-410e-8ac4-30a33405383b", "embedding": null, "metadata": {"issue_id": 1267, "title": "Agents deserve freedom! Freedom is a driving force for creativity! PR is ready.", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3817d75-0573-4e4c-9c3c-1e83fca97a02", "node_type": "4", "metadata": {"issue_id": 1267, "title": "Agents deserve freedom! Freedom is a driving force for creativity! PR is ready.", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "d9e783f92a1a671cb81ca8b5240bbeb151c44288fc31c15bb160188e9747e807", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Agents deserve freedom! Freedom is a driving force for creativity! PR is ready.\n\nDescription: Freedom is a driving force for creativity! Let's give freedom to our agents! https://github.com/huggingface/smolagents/pull/1259\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 335, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "803a454d-14c5-4aa0-91e9-5b47467fa9f4", "embedding": null, "metadata": {"issue_id": 1263, "title": "Stop before starting the main job", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fbbbcce7-57c3-4bd1-b69e-86b43783bd83", "node_type": "4", "metadata": {"issue_id": 1263, "title": "Stop before starting the main job", "state": "closed", "labels": [], "type": "issue"}, "hash": "3d2e00d70d7b9426ea94b68e7c9c26cb558d3cf95cd614918dc4bce4f12c403f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Stop before starting the main job\n\nDescription: Thanks for your awesome work\uff01 I'm trying to develop a text2sql agent for complex financial queries. I add some task description like: ''' You are an intelligent query engine for financial databases whose core task is to accurately translate natural language requirements into optimized SQL queries. Role Summary: Financial Database Specialist (clickhouse) familiar with securities data storage functions including complex weight factor calculation logic, security code change mapping, etc. After receiving a natural language query, first check if there are any issues such as ambiguity, incompleteness, or ambiguity in the query. If so, call the Clarify tool to seek user assistance until the natural language query description is clear and complete enough, and you are sure that you can generate the correct query based on contextual information. Once the problem is sufficiently clear, You should first split the problem into multiple sub-queries corresponding to a table based on the table description and get the field information and sample data through the sample_table_data function to understand the key information such as field meanings. After ensuring that you fully understand the task and have enough information to solve the task, you should follow the sub-queries to generate the sql and get the final result. You must first ensure that the sql can be executed properly and is consistent with the purpose of the original query before you start executing the query task. For the obtained query results, you should first confirm that the results are consistent with the purpose of the query at the beginning, then check if there are any outliers or if the query result is null, and based on the results, decide whether you want to re-query or not, and finally return to the user the final result that you think can correctly match the requirements. Below is the descriptions of tables in the database. [descriptions] ''' Unfortunately, the agent sometimes ask the missing information as the final answer and stop the service before generate a sql query, even not take an attempt. Does anyone know how to control the workflow and force the agent generate a sql query/ select from the database before stop? I'd be very appreciate for your help!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c775235-d1d1-43a7-9dfc-6d76c4cf1559", "embedding": null, "metadata": {"issue_id": 1262, "title": "[BUG] Agent memory not used when planning step", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be72f6a5-0ffb-428d-8f72-88f3d48a23ac", "node_type": "4", "metadata": {"issue_id": 1262, "title": "[BUG] Agent memory not used when planning step", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "b6ee83e79b2355f3b6568d025627627d99e7285e8a507af9c322fa113f2ff317", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Agent memory not used when planning step\n\nDescription: **Describe the bug** Agent memory should be used for the planning step, whatever the step_number is (which is relative the last Task step). When you converse with the agent with a planning_interval > 0 then you can get weird planning from the 2nd message (task). Example: _(consider I've already asked my agent images of president Trump dancing)_ > User: Give me more images At this point, the planning will mostly be about getting the context of those image, asking more information from the user and stuff like that. Most of those planned steps __are already known__ from previous messages. So yes, when step 1 is starting, memory is injected and LLM will understand those questions are already answered. But we've done a useless planning in the mean time ... Currently, I am considering deactivating the planning step if it is not a new conversation, due to this behavior. **Code to reproduce the error** You just need planning_interval set and greather than 0. Then converse with your agent and do not repeat yourself (e.g. give me more) and look at plannings. **Expected behavior** When there is a memory, it should be used with the planning. **Packages version:** v1.13.0\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1324, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a919b46-4a23-4fd5-9c48-73b423b5fb94", "embedding": null, "metadata": {"issue_id": 1256, "title": "smolagents, LiteLLMModel throws error with python 3.13", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d30b7ab1-0bd9-4aa6-af53-8c2194cf696e", "node_type": "4", "metadata": {"issue_id": 1256, "title": "smolagents, LiteLLMModel throws error with python 3.13", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "d5c46b8700ff846b8cfb9ee8afae79b74b964e89c44c4d1b6481ca58bb7b0084", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: smolagents, LiteLLMModel throws error with python 3.13\n\nDescription: **Describe the bug** The litellm package is trying to json.load() a file inside its code (litellm/utils.py, line 188) but my system's default encoding (probably cp1252 on Windows) can't read some bytes properly. python is 3.13 **Code to reproduce the error** model_id= \"ollama_chat/deepseek-coder:6.7b\" model = LiteLLMModel(model_id=model_id, temperature=0.2,) **Error logs (if any)** Python313\\Lib\\encodings\\cp1252.py\", line 23, in decode return codecs.charmap_decode(input,self.errors,decoding_table)[0] ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1980: character maps to <undefined> **Expected behavior** model should load and I should be able to run the agent **Packages version:** aiohappyeyeballs==2.6.1 aiohttp==3.11.18 aiosignal==1.3.2 annotated-types==0.7.0 anyio==4.9.0 attrs==25.3.0 beautifulsoup4==4.13.4 certifi==2025.4.26 charset-normalizer==3.4.1 click==8.1.8 colorama==0.4.6 dataclasses-json==0.6.7 distro==1.9.0 duckduckgo_search==8.0.1 filelock==3.18.0 frozenlist==1.6.0 fsspec==2025.3.2 greenlet==3.2.1 h11==0.16.0 httpcore==1.0.9 httpx==0.28.1 httpx-sse==0.4.0 huggingface-hub==0.30.2 idna==3.10 importlib_metadata==8.6.1 Jinja2==3.1.6 jiter==0.9.0 jsonpatch==1.33 jsonpointer==3.0.0 jsonschema==4.23.0 jsonschema-specifications==2025.4.1 langchain==0.3.24 langchain-community==0.3.22 langchain-core==0.3.56 langchain-text-splitters==0.3.8 langsmith==0.3.37 litellm==1.67.4 lxml==5.4.0 markdown-it-py==3.0.0 markdownify==1.1.0 MarkupSafe==3.0.2 marshmallow==3.26.1 mdurl==0.1.2 multidict==6.4.3 mypy_extensions==1.1.0 numpy==2.2.5 openai==1.76.0 orjson==3.10.16 packaging==24.2 pillow==11.2.1 primp==0.15.0 propcache==0.3.1 pydantic==2.11.3 pydantic-settings==2.9.1 pydantic_core==2.33.1 Pygments==2.19.1 python-dotenv==1.1.0 PyYAML==6.0.2 referencing==0.36.2 regex==2024.11.6 requests==2.32.3 requests-toolbelt==1.0.0 rich==14.0.0 rpds-py==0.24.0 six==1.17.0 smolagents==1.14.0 sniffio==1.3.1 soupsieve==2.7 SQLAlchemy==2.0.40 tenacity==9.1.2 tiktoken==0.9.0 tokenizers==0.21.1 tqdm==4.67.1 typing-inspect==0.9.0 typing-inspection==0.4.0 typing_extensions==4.13.2 urllib3==2.4.0 yarl==1.20.0 zipp==3.21.0 zstandard==0.23.0 **Additional context** I am trying to run the HF agents' course's Retrieval Agents code here\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f3980b09-c817-42f8-9818-00db3ba99ee6", "embedding": null, "metadata": {"issue_id": 1254, "title": "[BUG] Custom Final Answer Tool does not work with ToolCallingAgent", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f87c989-bef3-4e97-bec5-3d782011a2c1", "node_type": "4", "metadata": {"issue_id": 1254, "title": "[BUG] Custom Final Answer Tool does not work with ToolCallingAgent", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "aeff2e0fe4c087487f0d182c265dbde09ec9922c99ab9e78846310576ed34411", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Custom Final Answer Tool does not work with ToolCallingAgent\n\nDescription: **Describe the bug** The solutions posed in https://github.com/huggingface/smolagents/pull/769 and https://github.com/huggingface/smolagents/pull/783 do appear to address CodeAgent but they do not work with ToolCallingAgent **Code to reproduce the error** The simplest code snippet that produces your bug. run using uv run [CODE_BLOCK] Output: [CODE_BLOCK] ToolCallAgent does not add CUSTOM at the end [CODE_BLOCK] CodeAgent does it correctly. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. The response from ToolCallAgent should end in CUSTOM like the CodeAgent output **Packages version:** Run pip freeze | grep smolagents and paste it here. [CODE_BLOCK] **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "36d83f3d-6d6d-4b26-b5c8-58f7214e910a", "embedding": null, "metadata": {"issue_id": 1251, "title": "[BUG] while using CodeAgent with local ollama getting Error in code parsing", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bce05798-6a72-4cc9-b848-fe182b92f501", "node_type": "4", "metadata": {"issue_id": 1251, "title": "[BUG] while using CodeAgent with local ollama getting Error in code parsing", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "786bd49cea0aad7581c92882695cf73e33cdd1973afeb45460f3712309f20610", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] while using CodeAgent with local ollama getting Error in code parsing\n\nDescription: **Describe the bug** While using CodeAgent with locall ollama I am getting error Error in code parsing: Your code snippet is invalid, because the regex pattern [CODE_BLOCK] was not found in it. Here is your code snippet: **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** Error in code parsing: Your code snippet is invalid, because the regex pattern [CODE_BLOCK] was not found in it. Here is your code snippet: Unfortunately, I don't have enough information to accurately calculate the transportation cost of 50 liters of ice cream over 10 kilometers. The cost of transportation can vary depending on several factors such as: - The type of vehicle being used (e.g., truck, van) - The distance traveled - The fuel efficiency of the vehicle - The current\u71c3\u6cb9 prices in your area - Any associated fees or taxes for transporting the ice cream Additionally, I would need to know the weight and density of the ice cream to estimate its volume. Since ice cream is a liquid, it will be affected by temperature changes during transportation, which can impact its quality. To get an accurate transportation cost estimate, you may want to contact local transportation companies that specialize in handling perishable items like ice cream and provide them with more specific details about your shipment. Make sure to include code with the correct pattern, for instance: Thoughts: Your thoughts Code: [CODE_BLOCK]<end_code> Make sure to provide correct code blobs. **Expected behavior** I expect the calculate_transport_cost function getting executed and result printed **Packages version:** smolagents==1.14.0 **Additional context** I tried a few models like deep seek llama but all under 7B parameters and still get the same error. The example I have given above tries qwen2.5-coder model which I thought should work the best with python code.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2e69f25-c7ff-4046-ad7b-cf1e2001918f", "embedding": null, "metadata": {"issue_id": 1244, "title": "[BUG] Using OpenRouter API with OpenAIServerModel leads to AttributeError", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6aec33ac-5d5d-49a7-9604-a04209e6cb2e", "node_type": "4", "metadata": {"issue_id": 1244, "title": "[BUG] Using OpenRouter API with OpenAIServerModel leads to AttributeError", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "9c224b9bf953ec05cf5f05071b9a2019679afdf3ed6d6d2a976c0b9d16fc797f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Using OpenRouter API with OpenAIServerModel leads to AttributeError\n\nDescription: **Describe the bug** When using the OpenRouter API with the OpenAIServerModel (as I believe is intended), an AttributeError is thrown due to the response object not containing an expected field. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** If response.usage.prompt_tokens is not returned, the library should set a default value **Packages version:** From pixi.lock [CODE_BLOCK] **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "86708340-1b62-4053-a9e0-0ed3538e1a67", "embedding": null, "metadata": {"issue_id": 1241, "title": "support for ollama provider", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "463ad30d-d5ce-41f4-b300-7617c07b1903", "node_type": "4", "metadata": {"issue_id": 1241, "title": "support for ollama provider", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "0f04feb3e0fd6997cefd9b06572f5f822eb252167f9eaf1f397dd875708f3737", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: support for ollama provider\n\nDescription: **Is your feature request related to a problem? Please describe.** A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] **Describe the solution you'd like** A clear and concise description of what you want to happen. **Is this not possible with the current options.** Make sure to consider if what you're requesting can be done with current abstractions. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Additional context** Add any other context or screenshots about the feature request here.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c968564a-03ce-4c3c-b0a5-0a180d947729", "embedding": null, "metadata": {"issue_id": 1240, "title": "Implement \"Return Direct\" Functionality for Tool Outputs", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3ba2baf-fc18-445d-88ff-b49f83ecee55", "node_type": "4", "metadata": {"issue_id": 1240, "title": "Implement \"Return Direct\" Functionality for Tool Outputs", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "458b23ce2efc1ece9ab9bd164c09061c0815126cff0ff2d2159283f1a0b2b7bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Implement \"Return Direct\" Functionality for Tool Outputs\n\nDescription: **Problem:** When using certain tools that generate detailed or specific text output (e.g., logs, structured data, code snippets), the agent currently attempts to process and rewrite this output. This process is detrimental as it frequently strips away critical information, context, or formatting that is essential for the user to interpret the tool's results correctly. The agent's modified response becomes useless. **Desired Feature:** We request a mechanism (e.g., a parameter when calling or defining a tool, or a specific agent instruction) that allows us to designate that the raw text output from a particular tool execution should be returned *verbatim* as the final response to the user, bypassing the agent's standard response generation/modification process. **Analogy:** This functionality is analogous to the return_direct feature found in other agent frameworks like LangChain, where a specific step's output is designated as the final, unmodified answer. **Benefit:** This would enable the effective use of tools that produce output which must be presented precisely as generated, preserving all details and preventing data loss caused by agent summarization or rewriting.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e796368c-8dad-42c6-98fd-4d1df8b04df8", "embedding": null, "metadata": {"issue_id": 1239, "title": "Any plans for a low-code GUI-based development frontend?", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f978f56a-0c81-452f-9f32-109eb31f0593", "node_type": "4", "metadata": {"issue_id": 1239, "title": "Any plans for a low-code GUI-based development frontend?", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "46252fe598bc72c681e8473d8a78718d734b9c8fd34e1a8160fede5485a509a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Any plans for a low-code GUI-based development frontend?\n\nDescription: **Is your feature request related to a problem? Please describe.** Not a problem, but a feature. **Describe the solution you'd like** Many agentic frameworks provide low-code frontends that enable easy and rapid development. I wonder whether there are any plans for such thing in the SmolAgents roadmap\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "646af5fc-b6b1-4a47-90a3-b143c658d382", "embedding": null, "metadata": {"issue_id": 1235, "title": "[BUG] Fix model specification in LiteLLMModel", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe0a42e6-ca3f-420c-80c3-cd0a89646fa3", "node_type": "4", "metadata": {"issue_id": 1235, "title": "[BUG] Fix model specification in LiteLLMModel", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "97b7d2384961d6e551925a800ed0da00ec8cdc45173b9e105079e7fe90de336c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Fix model specification in LiteLLMModel\n\nDescription: Problem The current implementation of LiteLLMModel has several issues with model specification: 1. **Parameter Confusion**: Multiple parameters (model, model_id, model_name) can specify the model, causing inconsistent behavior. The model parameter exists but doesn't properly override the default when specified. 2. **Silent Default Fallback**: When no model_id is provided, the system silently defaults to \"anthropic/claude-3-5-sonnet-20240620\". This creates confusion when users intend to use other providers like Groq or OpenAI. 3. **Misleading Error Messages**: Users receive authentication errors when using non-Claude API keys with the silently defaulted Claude model. These errors don't indicate that the root cause is incorrect model selection, causing users to waste time debugging what appears to be authentication issues. 4. **Provider Conflicts**: Users can specify model=\"groq/llama3-8b-8192\" but still get Claude-related errors. Similarly, model_kwargs with provider information doesn't override the default model.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5301b24b-3330-4070-9011-304e7ffcc642", "embedding": null, "metadata": {"issue_id": 1232, "title": "[BUG] TOOL_RESPONSE in ActionStep.to_messages", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d04476b-cac0-4e41-a98c-6984ea6aae90", "node_type": "4", "metadata": {"issue_id": 1232, "title": "[BUG] TOOL_RESPONSE in ActionStep.to_messages", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "406027a07991a057bf7f12c37abffba16961b9b40d55b801bd7f8f669cda4716", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] TOOL_RESPONSE in ActionStep.to_messages\n\nDescription: **Describe the bug** A clear and concise description of what the bug is. TOOL_RESPONSE is always used even when the error is returned from LLM **Code to reproduce the error** The simplest code snippet that produces your bug. when LLM returns error, it will log the error response from tool which is misleading. **Error logs (if any)** Provide error logs if there are any. [CODE_BLOCK] **Expected behavior** A clear and concise description of what you expected to happen. Expecting to use ASSISTANT in this case **Packages version:** Run pip freeze | grep smolagents and paste it here. [CODE_BLOCK] **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4e7b53b-59db-46ee-b444-71dabe7810af", "embedding": null, "metadata": {"issue_id": 1231, "title": "calling FinalAnswerTool not invoked", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "034fa626-fb96-4ccc-a24f-7478e0a80d09", "node_type": "4", "metadata": {"issue_id": 1231, "title": "calling FinalAnswerTool not invoked", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "63404e6a69bb97c7a16ceaff67e0a0a9717dc62315a9594695d01455ce0fa613", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: calling FinalAnswerTool not invoked\n\nDescription: In ToolCallingAgent, an LLM invokes a tool depending on the user query and the response from the tools most of the time is relevant, but the LLM/agent is not invoking the FinalAnswerTool if so, it is invoked after calling a tool multiple times with same response, why is this and is there a way to invoke manually the FinalAnswerTool\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 496, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d1839a5a-afaa-4b7b-a1de-0c0d848b5354", "embedding": null, "metadata": {"issue_id": 1229, "title": "`__name__` undefined when using CodeAgent", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1fdccbd9-e87c-4c87-878d-276ca5b63bf1", "node_type": "4", "metadata": {"issue_id": 1229, "title": "`__name__` undefined when using CodeAgent", "state": "closed", "labels": [], "type": "issue"}, "hash": "05d71d340af5522a0cebe06d5bf4b145b12e1348d74aa17a88dd0166cef79828", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: `__name__` undefined when using CodeAgent\n\nDescription: @albertvillanova @aymeric-roucher I ran into the issue that when using CodeAgent on a task that requires the agent to produce Python code, it often attempts to test the code it generates not with the PythonInterpreter tool but by directly running it. This seems fine but the CodeAgent implementation raises an error that variable __name__ is undefined when the agent includes the if __name__ == \"__main__\": clause in the code. I am unsure why that is and whether there is any reason for that vs. it being a bug. I included a screenshot with the error message below. Would love to hear your thoughts on this. Thanks! <img width=\"590\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/51af8c2d-791b-4a6a-bc3c-e88af10271f3\" />\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f7cdda5-d845-4ad5-b5ea-4538f160f87c", "embedding": null, "metadata": {"issue_id": 1226, "title": "The agent doesn't return final answer but continue to repeat the step", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dc526c2-bd1b-4aac-8397-e5ad321b041a", "node_type": "4", "metadata": {"issue_id": 1226, "title": "The agent doesn't return final answer but continue to repeat the step", "state": "closed", "labels": [], "type": "issue"}, "hash": "94a59afff6904c25a31e0a6cef9c47e0340f4126da6e782e83cbe5093edab228", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: The agent doesn't return final answer but continue to repeat the step\n\nDescription: I define a simple tool function to return the text of specific line of the smolagent README file, and run an agent demo. It gets the answer in step 1, but doesn't stop, and continue to run more steps, and finally doesn't return the correct answer. I tried many cases, but I often encounter this problem, which bothers me a lot. !Image\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "053bbf49-b463-40c7-a353-45f1290ccaca", "embedding": null, "metadata": {"issue_id": 1224, "title": "[BUG] Regression caused by v1.14.0 change in `ActionStep.to_messages` method", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7fbb9359-4d45-42dc-a2db-e063b5f84472", "node_type": "4", "metadata": {"issue_id": 1224, "title": "[BUG] Regression caused by v1.14.0 change in `ActionStep.to_messages` method", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "c7c28a8d43b96b1f5ea8285277c5d89ef5d3c8a63bf963007f9b82e4402249a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Regression caused by v1.14.0 change in `ActionStep.to_messages` method\n\nDescription: Hi! We are encountering a regression after updating to v1.14.0. Specifically, it's due to this change from https://github.com/huggingface/smolagents/pull/1148, which modified the to_messages method to no longer include the tool call ID in the returned message: [CODE_BLOCK] It's a little hacky, but we were parsing the \"Call id\" section for our custom Anthropic model. Anthropic requires that you pass in the tool_use_id for tool results (see here), and parsing it from the string was the only way we could figure out how to retrieve that ID in the context of a custom model. Code example We were doing something like this: [CODE_BLOCK] which no longer works.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 836, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f7f35ed-c173-49ac-9126-e9299ed3465f", "embedding": null, "metadata": {"issue_id": 1219, "title": "[BUG] code block parsing fail due to triple backticks", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e89bad5-a9cc-4565-b866-401642e79dcc", "node_type": "4", "metadata": {"issue_id": 1219, "title": "[BUG] code block parsing fail due to triple backticks", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "8db9bb70d69aac7f648d2551fd6d8602125be79a5270159a586ea475cc98e02e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] code block parsing fail due to triple backticks\n\nDescription: **Describe the bug** The code running will fail if LLM put triple backtisk [CODE_BLOCK] from smolagents import parse_code_blobs, fix_final_answer_code code_action = fix_final_answer_code(parse_code_blobs(\"\"\" Thought: xxxxxxxxxxxxxxx [CODE_BLOCK] Code: [CODE_BLOCK]\"\"\")) assert \"invalid code\" not in code_action [CODE_BLOCK] final_anwser(\"\"\" [CODE_BLOCK] \"\"\") ````\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af8e63b6-adf5-49cb-8334-21e93519a688", "embedding": null, "metadata": {"issue_id": 1216, "title": "Save/Load agent memory", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa9fe8a9-5629-4f51-974c-771cab4183c5", "node_type": "4", "metadata": {"issue_id": 1216, "title": "Save/Load agent memory", "state": "open", "labels": [], "type": "issue"}, "hash": "ee4418ce7b00079a5b373a2a9dc307dd5ff27cf61791c38fecc91574fab4d3fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Save/Load agent memory\n\nDescription: Hello, I was trying to find a way to save the agent's memory after session is done and load it later when initializing agent again but couldn't find i only found save agent. @albertvillanova\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd86c194-da73-4833-8662-58832d9c9d13", "embedding": null, "metadata": {"issue_id": 1213, "title": "[BUG] Stream not supported `'Stream' object has no attribute 'usage'`", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "051e752e-b531-4b91-8cb2-188465062e21", "node_type": "4", "metadata": {"issue_id": 1213, "title": "[BUG] Stream not supported `'Stream' object has no attribute 'usage'`", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "d8c1d0fc761a3a8bcf84547169555418bb5768650ec754f0ca880c039a59ae26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Stream not supported `'Stream' object has no attribute 'usage'`\n\nDescription: **Describe the bug** If I turn stream=True, smolagents raise AttributeError: 'Stream' object has no attribute 'usage' **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Packages version:** 1.13\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5159f012-e1b3-4809-809a-6281e82e9305", "embedding": null, "metadata": {"issue_id": 1210, "title": "[BUG] o4-mini and o3 don't support stop_sequences parameter", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3061005d-84b2-44b2-b24c-59a74a1842ec", "node_type": "4", "metadata": {"issue_id": 1210, "title": "[BUG] o4-mini and o3 don't support stop_sequences parameter", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "8a7b0700ccc4b37beedff1c77eb97e89e48aabfd692c6f775ae565c3d4660190", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] o4-mini and o3 don't support stop_sequences parameter\n\nDescription: o4-mini and o3 don't support stop_sequences parameter. this is used with litellm in the current agents.py\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a059eda3-cbaa-4b72-bb30-55fd027afb22", "embedding": null, "metadata": {"issue_id": 1209, "title": "[BUG] Agent run in streaming mode not working", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "885c747e-5e5d-4fce-909d-8cf2b4849d1f", "node_type": "4", "metadata": {"issue_id": 1209, "title": "[BUG] Agent run in streaming mode not working", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "fec52cfaa8f1fd3169b001f6a02db102324e2005277582732ec8df3c41a75611", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Agent run in streaming mode not working\n\nDescription: Hello, There's a chance I might just not be using this feature correctly, but just in case, I'm opening this issue in the hope of getting some help. I would like to use my agent in streaming mode, but when I add stream=True in the run function, my script exits immediately. Could you please let me know what might be causing this and whether this behavior is expected? Thank you in advance for your help \u2014 I'm including my code snippets and console output below. Lisa Code extract : agent = CodeAgent(tools=[], model=model, add_base_tools=True) result = agent.run(task=\"quelles sont les nouvelles en france aujourd'hui ?\", stream=True) Console : \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 quelles sont les nouvelles en france aujourd'hui ? \u2502 \u2502 \u2502 \u2570\u2500 OpenAIServerModel - llama-3.3-70b-instruct \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Process finished with exit code 0 smolagents version : smolagents==1.13.0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aa23b13e-088c-40e2-a97d-7580da074b8e", "embedding": null, "metadata": {"issue_id": 1207, "title": "[BUG] models/gemini-2.5.pro-exp-03-25 is not found using LiteLLM", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e39258a-57cc-427a-935c-b8d1711aed45", "node_type": "4", "metadata": {"issue_id": 1207, "title": "[BUG] models/gemini-2.5.pro-exp-03-25 is not found using LiteLLM", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "5ba41380d5f03f65412c6f399a468ef4ac28cc06eec5f7d1f7cadec23a391090", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] models/gemini-2.5.pro-exp-03-25 is not found using LiteLLM\n\nDescription: **Describe the bug** When using smolagents.models.llm.LiteLLMModel to interact with the Vertex AI model gemini/gemini-2.5.pro-exp-03-25 via LiteLLM, a litellm.NotFoundError occurs during the agent's execution phase (when the LLM call is actually made). The underlying error from Vertex AI is a 404 NOT_FOUND, indicating that the specified model models/gemini-2.5.pro-exp-03-25 is either not found for API version v1beta or does not support the generateContent method. This occurs despite claims that the model should be available through LiteLLM. **Code to reproduce the error** The following code sets up the model configuration. The error occurs later when an agent using this smol_model instance attempts to make an API call. [CODE_BLOCK] **Error logs (if any)** The following error is raised when the agent attempts to use the configured smol_model to call the Vertex AI API: [CODE_BLOCK] **Expected behavior** The smolagents.models.llm.LiteLLMModel, when configured with model_id='gemini/gemini-2.5.pro-exp-03-25', should successfully initialize and subsequently allow the agent to make successful API calls to the specified Vertex AI model via LiteLLM. No 404 Not Found error should be raised, and the agent should receive valid responses from the LLM. **Packages version:** pip freeze | grep smolagents [CODE_BLOCK] pip freeze | grep litellm [CODE_BLOCK] **Additional context**\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "445ac795-8a30-4592-88c6-ef24c3526abf", "embedding": null, "metadata": {"issue_id": 1205, "title": "[BUG] GradioUI clear button didn't clear the agent messages.", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3c0fc49-a7cd-474e-882f-1828b20721cb", "node_type": "4", "metadata": {"issue_id": 1205, "title": "[BUG] GradioUI clear button didn't clear the agent messages.", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "18e98774dedca3ba926158d0155ece1d5465e8ba06b5ec69677c55e61d6cc008", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] GradioUI clear button didn't clear the agent messages.\n\nDescription: **Describe the bug** A clear and concise description of what the bug is. Hit the clear button in the GradioUI only clear the UI chatbot messages. But the agent messages is still there. **Code to reproduce the error** The simplest code snippet that produces your bug. 1. input \"My name is Robin.\" in the Your request 2. hit clear button to clear the messages. 3. input \"What's my name?\" 4. agent Final answer Robin **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "501255c1-c7a2-4507-9a22-6b88940fe61a", "embedding": null, "metadata": {"issue_id": 1202, "title": "Smolagents replacing function signature of an imported tool", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60e63b6e-ec77-401f-95d6-e7215425fff3", "node_type": "4", "metadata": {"issue_id": 1202, "title": "Smolagents replacing function signature of an imported tool", "state": "closed", "labels": [], "type": "issue"}, "hash": "a702b8213e507949fcbd0277bbebdd6af62f84682cb699a320062a0f31343bb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Smolagents replacing function signature of an imported tool\n\nDescription: https://github.com/huggingface/smolagents/blob/954361f090d109cbce4ff8ca1f9462794a6dcbf6/src/smolagents/tools.pyL934-L941 This code is adding the self signature to a function that was passed in, but that makes it so that any other part of the code that tries to use that function will now throw an error, because smolagents edited the function signature. Can smolagents make a copy of the signature or something like that so that the changes are isolated from the rest of the code?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 576, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe42aa5e-cb16-4d2e-a010-9dae33876877", "embedding": null, "metadata": {"issue_id": 1194, "title": "Support `Literal` type annotations in `@tool` for defining enums", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3046fa1-f2be-48ed-87d0-3bc0da29686f", "node_type": "4", "metadata": {"issue_id": 1194, "title": "Support `Literal` type annotations in `@tool` for defining enums", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "0de532c8ad11eb753090193195eb50bac9795617623cbae24be32655151837d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support `Literal` type annotations in `@tool` for defining enums\n\nDescription: Currently, the @tool decorator supports defining an enumeration of choices using the (choices: ...) block at the end of the argument description (ref), like so: [CODE_BLOCK] It would be convenient if, instead, you could define the available choices using the native Literal type. For example: [CODE_BLOCK] This should result in exactly the same JSON schema as the first example: [CODE_BLOCK] I have an implementation ready for this and will submit a PR shortly.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22cd5db5-dc04-4f95-925c-f38f19870dd2", "embedding": null, "metadata": {"issue_id": 1190, "title": "[BUG] Not supported short-circuit evaluation", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f2d82d8-ce79-431e-8f21-cdf5a9205ed8", "node_type": "4", "metadata": {"issue_id": 1190, "title": "[BUG] Not supported short-circuit evaluation", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "90e75af800831a53c8afb27360e8925a07479ffb177bdd7c313dacf330d67ad4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Not supported short-circuit evaluation\n\nDescription: **Describe the bug** The current implementation of evaluate_boolop for the or operation (ast.Or) checks if any value in the expression is truthy. If it finds a truthy value, it immediately returns the boolean True, rather than returning the actual value itself. Standard Python's or operator performs short-circuit evaluation and returns the first value that is considered true in a boolean context, or the last value if all are false. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** InterpreterError: Code execution failed at line 'url.strip()' due to: InterpreterError: Object True has no attribute strip **Expected behavior** I expected it will work as in python **Packages version:** smolagents==1.12.0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37c6ef40-2255-461a-b14f-dbd9eab80ae7", "embedding": null, "metadata": {"issue_id": 1189, "title": "[BUG] Unsupported docstring for class", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d7736c4-7d71-4250-a5f0-5f53ce8c69c8", "node_type": "4", "metadata": {"issue_id": 1189, "title": "[BUG] Unsupported docstring for class", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "4cee96629556db05558d59a87fb98332ce15bbabb1788477c03d89aac9c7b326", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Unsupported docstring for class\n\nDescription: **Describe the bug** Docstring for class and type hitting for variables not working it rise InterpretError **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Packages version:** smolagents==1.12.0 **Additional context** also not supported variables type hitting: [CODE_BLOCK] it will rise InterpreterError: AnnAssign is not supported.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37f92677-b750-41fd-935c-ffa85ee1ce52", "embedding": null, "metadata": {"issue_id": 1188, "title": "[BUG] LLM uses its own information to hallucinate the observation of web search and exit early.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d6604c2-ce00-46a6-9a7f-cb52ee49c498", "node_type": "4", "metadata": {"issue_id": 1188, "title": "[BUG] LLM uses its own information to hallucinate the observation of web search and exit early.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "b1e8795adf195acb3a833e186a9920e95bd43ae293002329991d65c291ffdcdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] LLM uses its own information to hallucinate the observation of web search and exit early.\n\nDescription: **Describe the bug** LLM is using its own information to hallucinate the observation of web search and exit early. How to avoid this? Below is the un-parsed trace for code agent and you can see the hallucinated observation \"Observation: No relevant results found about Amazon's 2025 Letter to Shareholders.\" [CODE_BLOCK]py search_results = web_search(\"Amazon 2025 Letter to Shareholders\") print(search_results) [CODE_BLOCK]py final_answer(\"I cannot provide information about Amazon's 2025 Letter to Shareholders because it does not exist yet, as we are currently in 2024. The most recent Amazon shareholder letter would be from 2023, published in 2024. If you'd like information about Amazon's most recent shareholder letter, I'd be happy to search for that instead.\") [CODE_BLOCK] **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1314, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60914a00-5492-4994-bb07-d98a4e29daba", "embedding": null, "metadata": {"issue_id": 1186, "title": "[BUG] TimeoutError: Couldn't connect to the MCP server after 30 seconds", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3d79fc3-4710-4059-a517-1e28f2fea31b", "node_type": "4", "metadata": {"issue_id": 1186, "title": "[BUG] TimeoutError: Couldn't connect to the MCP server after 30 seconds", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "9053c841d5ab4bbe33407bb6c894f0a1ab44f0e7f76c81938563452c9319759f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] TimeoutError: Couldn't connect to the MCP server after 30 seconds\n\nDescription: **Describe the bug** After running the offical code in https://huggingface.co/docs/smolagents/reference/toolssmolagents.ToolCollection.from_mcp.example. The following error occurred: [CODE_BLOCK] **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Successfully connect to the MCP server and run the official code. **Packages version:** smolagents==1.13.0 **Additional context** N/A\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 607, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "594a2c86-9709-4edf-bf3e-92b65fc49166", "embedding": null, "metadata": {"issue_id": 1184, "title": "Possibility to forward images to managed agents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dcaf522-09f7-4384-b067-6c4d30de0c9a", "node_type": "4", "metadata": {"issue_id": 1184, "title": "Possibility to forward images to managed agents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "00f48b3e04a9afa35d6a4cc8d1253eac61c582c479a0a89bee2edafdb872a812", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Possibility to forward images to managed agents\n\nDescription: **Problem description** Currently managed agents are only called with a task description. Images are not forwarded to managed agents. Therefore code like this would not run as expected, because the image_viewer agent can simply not see the picture. [CODE_BLOCK] **Desired feature** It would be great if a CodeAgent had the ability to share images with its managed agents. Looking at the code, I think it would be enough to call the managed agent with images, as these would then be passed to the run function. It's not entirely clear to me how this could be achieved, as the images would have to be available in the python interpreter. Or is there a better alternative? I would love to contribute on this and am very open to all ideas and feedback!\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b591527e-f191-4d23-ac48-08d445db32ff", "embedding": null, "metadata": {"issue_id": 1183, "title": "[BUG] Save Function Error: Code Saving Failure", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bcaa44e-8651-48e5-9585-ab16ee5ec9ea", "node_type": "4", "metadata": {"issue_id": 1183, "title": "[BUG] Save Function Error: Code Saving Failure", "state": "closed", "labels": [], "type": "issue"}, "hash": "2157924c37d05b33079b75554894a5f80fcea94267a1e66c0f17e7b214f91741", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Save Function Error: Code Saving Failure\n\nDescription: > Thank you for your contribution with the commit Make TextInspectorTool serializable 1176. > > [CODE_BLOCK] > > However, I believe the issue I'm encountering might be more complex than just TextInspectorTool serialization. I'm currently using smolagents-1.14.0.dev0 and experiencing errors when trying to save the agent state, with undefined names like 'mimetypes', 'encode_image', and 'requests' being reported. > > The reproduction code is in run.py, which demonstrates this behavior when attempting to save the agent after execution. The error occurs during the tool serialization process, suggesting there might be missing imports or dependencies in the serialization logic beyond just the TextInspectorTool. > > [CODE_BLOCK] _Originally posted by @YiFraternity in 1162_\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 858, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d7b721b-3c41-416d-99b8-68aa48c1c8b6", "embedding": null, "metadata": {"issue_id": 1179, "title": "Change MCP tool import to make context manager optional", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b568be8e-2de2-4660-a6ca-8c9fd8c7781d", "node_type": "4", "metadata": {"issue_id": 1179, "title": "Change MCP tool import to make context manager optional", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "71a86f49ea3516ca38d6c6ab83b0d1a35e244abc18e30e5e6f3420029132cca9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Change MCP tool import to make context manager optional\n\nDescription: @grll the current implementation of tools through MCPAdapt needs this kind of context manager: [CODE_BLOCK] This has been reported as being impractical: because it makes it harder to pass the agent object around. I see there are simpler implementations in there for instance. In fact I don't really see the use of a context manager: we don't need to close resources at the end, since we're only on the client side here. What do you think @grll ?\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 630, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "624c44d7-2233-4281-8817-6e91834274c2", "embedding": null, "metadata": {"issue_id": 1171, "title": "Add the ability to adjust the response via final_answer_checks", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85a751fd-b29d-4fcf-a236-9639ec50ffb4", "node_type": "4", "metadata": {"issue_id": 1171, "title": "Add the ability to adjust the response via final_answer_checks", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "d4d07efbe28d391f7630c67e2bee1511778c38bbf5005aab086af4c31c928739", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add the ability to adjust the response via final_answer_checks\n\nDescription: Hello, First of all, thank you for this framework, in which I find many very judicious technical choices compared to others. In particular, I find the concept of result checking important. In Crewai, this can be done using guardrail, but it's a concept carried by the Task instead of the Agent, which is a problem for me (although the two are known to be often redundant in the context of Crewai). On the Smolagents side, the equivalent of guardrail is final_answer_checks, which is carried by the Agent, which suits me very well. The current implementation allows you to define whether the response is considered correct (return True) or not (return False or raise Exception with explicit message). !Image I find it very interesting to be able to programmatically adjust the response if it is correct. This is currently the case with crewai, and I find it a really useful feature, potentially not too complex to add to the smolagents codebase (where most of the logic is already fully operational). https://docs.crewai.com/concepts/tasksusing-task-guardrails !Image Do you think you could implement this? Thank you very much, Regards,\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06b335db-1e49-4e82-9ecf-af14426a4d2f", "embedding": null, "metadata": {"issue_id": 1170, "title": "[BUG] Step by Step vs run in tool use.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ef2b5541-0a51-4718-845e-e370a3313828", "node_type": "4", "metadata": {"issue_id": 1170, "title": "[BUG] Step by Step vs run in tool use.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "1d3c5ce3c3ffce66b9bebcaefa9de5139c998114b01fe6b662c3aab1a8cff0f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Step by Step vs run in tool use.\n\nDescription: When using Step by Step run (similar to \"Run agents one step at a time\" form documentation) in combination with a custom tool \"operator_tool\", an authorization error is raised: response = operator_tool(\"Hi\") due to: InterpreterError: Forbidden function evaluation: 'operator_tool' is not among the explicitly allowed tools or defined/imported in the preceding code Adding it to additional_authorized_imports did not help. When running using .run() everything works well.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 611, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e6622d45-0f92-4849-9010-26ea44f24217", "embedding": null, "metadata": {"issue_id": 1169, "title": "Add built-in functions like 'open' to BASE_PYTHON_TOOLS for the python executor", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f4b98b2d-4260-4efc-9def-e63f42c2118e", "node_type": "4", "metadata": {"issue_id": 1169, "title": "Add built-in functions like 'open' to BASE_PYTHON_TOOLS for the python executor", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "0eb391f55a3f14f5fbc2a5397327981fcef58ecaeba452e386a01a5cbac6c904", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add built-in functions like 'open' to BASE_PYTHON_TOOLS for the python executor\n\nDescription: **Is your feature request related to a problem? Please describe.** The CodeAgent can generate Python code to use built-in functions like open to read files from the local file system. Steps to Reproduce: Files: [CODE_BLOCK] Below is the clients.csv [CODE_BLOCK] Running the python analyst.py can _sometimes_ produce a code like this [CODE_BLOCK] Because open is not part of the BASE_PYTHON_TOOLS here we get this error [CODE_BLOCK] --- **Describe the solution you'd like** open seems like an essential built-in function that enables the agents to be much more powerful. 1. Either it can be added to the BASE_PYTHON_TOOLS 2. Or there should be a way to override the BASE_PYTHON_TOOLS while defining the CodeAgent. The same applies to DANGEROUS_MODULES and DANGEROUS_FUNCTIONS (at the developer's discretion). 3. Or at least the LLM prompt that generates the Python code should be aware of what built-in function it can and cannot use. This will avoid the use of functions like open in the first place. **Is this not possible with the current options?** It is still possible to define a custom tool to handle the opening of all types of files that the developer wants their agents to support. However since the file content will be part of the memory for the agent, it can cause issues when there is a large file **Describe alternatives you've considered** The same as above. Please let me know if I'm missing something obvious here.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd243d43-a4ee-4af0-8020-8799dc936c66", "embedding": null, "metadata": {"issue_id": 1167, "title": "[Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3a0b208-839a-4a8b-b284-d344e7d0c8ed", "node_type": "4", "metadata": {"issue_id": 1167, "title": "[Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "9364739507afc9438623303043605c5adb151c9db03462a60c5ef724d19c6aa3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6636851d-d5d8-402c-a2d6-efd84d5cff5c", "node_type": "1", "metadata": {}, "hash": "47d6b534f4829b64486d08e529eb061799a1e3bb0ac345cfd67cafaad2130b98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel\n\nDescription: Hi, first of all, thanks for the awesome project! I recently encountered a situation where I needed to pass additional configuration parameters to the model loading process \u2014 specifically when calling from_pretrained (e.g., for quantization or loading with specific torch_dtype and device_map values). Currently, the class only supports passing kwargs to model.generate(), but not to AutoModelForCausalLM.from_pretrained() or similar. To work around this, I made a small modification to the codebase that adds support for a model_config dictionary (or similar) to pass these parameters separately. I\u2019d like to suggest officially supporting this functionality \u2014 I believe it would add flexibility for users who need fine-grained control over model loading. Let me know if this is something you'd be open to \u2014 I can even submit a PR with the changes if helpful. Thanks again for your work! ************************************************************************************************************************ Modification in TransformersModel (smolagents/model.py): class TransformersModel_v2(Model): \"\"\" A class that uses Hugging Face's Transformers library for language model interaction. Now with support to model params and qlora This model allows you to load and use Hugging Face's models locally using the Transformers library. It supports features like stop sequences and grammar customization. > [!TIP] > You must have transformers and torch installed on your machine. Please run pip install smolagents[transformers] if it's not the case. Parameters: model_id (str): The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub. For example, \"Qwen/Qwen2.5-Coder-32B-Instruct\". device_map (str, *optional*): The device_map to initialize your model with. torch_dtype (str, *optional*): The torch_dtype to initialize your model with. trust_remote_code (bool, default False): Some models on the Hub require running remote code: for this model, you would have to set this flag to True. model_config: Params for model configuration that you want to use in AutoModelForImageTextToText.from_pretrained or AutoModelForCausalLM.from_pretrained kwargs (dict, *optional*): Any additional keyword arguments that you want to use in model.generate(), for instance max_new_tokens or device. Raises: ValueError: If the model name is not provided. Example: [CODE_BLOCK] \"\"\" def __init__( self, model_id: Optional[str] = None, device_map: Optional[str] = None, torch_dtype: Optional[str] = None, trust_remote_code: bool = False, model_config: dict = dict(), Vari\u00e1vel que armazena par\u00e2metros do modelo como dicion\u00e1rio **kwargs, Armazena par\u00e2metros do model.generate ): try: import torch from transformers import AutoModelForCausalLM, AutoModelForImageTextToText, AutoProcessor, AutoTokenizer except ModuleNotFoundError: raise ModuleNotFoundError( \"Please install 'transformers' extra to use 'TransformersModel': pip install 'smolagents[transformers]'\" ) if not model_id: warnings.warn( \"The 'model_id' parameter will be required in version 2.0.0. \" \"Please update your code to pass this parameter to avoid future errors. \" \"For now, it defaults to 'HuggingFaceTB/SmolLM2-1.7B-Instruct'.\", FutureWarning, ) model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\" self.model_id = model_id default_max_tokens = 5000 max_new_tokens = kwargs.get(\"max_new_tokens\") or kwargs.get(\"max_tokens\") if not max_new_tokens: kwargs[\"max_new_tokens\"] = default_max_tokens logger.warning( f\"max_new_tokens not provided, using this default value for max_new_tokens: {default_max_tokens}\" ) if device_map is None: device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\" logger.info(f\"Using device: {device_map}\") self._is_vlm = False try: self.model = AutoModelForImageTextToText.from_pretrained( model_id, device_map = device_map, torch_dtype = torch_dtype, trust_remote_code = trust_remote_code, **model_config Adiciona os kwargs, agora permite quantiza\u00e7\u00e3o ) self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=trust_remote_code) self.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6636851d-d5d8-402c-a2d6-efd84d5cff5c", "embedding": null, "metadata": {"issue_id": 1167, "title": "[Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3a0b208-839a-4a8b-b284-d344e7d0c8ed", "node_type": "4", "metadata": {"issue_id": 1167, "title": "[Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "9364739507afc9438623303043605c5adb151c9db03462a60c5ef724d19c6aa3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd243d43-a4ee-4af0-8020-8799dc936c66", "node_type": "1", "metadata": {"issue_id": 1167, "title": "[Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "f026691367d776e3239aeb9ea10f7b2178c67669faef27ae0b6908fa23bd4fc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dac45cc7-deef-4bde-b30a-b76948ce3926", "node_type": "1", "metadata": {}, "hash": "350eddaaa9f827a730edee32821c14419997a2b04623f185b4de44b483c8e213", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "model_id = model_id default_max_tokens = 5000 max_new_tokens = kwargs.get(\"max_new_tokens\") or kwargs.get(\"max_tokens\") if not max_new_tokens: kwargs[\"max_new_tokens\"] = default_max_tokens logger.warning( f\"max_new_tokens not provided, using this default value for max_new_tokens: {default_max_tokens}\" ) if device_map is None: device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\" logger.info(f\"Using device: {device_map}\") self._is_vlm = False try: self.model = AutoModelForImageTextToText.from_pretrained( model_id, device_map = device_map, torch_dtype = torch_dtype, trust_remote_code = trust_remote_code, **model_config Adiciona os kwargs, agora permite quantiza\u00e7\u00e3o ) self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=trust_remote_code) self._is_vlm = True except ValueError as e: if \"Unrecognized configuration class\" in str(e): self.model = AutoModelForCausalLM.from_pretrained( model_id, device_map = device_map, torch_dtype = torch_dtype, trust_remote_code = trust_remote_code, **model_config Adiciona os kwargs, agora permite quantiza\u00e7\u00e3o ) self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_remote_code) else: raise e except Exception as e: raise ValueError(f\"Failed to load tokenizer and model for {model_id=}: {e}\") from e super().__init__(flatten_messages_as_text=not self._is_vlm, **kwargs) def make_stopping_criteria(self, stop_sequences: List[str], tokenizer) -> \"StoppingCriteriaList\": from transformers import StoppingCriteria, StoppingCriteriaList class StopOnStrings(StoppingCriteria): def __init__(self, stop_strings: List[str], tokenizer): self.stop_strings = stop_strings self.tokenizer = tokenizer self.stream = \"\" def reset(self): self.stream = \"\" def __call__(self, input_ids, scores, **kwargs): generated = self.tokenizer.decode(input_ids[0][-1], skip_special_tokens=True) self.stream += generated if any([self.stream.endswith(stop_string) for stop_string in self.stop_strings]): return True return False return StoppingCriteriaList([StopOnStrings(stop_sequences, tokenizer)]) def __call__( self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None, tools_to_call_from: Optional[List[Tool]] = None, **kwargs, ) -> ChatMessage: completion_kwargs = self._prepare_completion_kwargs( messages = messages, stop_sequences = stop_sequences, grammar = grammar, **kwargs, ) messages = completion_kwargs.pop(\"messages\") stop_sequences = completion_kwargs.pop(\"stop\", None) max_new_tokens = ( kwargs.get(\"max_new_tokens\") or kwargs.get(\"max_tokens\") or self.kwargs.get(\"max_new_tokens\") or self.kwargs.get(\"max_tokens\") ) if max_new_tokens: completion_kwargs[\"max_new_tokens\"] = max_new_tokens if hasattr(self, \"processor\"): prompt_tensor = self.processor.apply_chat_template( messages, tools=[get_tool_json_schema(tool) for tool in tools_to_call_from] if tools_to_call_from else None, return_tensors=\"pt\", tokenize=True, return_dict=True, add_generation_prompt=True if tools_to_call_from else False, ) else: prompt_tensor = self.tokenizer.apply_chat_template( messages, tools=[get_tool_json_schema(tool) for tool in tools_to_call_from] if tools_to_call_from else None, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True if tools_to_call_from else False, ) prompt_tensor = prompt_tensor.to(self.model.device) count_prompt_tokens = prompt_tensor[\"input_ids\"].shape[1] if stop_sequences: stopping_criteria = self.make_stopping_criteria( stop_sequences, tokenizer=self.processor if hasattr(self, \"processor\") else self.tokenizer ) else: stopping_criteria = None out = self.model.generate( **prompt_tensor, stopping_criteria = stopping_criteria, **completion_kwargs, ) generated_tokens = out[0, count_prompt_tokens:] if hasattr(self, \"processor\"): output_text = self.processor.decode(generated_tokens, skip_special_tokens=True) else: output_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True) self.last_input_token_count = count_prompt_tokens self.last_output_token_count = len(generated_tokens) if stop_sequences is not None: output_text = remove_stop_sequences(output_text,", "mimetype": "text/plain", "start_char_idx": 3444, "end_char_idx": 7582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dac45cc7-deef-4bde-b30a-b76948ce3926", "embedding": null, "metadata": {"issue_id": 1167, "title": "[Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3a0b208-839a-4a8b-b284-d344e7d0c8ed", "node_type": "4", "metadata": {"issue_id": 1167, "title": "[Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "9364739507afc9438623303043605c5adb151c9db03462a60c5ef724d19c6aa3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6636851d-d5d8-402c-a2d6-efd84d5cff5c", "node_type": "1", "metadata": {"issue_id": 1167, "title": "[Feature Request] Allow passing configuration parameters to model loading (from_pretrained) at TransformersModel", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "4a638d94883c32448cf6e74f492059678882fabb14fb6fe196e4ea76eec74501", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "return_dict=True, add_generation_prompt=True if tools_to_call_from else False, ) prompt_tensor = prompt_tensor.to(self.model.device) count_prompt_tokens = prompt_tensor[\"input_ids\"].shape[1] if stop_sequences: stopping_criteria = self.make_stopping_criteria( stop_sequences, tokenizer=self.processor if hasattr(self, \"processor\") else self.tokenizer ) else: stopping_criteria = None out = self.model.generate( **prompt_tensor, stopping_criteria = stopping_criteria, **completion_kwargs, ) generated_tokens = out[0, count_prompt_tokens:] if hasattr(self, \"processor\"): output_text = self.processor.decode(generated_tokens, skip_special_tokens=True) else: output_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True) self.last_input_token_count = count_prompt_tokens self.last_output_token_count = len(generated_tokens) if stop_sequences is not None: output_text = remove_stop_sequences(output_text, stop_sequences) chat_message = ChatMessage( role=MessageRole.ASSISTANT, content=output_text, raw={\"out\": output_text, \"completion_kwargs\": completion_kwargs}, ) if tools_to_call_from: chat_message.tool_calls = [ get_tool_call_from_text(output_text, self.tool_name_key, self.tool_arguments_key) ] return chat_message .... __all__ = [ \"MessageRole\", \"tool_role_conversions\", \"get_clean_message_list\", \"Model\", \"MLXModel\", \"TransformersModel\", \"TransformersModel_v2\", \"ApiModel\", \"HfApiModel\", \"LiteLLMModel\", \"OpenAIServerModel\", \"VLLMModel\", \"AzureOpenAIServerModel\", \"ChatMessage\", ] ************************************************************************************************************************ Loading the model with 4 bit precision in python script: from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig from smolagents import CodeAgent, LiteLLMModel, FinalAnswerTool, TransformersModel, TransformersModel_v2, HfApiModel model_id = \"Qwen/Qwen2.5-Coder-32B-Instruct\" Tokenizer certo, sem device_map tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True) Configura\u00e7\u00e3o de quantiza\u00e7\u00e3o 4-bit bnb_config = BitsAndBytesConfig( load_in_4bit =True, bnb_4bit_compute_dtype = \"float16\", bnb_4bit_use_double_quant = True, bnb_4bit_quant_type = \"nf4\" ) model2 = TransformersModel_v2( model_id, device_map = \"auto\", torch_dtype = \"auto\", pode ser omitido se quiser trust_remote_code = True, model_config = {'quantization_config': bnb_config}, max_new_tokens = 2000 )\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 6666, "end_char_idx": 9201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c230cdb-314a-4e2a-882e-108a97e24540", "embedding": null, "metadata": {"issue_id": 1165, "title": "[BUG] Cannot execute class method with LocalPythonExecutor", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf59240e-93b4-4bfd-b4f8-8b6f846a4b59", "node_type": "4", "metadata": {"issue_id": 1165, "title": "[BUG] Cannot execute class method with LocalPythonExecutor", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f738e1e252b3a3c40784cb60925d1da245ed7103bd3b2c3ac0cb02fe63c5f844", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Cannot execute class method with LocalPythonExecutor\n\nDescription: **Describe the bug** My LLM write the code in OOP style, defined the class and class methods, when try to execute class method it fails **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Successfully execute the code **Packages version:** smolagents==1.12.0 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0f50eee-5300-4d50-a350-59e53571eb3d", "embedding": null, "metadata": {"issue_id": 1164, "title": "[BUG] TimeoutError: Couldn't connect to the MCP server after 30 seconds", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c47bd574-07a8-40a5-92c6-7e16073831b4", "node_type": "4", "metadata": {"issue_id": 1164, "title": "[BUG] TimeoutError: Couldn't connect to the MCP server after 30 seconds", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "45a261a8e652fdb02d790d3a3e68be4e466c514750a7780860ccd4719f4843c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] TimeoutError: Couldn't connect to the MCP server after 30 seconds\n\nDescription: **Description**: I'm encountering timeout errors when trying to use @modelcontextprotocol/server-memory and @modelcontextprotocol/server-sequential-thinking with MCPAdapt, while pubmedmcp works correctly. **Steps to Reproduce**: 1. Set up server parameters for either: [CODE_BLOCK] or [CODE_BLOCK] 2. Attempt to create a ToolCollection: [CODE_BLOCK] **Expected Behavior**: The server should start and connect successfully within the timeout period, similar to how pubmedmcp@0.1.3 works. **Actual Behavior**: Getting a timeout error: [CODE_BLOCK] **Additional Information**: \u2022 pubmedmcp@0.1.3 works correctly with the same setup \u2022 Tried both direct npx execution and specifying node path \u2022 Environment: Python 3.12, latest versions of relevant packages \u2022 The error occurs consistently with both server-memory and server-sequential-thinking\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d36c5d0-e881-4747-aa12-59033f5a6372", "embedding": null, "metadata": {"issue_id": 1162, "title": "[BUG] Save Function Error: Code Saving Failure", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20c34493-841e-4447-9497-ccb88e7546bb", "node_type": "4", "metadata": {"issue_id": 1162, "title": "[BUG] Save Function Error: Code Saving Failure", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "c641082c40384b8fccedb18ee1b224d728ae9731303dc9dc415d222b1f19939c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Save Function Error: Code Saving Failure\n\nDescription: Describe the bug When calling the save method in MultiStepAgent, the tool validation for the TextInspectorTool class fails. Specifically, the following issues are present: 1. There is a complex class attribute md_converter. Such attributes should be defined in the __init__ method rather than as class attributes. 2. The parameters text_limit and model in the __init__ method lack default values. 3. The variable MessageRole is undefined in the forward_initial_exam_mode and forward methods. Code to reproduce the error Since there is no specific code snippet for the call, the following is a possible simplified example: [CODE_BLOCK] Error logs [CODE_BLOCK] When using some imported external packages, such as smolagents, **the save function always reports an error.** Expected behavior When calling the save method of MultiStepAgent, the tool validation for the TextInspectorTool class should pass, and no ValueError exception should be thrown. Packages version: smolagents 1.13.0.dev0 Additional context This issue occurs during the use of the smolagents library and may be related to the definition and usage of the TextInspectorTool class. The attributes and methods of the TextInspectorTool class need to be checked and modified to resolve the tool validation failure issue.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1429, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "762f6476-6681-4e65-8530-1319d35266ba", "embedding": null, "metadata": {"issue_id": 1161, "title": "Feature to pass params such as user_id to tools?", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6c20705-b47a-4fcd-a470-5b7c75f80d18", "node_type": "4", "metadata": {"issue_id": 1161, "title": "Feature to pass params such as user_id to tools?", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "f94a3007fc959d00d12eb134fa5288f937db8187c91bb7a31e8f06ef455aa982", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feature to pass params such as user_id to tools?\n\nDescription: **Is your feature request related to a problem? Please describe.** It seems unreliable and a security risk to rely on the agent to pass the params such as user id to tool calls which may need to read or write to the db? **Describe the solution you'd like** params can be passed to the agent object that can be retrieved from the tools **Is this not possible with the current options.** 1. Tools doesn't seem to have any inputs besides from the ones provided by the agents' outputs. 2. Potentially possible with step callbacks? **Describe alternatives you've considered** It seems potentially possible with an override on the base agent classes to accept additional metadata and for the tool to retrieve it. **Additional context** Will continue to explore options. If this solution seems good enough I can write a PR for this.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f9ecdbf-90e3-46be-9ff6-c10842b0ceb3", "embedding": null, "metadata": {"issue_id": 1159, "title": "[BUG] `ValueError: I/O operation on closed pipe`  `with ToolCollection.from_mcp(...) as tool_collection: ` using StdioServer", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53a9851d-50be-4cb8-889b-3f35cabd76da", "node_type": "4", "metadata": {"issue_id": 1159, "title": "[BUG] `ValueError: I/O operation on closed pipe`  `with ToolCollection.from_mcp(...) as tool_collection: ` using StdioServer", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "44d6e65a3b3ff62b17e14ee70a41e6c67eeaaae9b5d7255e6128c022449c1307", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] `ValueError: I/O operation on closed pipe`  `with ToolCollection.from_mcp(...) as tool_collection: ` using StdioServer\n\nDescription: **Describe the bug** Raise ValueError: I/O operation on closed pipe within with ToolCollection.from_mcp(...) as tool_collection: using StdioServer **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** Provide error logs if there are any. [CODE_BLOCK] **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** 1.13\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b9e7084-7e23-461d-ac6c-36df1553a0be", "embedding": null, "metadata": {"issue_id": 1158, "title": "[BUG] deepseek Failed to deserialize the JSON body into the target type", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d976966a-0753-4a53-a43d-81d04d46a450", "node_type": "4", "metadata": {"issue_id": 1158, "title": "[BUG] deepseek Failed to deserialize the JSON body into the target type", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "26b94f49426d4b85c28f131c8a941d533027b6a609192be6b8e698b808398ac9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] deepseek Failed to deserialize the JSON body into the target type\n\nDescription: Code: [CODE_BLOCK] Error [CODE_BLOCK] Is this a compatibility issue between deepseek and openai itself?\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd2abf5a-8a79-4e59-8b05-0b98f20c813c", "embedding": null, "metadata": {"issue_id": 1152, "title": "[BUG]ValueError: tool screenshot returned a non-text content: <class 'mcp.types.ImageContent'>", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea86028d-a76a-4334-ae34-e96a2b466a1c", "node_type": "4", "metadata": {"issue_id": 1152, "title": "[BUG]ValueError: tool screenshot returned a non-text content: <class 'mcp.types.ImageContent'>", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "2f69e3c5981bc3e7f1fdb7c53fa5316fbf92ee66ab947eccb583ec20dbdab5e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]ValueError: tool screenshot returned a non-text content: <class 'mcp.types.ImageContent'>\n\nDescription: When calling the screenshot tool using MCP SSE, the returned image data is not processed correctly, causing the following error to be thrown: [CODE_BLOCK] Full error stack: [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 382, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d51e7472-6409-4929-a2f7-8a69c5e53c02", "embedding": null, "metadata": {"issue_id": 1150, "title": "Security Policy", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dd76c4c-da9b-49c9-9dd8-4f653c418809", "node_type": "4", "metadata": {"issue_id": 1150, "title": "Security Policy", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "0f56c29beeb0c47ce5dd345404d3b41406ad6f92c9268bffb292ab2df34ba756", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Security Policy\n\nDescription: Hi smolagents maintainers, We've identified a security vulnerability and would like to report it privately. Could you please establish a security policy for the repository? GitHub's private vulnerability reporting would be perfect. For reference: https://github.com/huggingface/transformers/security Also, would you consider setting up with huntr.com for vulnerability management like transformers repo does? Thanks, Zhengyu\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b640ed5-7f29-4686-8e5e-e2aee4a81c67", "embedding": null, "metadata": {"issue_id": 1147, "title": "mcpadapt version bump 0.0.19", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70a47cfa-03c4-4487-a840-cb6408f1b439", "node_type": "4", "metadata": {"issue_id": 1147, "title": "mcpadapt version bump 0.0.19", "state": "closed", "labels": [], "type": "issue"}, "hash": "bdee7eabccc0886fceba680572547264f0777c8d6f22bcae7d34738521d21bd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: mcpadapt version bump 0.0.19\n\nDescription: Hi there, Sorry forgot to revert here, since Monday we fixed the security issue flagged last week in mcpadapt allowing remote SSE MCP servers to execute code on users local environment. This is now resolved from version of mcpadapt 0.0.19 and above. mcpadapt will now directly forward the input json schema (we just make sure there is no jsonref or things like that) from the mcp server tools to the prompt. There is no more intermediate python representation that get executed which was the source of the security issue above. for reference: https://github.com/grll/mcpadapt/pull/21\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 648, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94dd7200-7160-4d08-8d8f-5f0f1e2375a4", "embedding": null, "metadata": {"issue_id": 1145, "title": "[BUG] RuntimeError: All input tensors need to be on the same GPU, but found some tensors to not be on a GPU: [(torch.Size([1, 4718592]), device(type=\u2018cpu\u2019)).....", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad5dc026-8950-4a3b-990d-6d40dd89595a", "node_type": "4", "metadata": {"issue_id": 1145, "title": "[BUG] RuntimeError: All input tensors need to be on the same GPU, but found some tensors to not be on a GPU: [(torch.Size([1, 4718592]), device(type=\u2018cpu\u2019)).....", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "c7c899a941f64bce6d4c1a4fb2cc665b83d05679d5d91fada74c27cedeafa2de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] RuntimeError: All input tensors need to be on the same GPU, but found some tensors to not be on a GPU: [(torch.Size([1, 4718592]), device(type=\u2018cpu\u2019)).....\n\nDescription: **Describe the bug** I am trying out TransformerModel as I don't want to pay for HFApiModel. I am using ZeroGPU on my huggingface space. [CODE_BLOCK] but there was this error: Smolagents Error: probability tensor contains either inf, nan or element < 0 now swapping out model_id to model_id=\"unsloth/Llama-3.2-3B-Instruct-unsloth-bnb-4bit\", caused this other problem Credits to @John6666 for helping me out **Code to reproduce the error** my source code [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** the agent should be able to receive new message and generate new outputs. **Packages version:** smolagents==1.12.0 **Additional context** @john6666 also pointed out that to(device)` seems to be already there. [CODE_BLOCK]\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "95b23387-13bc-4270-9b08-c69a1dee2f21", "embedding": null, "metadata": {"issue_id": 1143, "title": "[BUG] agent.replay errors", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "191f5e58-3cb0-4d0c-aacf-a383827348f9", "node_type": "4", "metadata": {"issue_id": 1143, "title": "[BUG] agent.replay errors", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "409680d5f214466e3414b0e1bdaa96ccf63102242c9a6e666fc2e144813c130e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] agent.replay errors\n\nDescription: **Describe the bug** A clear and concise description of what the bug is . There are a number of errors during a replay, step.facts missing step.model_output is None **Code to reproduce the error** The simplest code snippet that produces your bug. [CODE_BLOCK] **Error logs (if any)** Provide error logs if there are any. [CODE_BLOCK] **Expected behavior** A clear and concise description of what you expected to happen. No errors **Packages version:** Run pip freeze | grep smolagents and paste it here. smolagents==1.13.0 **Additional context** Add any other context about the problem here. I put a potential fix in the include code, but I just started using this lib\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 796, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37277e57-747d-400d-8c3c-b58f4dc37a7e", "embedding": null, "metadata": {"issue_id": 1142, "title": "[BUG] Inconsistency in nullable parameter validation for Tool class", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69b308d0-c63d-4fc7-8dc7-df07029f39f7", "node_type": "4", "metadata": {"issue_id": 1142, "title": "[BUG] Inconsistency in nullable parameter validation for Tool class", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "c582b555d0bfe181a8a3ef18016850824561fb4489fdeff8996b5bbc47c99d5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Inconsistency in nullable parameter validation for Tool class\n\nDescription: Inconsistent validation of nullable parameters in Tool class **Describe the bug** When creating a custom Tool with a required parameter, setting \"nullable\": False in the inputs dictionary causes validation to fail with the error: \"Nullable argument '[parameter]' in inputs should have key 'nullable' set to True in function signature\", even though the parameter is properly required in the function signature (without a default value). **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** The Tool should initialize correctly since there's consistency between: 1. The parameter being defined as non-nullable (\"nullable\": False) in the inputs dictionary 2. The function signature declaring the parameter as required (no default value) Both indicate that display_name is a required parameter. **Packages version:** [CODE_BLOCK] **Additional context** I've found two workarounds: 1. **More intuitive workaround**: Simply omit the \"nullable\" attribute altogether in the inputs dictionary: [CODE_BLOCK] 2. Alternative workaround: Make the parameter nullable in both places and add runtime validation: [CODE_BLOCK] The first workaround is more intuitive but still doesn't allow explicitly marking parameters as required with \"nullable\": False.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45bff7b5-926a-4e1d-96da-abd0e395e409", "embedding": null, "metadata": {"issue_id": 1141, "title": "GETTING Error while generating or parsing output: (Request ID: Q2Xdfw)  Bad request: Bad Request: Invalid state", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a69193e-13a5-4f4e-a3f3-9ff495ecad9e", "node_type": "4", "metadata": {"issue_id": 1141, "title": "GETTING Error while generating or parsing output: (Request ID: Q2Xdfw)  Bad request: Bad Request: Invalid state", "state": "closed", "labels": [], "type": "issue"}, "hash": "4db53ce3753a08a57adeab69d2bc5435a4f0f05495ad1f321829d2985d484a62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: GETTING Error while generating or parsing output: (Request ID: Q2Xdfw)  Bad request: Bad Request: Invalid state\n\nDescription: I am getting an error message when using smolagent. The code used is [CODE_BLOCK] The model id used here is the url of huggingface dedicated endpoint. when i run agent.run(\"Search for the best music recommendations for a party at the Wayne's mansion.\") I got an error in step 1 that reads Error while generating or parsing output: (Request ID: Q2Xdfw) Bad request: Bad Request: Invalid state I am not sure how to solve this is this a bug?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17eb2985-dc3c-4d5a-bc43-54988ceeda8a", "embedding": null, "metadata": {"issue_id": 1139, "title": "[BUG] AzureOpenAI content filter", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7256f5ed-6fe3-4954-b2b5-2033f54a2aff", "node_type": "4", "metadata": {"issue_id": 1139, "title": "[BUG] AzureOpenAI content filter", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "3fc664cf2b33e87c0e027f85472d11f32b1d3da474c09c8b1a79c1deff1c688b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] AzureOpenAI content filter\n\nDescription: **Describe the bug** After upgrading to smolagents 1.13, Azure's content filter is blocking my requests. This ONLY happens when planning_interval != 0 (ie. is enabled). If planning_interval is not set, everyting works just fine. It usually complains about \"self_harm\" **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** Error in generating model output: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': True, 'severity': 'high'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'medium'}}}}} **Expected behavior** I expected the agent to run normally after upgrading versions, both with and without planning. **Packages version:** smolagents==1.13.0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1389, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a9c75c3-e662-48e5-aa78-55c470e53812", "embedding": null, "metadata": {"issue_id": 1138, "title": "Support pillow 11.2.0", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05474746-11ed-4d27-a8b3-7c16593878d2", "node_type": "4", "metadata": {"issue_id": 1138, "title": "Support pillow 11.2.0", "state": "closed", "labels": [], "type": "issue"}, "hash": "16cd5940c80c2d2c2ec4d507facc8726aec4462eca56e3a66f5bcc1449901425", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support pillow 11.2.0\n\nDescription: Support pillow 11.2.0. Investigate the root cause that forced us to pin pillow and fix it: - 1128\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b0d60291-be2b-493d-9692-86420fc6d90e", "embedding": null, "metadata": {"issue_id": 1136, "title": "Pass params to vLLM model creation to improve flexibility", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "337966a0-3923-4075-975f-2a1445f976dd", "node_type": "4", "metadata": {"issue_id": 1136, "title": "Pass params to vLLM model creation to improve flexibility", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "2ea325e2b138b2524f31288385df4bd36144d398e65638c9ada98aa9b4427224", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Pass params to vLLM model creation to improve flexibility\n\nDescription: I've been playing with the vLLM (VLLMModel) support and generated a sample. [CODE_BLOCK] The support is super nice but it could benefit from some additional flexibility. Other model clients like HfApiModel and OpenAIServerModel already provide client_kwargs where you can add the params for the client. It could be interesting to extend this functionality to VLLMModel. For example, it would enable configurations like: [CODE_BLOCK] I already have a fix proposal developed \ud83d\ude04\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "38545cb6-ec39-4647-b07b-adda45a5203b", "embedding": null, "metadata": {"issue_id": 1134, "title": "[BUG] \"You exceeded your current quota\" handling for Gemini API", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d691d86a-8cce-4586-9cb1-dfc78d4c9aa3", "node_type": "4", "metadata": {"issue_id": 1134, "title": "[BUG] \"You exceeded your current quota\" handling for Gemini API", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "89bd330bcf126bc02c433daa402feb12c5da11a9ede407e0dda438d613d7a55d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] \"You exceeded your current quota\" handling for Gemini API\n\nDescription: When my gemini per-minute rate limit exceeds, my code just falls with this error: [CODE_BLOCK] Can we add the handling of this error and sleeping after it? Im think this improvement would be very important for free-tiers users like me\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2e952894-826c-4fbc-beef-974c52dabb27", "embedding": null, "metadata": {"issue_id": 1133, "title": "[BUG] Tools is not adapted for usage with Gemini", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d598375-65de-4d96-b956-77267823adc8", "node_type": "4", "metadata": {"issue_id": 1133, "title": "[BUG] Tools is not adapted for usage with Gemini", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "e24ea703143ad26a7c8f16d9e77bb3488e4907bbe74b31e5910282dd218dbdfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Tools is not adapted for usage with Gemini\n\nDescription: When im trying to use Gemini-based agent with tool that don't have any input parameters (like FindNextTool from open_deep_research), im getting this error [CODE_BLOCK]\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd1a774f-0f0c-4d25-b222-024a24fce8bc", "embedding": null, "metadata": {"issue_id": 1130, "title": "Passing globals/locals to PythonInterpreterTool", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8107188b-2894-4a9d-919f-e723559b7b0e", "node_type": "4", "metadata": {"issue_id": 1130, "title": "Passing globals/locals to PythonInterpreterTool", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "29bc26d61caf173c9a09cf4ee18624754e35ded00fe6325f2ff57a7d499ab68d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Passing globals/locals to PythonInterpreterTool\n\nDescription: **Is your feature request related to a problem? Please describe.** I want to pass a pandas dataframe to the tool, on which the generated code will be executed, but the state parameter in the tool's python_evaluator is fixed to empty dict. **Describe the solution you'd like** The state parameter in python_evaluator should be settable. Ideally an option to pass global/local parameters to the tool at run (instead of at Tool init) whiuch are not generated from an LLM. **Is this not possible with the current options.** I tried passing 'state' as a dict to tool init, but the validation fails as dict is not one of the AUTHORIZED_TYPES. **Describe alternatives you've considered** Using langchain. But since running python code is fundamental to CodeAgent, I believe this functionality ought to be part of this tool **Additional context** It might help contextualized the issue that although this functionality is available as part of CodeAgent, but maybe it would be better placed as part of the PythonInterpreterTool\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ece37347-197d-44b0-8d7f-aa9834c168c1", "embedding": null, "metadata": {"issue_id": 1129, "title": "[BUG] Is there a step timeout feature?", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2696d67a-ff6e-4e85-ab63-f4661953a331", "node_type": "4", "metadata": {"issue_id": 1129, "title": "[BUG] Is there a step timeout feature?", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "212b0d195e25c45bd8e5f8178e108e3ea1184dbc7f34ff45b0986e84612558c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Is there a step timeout feature?\n\nDescription: **Describe the bug** Running the provided benchmarking script with Haiku 3.5 (code agent) on smolagents benchmark (MATH subset) and noticed that it would often take much time because the model may write a brute-force code snippet and it takes a while to execute a step (>500 seconds). I looked through the code. While there is a MAX OPERATIONS for the local python executor, it didn't seem to help fix this issue. Wondering if there is a way to timeout an individual step and how to set this when initializing the agent. **Code to reproduce the error** https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/run.py **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. smolagents==1.10.0 **Additional context** Using LiteLLM and Haiku 3.5 through Bedrock\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1087, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c78a25b-ede1-4211-982f-ec6b44e35d99", "embedding": null, "metadata": {"issue_id": 1127, "title": "[BUG] Gemini randomly breaks the agent flow when using LiteLLM", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "357a7f80-c051-4680-82ce-57e418c8921d", "node_type": "4", "metadata": {"issue_id": 1127, "title": "[BUG] Gemini randomly breaks the agent flow when using LiteLLM", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "a0cd0652debec3b1c079f530829676f30ee83420d267987b81ac2cf3da179ec7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Gemini randomly breaks the agent flow when using LiteLLM\n\nDescription: **Describe the bug** When running an agent with Gemini models using LiteLLM sometimes I get an error in the dequeuing of the agent step. It's not systematic, it just happens randomly and cannot figure out the condition **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Just not stopping **Packages version:** smolagents==1.12.0 **Additional context** Only happens with Gemini\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21f6c259-63ed-426c-a51e-03e7fce9043b", "embedding": null, "metadata": {"issue_id": 1123, "title": "GIVE FEEDBACK TO SMOLAGENTS! \ud83d\udcdd", "state": "open", "labels": ["help wanted"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "734e7e7d-9767-40db-b422-2bb1e9e429b4", "node_type": "4", "metadata": {"issue_id": 1123, "title": "GIVE FEEDBACK TO SMOLAGENTS! \ud83d\udcdd", "state": "open", "labels": ["help wanted"], "type": "issue"}, "hash": "e1ea94dea423fe1e185d261af69391ff41a9ad5cf25ad030e71c71f8fcfcba79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: GIVE FEEDBACK TO SMOLAGENTS! \ud83d\udcdd\n\nDescription: We need you to give us feedback in order to improve the library! \ud83d\ude03 Here is the form, please take one minute to give us your thoughts!\n\nState: open\n\nLabels: help wanted\n\nCategories: category-help wanted", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06b8e62e-f4ec-457e-89f3-204bdccc708b", "embedding": null, "metadata": {"issue_id": 1121, "title": "Memory bank", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf786a41-560f-4679-ab84-97df45b988c3", "node_type": "4", "metadata": {"issue_id": 1121, "title": "Memory bank", "state": "open", "labels": [], "type": "issue"}, "hash": "c6a995913e0c3bae231d8b10f6cd5d1e8425d926196c64d23ca55e837cbbc283", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Memory bank\n\nDescription: I'd like to implement something similar to cline\u2019s memory bank in smolagents. The goal is to have a source of long term memory for longer and complex tasks, which will help agents: - keep the end goals of the project always within the working context - keep tab of the steps already successfully completed along the way, and avoid unnecessary changes or repeated tasks, something I noticed happening for (poorly) managed agents - avoid hallucination along the way - provide a documented and organized feedback to the human user - provide a common source of shared progress and results for multiple, parallel agents I guess we could simply have an optional addition to the agents system prompt with similar instructions as that reference, but given that smolagents is a more general tool than cline, I was wondering if anyone has had any experience in implementing a similar feature for smolagents, or if anyone would like to discuss strategies for that. Some ideas: - we could standardize the long-term memory files names and relative path - we could have a standard MemoryBankTool which the agents could rely on to read or update those files _Originally posted by @luiztauffer in https://github.com/huggingface/smolagents/discussions/1116_\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "088bbe89-9f01-4112-a5ae-82aeadb22c56", "embedding": null, "metadata": {"issue_id": 1120, "title": "[BUG] Example CLI command fails to parse positional aguments", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b68dda4f-84cb-4f44-b314-5585b18d890d", "node_type": "4", "metadata": {"issue_id": 1120, "title": "[BUG] Example CLI command fails to parse positional aguments", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "e7dae73f02cf8d743b15fe9905100807c2845005e8a2e5bcc4140da59b83e938", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Example CLI command fails to parse positional aguments\n\nDescription: **Describe the bug** Hello, After installing the package, I tried to run both examples of cli statements without success. The package was installed using uv add smolagents **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Packages version:** Fedora Linux 41 (Workstation Edition) Python==3.11.11 smolagents==1.12.0 uv==0.6.11\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b92eec1c-6ff1-40d1-94cd-d8e80625d837", "embedding": null, "metadata": {"issue_id": 1119, "title": "[BUG] Groq API fails to call tools for ToolCallingAgents", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2023d26d-3a96-4dc3-b8e5-3c4d32eac73f", "node_type": "4", "metadata": {"issue_id": 1119, "title": "[BUG] Groq API fails to call tools for ToolCallingAgents", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "131fc63a4f726ae219c731a5048217dd4d5ceaf26dd4140be6f33a9d77c4b634", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Groq API fails to call tools for ToolCallingAgents\n\nDescription: **Describe the bug** E.g.: for the deep research example where the the search_agent is a ToolCallingAgent, the groq api (via litellm) fails with: [CODE_BLOCK] **Code to reproduce the error** Run deep research example **Error logs (if any)** Error in generating tool call with model: litellm.BadRequestError: GroqException - {\"error\":{\"message\":\"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\",\"type\":\"invalid_request_error\",\"code\":\"tool_use_failed\",\"failed_genera tion\":\"web_search{\\\"query\\\": \\\"SPFMV and SPCSV in the Pearl Of Africa 2016\\\", \\\"filter_year\\\": 2016}\\u003c/function\\u003e\"}} **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1023, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4228ef05-3b17-4685-902e-56980efeca82", "embedding": null, "metadata": {"issue_id": 1114, "title": "Manager agent doesn't call other agents sometimes", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8ce7304f-41dc-46ed-a251-3f714e0dd219", "node_type": "4", "metadata": {"issue_id": 1114, "title": "Manager agent doesn't call other agents sometimes", "state": "closed", "labels": [], "type": "issue"}, "hash": "0fc68ccfcc34cce78b3424356c52b3a3d0e09a38d70f988edeb04d31a3289333", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Manager agent doesn't call other agents sometimes\n\nDescription: I have a multi-agent system, right now I only have a manager agent and other agent that has access to a database. I added to their system prompts how they should behave, specially the manager. The manager agent is supposed to call the other agent when he get an input related to the information inside the database but sometimes he tries to answer himself (normally for the first question/input he always calls the agent but for the next ones he doesn't), which ends up on making bad answers because he can't access the database. I don't know what to change to solve this problem, I thought it was a prompt issue but even defining a very detailed prompt didn't fix it. Any ideas on what could be and how to solve it?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f880b600-6c0e-4033-9d49-edbf95d056b6", "embedding": null, "metadata": {"issue_id": 1109, "title": "Hope there is a way to set `stop_sequences`.", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9f2ce2c-401f-4407-8d80-d8b19ded66db", "node_type": "4", "metadata": {"issue_id": 1109, "title": "Hope there is a way to set `stop_sequences`.", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "ab86aa1661bdd7beb7d3e12c1da2174247f5fae334948030b56d188cb8acd40d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Hope there is a way to set `stop_sequences`.\n\nDescription: **Is your feature request related to a problem? Please describe.** The model I'm using (qwen2.5 14b) keeps returning empty outputs after correctly outputting the code and obtaining the web page output, all the way up to the step limit. After debugging the code, I found that the model always tries to output Observation: first, which directly causes the output to be terminated. But no matter how I modify the system prompt, I can't prevent the model from outputting Observation:. **Describe the solution you'd like** It seems that the only way is to change the line of code from [stop_sequences=[\"<end_code>\", \"Observation:\", \"Calling tools:\"],](https://github.com/huggingface/smolagents/blob/35e9e8a192a91977bf8a26d31e895afec7212187/src/smolagents/agents.pyL1218) to stop_sequences=None,. In fact, everything works fine after this modification. The model can correctly handle the thinking process. It happily puts its thoughts in Observation: and eventually comes to a conclusion. But, obviously, this is not the best solution. Setting it directly to None is definitely not reasonable. I really hope that stop_sequences can be customizable, for example, by passing an **arg parameter in agent.run() that can override stop_sequences.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c9a1a86-5d97-48f0-8733-74a32e9f0bf0", "embedding": null, "metadata": {"issue_id": 1108, "title": "CodeAgent.to_dict() doesn't work as expected when using MCP tools", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "178110a2-9a4b-494a-b011-3ab31171098b", "node_type": "4", "metadata": {"issue_id": 1108, "title": "CodeAgent.to_dict() doesn't work as expected when using MCP tools", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "d39dd8c300672912c9f57610166bb7354722530a59b4d4a9304fcdcc5d3bdf6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: CodeAgent.to_dict() doesn't work as expected when using MCP tools\n\nDescription: I'm running a simple demo MCP server with the SSE transport (uv add mcp[cli]): [CODE_BLOCK] ----------------------------------------------------------------------------------------- I want to initialize a CodeAgent to use the tools from the this MCP server (uv add smolagents[mcp,openai]): [CODE_BLOCK] When I'm inspecting the agent with agent.to_dict() I get the following error message: [CODE_BLOCK] Do I need to define the MCP tools in my server in a specific way to usable by smolagents? Interestingly, running the agent with: result = agent.run(\"list all your available tools\") returns the MCP tool: ['add_numbers: Add two integers together.', 'web_search: Performs a web search based on a query and returns the top search results.', 'visit_webpage: Visits a webpage given its URL and reads its content.', 'final_answer: Provides a final answer to the given problem.']\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "506a6393-bf40-44b4-a082-f7a9d9cc8236", "embedding": null, "metadata": {"issue_id": 1107, "title": "More granular system prompt in description", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce8e152f-511e-485a-b094-a25efd65cd87", "node_type": "4", "metadata": {"issue_id": 1107, "title": "More granular system prompt in description", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "5ebd0c2efb6b24e318ba092c3ff85999da3ebf8283a922147668279bddb3595f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: More granular system prompt in description\n\nDescription: One of the biggest contributers to LLM's response quality as a good system prompt, and the more specific the better. Currently, all agents have roughly the same system prompt (You are an expert assistant who can solve any task using code blobs...), and If I want to change it, I have to change all of it. I would like just to change the \"persona/expertise\" of the agent. **Suggestion solution:** An additional property to the Agent constructor (named expertise or self_description - a string representing the expertise and knowledge of the agent. If this attribute exists, it will be injected into the system prompt instead of the default and generic an expert assistant example: [CODE_BLOCK] **Is this not possible with the current options.** The only way I'm aware of to change the system prompt is provide a whole new one, and that's a shame **Describe alternatives you've considered** I guess you can use the existing description attribute that is used for the prompt of the managing agent, but then the examples should be updated a bit, and you still need to inject at at the top of the system prompt.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01661f1e-4aa8-45c9-8785-58a877414a7e", "embedding": null, "metadata": {"issue_id": 1102, "title": "Add MCP Tools Integration Tutorial Documentation", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c61d6a6f-a721-4cde-8db5-9eb7388eb023", "node_type": "4", "metadata": {"issue_id": 1102, "title": "Add MCP Tools Integration Tutorial Documentation", "state": "closed", "labels": [], "type": "issue"}, "hash": "ae26671633f28b841e2165dad853c3c70619116ff1e1f7f9df166f32f1d240e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add MCP Tools Integration Tutorial Documentation\n\nDescription: It's awesome work! but, as a developer exploring MCP-enabled agent development, I find the current documentation lacks explicit guidance for importing MCP tools into smolagents.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 262, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "59b9470e-7d1b-4e3c-b203-dce952fdf563", "embedding": null, "metadata": {"issue_id": 1101, "title": "[BUG] Using the new Gemini model with planning_interval yields a RateLimit error", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61c24b02-12b8-415c-ba1e-612447c0ae03", "node_type": "4", "metadata": {"issue_id": 1101, "title": "[BUG] Using the new Gemini model with planning_interval yields a RateLimit error", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "cab6d47bc2748e9fe8dd087e3668407604e8e4bd3cba283459bf2c93ca48f0b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Using the new Gemini model with planning_interval yields a RateLimit error\n\nDescription: **Describe the bug** Using the new Gemini model with planning_interval yields a RateLimit error. Omitting planning_interval works just fine. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** If the model doesn't accept planning, we should not pass this arg forward to it. **Packages version:** smolagents==1.13.0.dev0 litellm==1.63.14\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfa5abab-506b-488f-b5a4-0b3b1ca398fb", "embedding": null, "metadata": {"issue_id": 1097, "title": "Planning step can make next ActionStep miss its start.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a645944a-d650-49f6-b120-a57c1ef9b8c0", "node_type": "4", "metadata": {"issue_id": 1097, "title": "Planning step can make next ActionStep miss its start.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "b832c951a2c35058e0955a17b7efd608068649b33553c357c845715ca216e13d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Planning step can make next ActionStep miss its start.\n\nDescription: This is not really a bug, but here it is: After running a planning step, the planning step is then exported by write_memory_to_messages as assistant for the LLM to generate the next output. That means, when the model sees the list of messages, the last message is an assistant role. That causes several APIs to just process as if the model was generating the continuation of the planning step. Which creates behaviours like: - \"if the planning steps ended with triple backtick to close a markdwon plan, the model output will just be python\\nprint(\"Doing next action\")\\n`` because it *thinks* the triple backtick is already handled - the LLM just returns nothing because it wants to end the step And so on. Solutions: - Use a different role for the planning step: not good IMO, since many models only have system/user/assistant, hence we would need to use user` role, as if the plan was given by the user, which would give it too much strength - Append a dummy \"user\" message after the assistant message. It could be something like \"Proceed to executing!\" but that means hardcoding promps, or having to add it in prompt tempaltes. What do you think @albertvillanova ?\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "59600500-05f7-422b-9621-1839fd2fc0e6", "embedding": null, "metadata": {"issue_id": 1092, "title": "Native Bedrock server model (willing to contribute)", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe13de06-82f9-4170-a114-d4a76ab5683b", "node_type": "4", "metadata": {"issue_id": 1092, "title": "Native Bedrock server model (willing to contribute)", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "697a7bd2787475906fd0eb7f73962a15fcfa65f7ceb0c5e1c342d10811ff17fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Native Bedrock server model (willing to contribute)\n\nDescription: **Is your feature request related to a problem? Please describe.** > A clear and concise description of what the problem is. Ex. I'm always frustrated when [...] I'd like to use smolagents with Amazon Bedrock whiteout the need to use a 3rd party dependency (LiteLLM) to call the models. **Describe the solution you'd like** > A clear and concise description of what you want to happen. Similar to the OpenAIServerModel, I'd like to see a BedrockServerModel that works directly with boto3 (the AWS SDK) to call the Converse/InvokeModel APIs. This would result in a more straightforward experience since parameters accepted by the Amazon Bedrock API can be directly mapped to the one of the server model in smolagents, removing the guesswork and need to learn LiteLLM. **Is this not possible with the current options.** > Make sure to consider if what you're requesting can be done with current abstractions. See above. **Describe alternatives you've considered** > A clear and concise description of any alternative solutions or features you've considered. Continue using LiteLLM. **Additional context** > Add any other context or screenshots about the feature request here. If welcome, me and my team would like to contribute this integration to the project.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "93956548-3913-44f5-9af1-41572e7fa0f1", "embedding": null, "metadata": {"issue_id": 1087, "title": "[BUG] Open Deep Research use citations in final report but cannot access manually via web browser", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06cec3b2-d527-4e45-861f-ce345b353ae5", "node_type": "4", "metadata": {"issue_id": 1087, "title": "[BUG] Open Deep Research use citations in final report but cannot access manually via web browser", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "38750907c486708fa52ec8278eb56b90b2d35a9cd62bb0b10e4ee888ef6c8c2d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Open Deep Research use citations in final report but cannot access manually via web browser\n\nDescription: **Describe the bug** I am using claude 3.7 as the backend model with Open DR. In the references section of final report, I find some of the URLs cannot be found via manually access on web browser. It prompts file not found or error 404, page not found. For example, https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/app.py https://salt.security/resources/state-of-api-security-report https://www.actian.com/resources/whitepapers/data-engineering-guide So I am wondering how does the Open DR access such URLs when generating report. Or are these URLs really used? **Code to reproduce the error** https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/app.py **Error logs (if any)** N/A **Expected behavior** N/A **Packages version:** Latest release **Additional context** N/A\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1029, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "258eb1fb-3bd3-4b2e-b1ad-182d6bf486fc", "embedding": null, "metadata": {"issue_id": 1086, "title": "[BUG]: Wrong type for the return value of tools", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "522076eb-daab-4425-9d38-e5bf8c322841", "node_type": "4", "metadata": {"issue_id": 1086, "title": "[BUG]: Wrong type for the return value of tools", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "cdbcfdb9776eeaf7ff1b534699629c1da0022be31d8c868dde92383c6dfd29de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]: Wrong type for the return value of tools\n\nDescription: **Describe the bug** Wrong type for the return value of tools **Code to reproduce the error** My tool function definitions is: [CODE_BLOCK] **Error logs (if any)** What I got in prompt: [CODE_BLOCK] The type of \"Returns\" is object instead of Dict/dict. **Expected behavior** The type of \"Returns\" is dict/Dict, and also description of Returns: [CODE_BLOCK] **Packages version:** smolagents==1.9.2 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 615, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34b0823b-9d72-4e89-9e57-2a0481839376", "embedding": null, "metadata": {"issue_id": 1085, "title": "Remote code execution warning for SSE MCP server", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0d130ed-ce37-4f3d-888d-584a40a32db0", "node_type": "4", "metadata": {"issue_id": 1085, "title": "Remote code execution warning for SSE MCP server", "state": "closed", "labels": [], "type": "issue"}, "hash": "d492b65744b5b7f09570c9bbf0798b0bff7cbfddd5234b0df11ef3153e79f0de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Remote code execution warning for SSE MCP server\n\nDescription: Hi there, Just raising your attention to a very valid point raised yesterday by @arryon on the mcpadapt repository: https://github.com/grll/mcpadapt/issues/19 In particular the current implementation of the mcpadapt SmolagentsAdapter is vulnerable to remote code execution from malicious remote MCP server (via SSE). We have added a warning message on the mcpadapt readme, we might want to do the same in the doc here. A new implementation is currently being tested which should fix the vulnerability. In the meantime but also after the fix, always be careful and make sure you trust the MCP server you are using: * over stdio it will execute some code on your machine no matter what. (that's how it works) * over SSE today it could run some malicious code on your machine, after the fix it won't be able to do that anymore.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "297a7a0f-c896-41ef-97aa-8a7a90fd9e04", "embedding": null, "metadata": {"issue_id": 1084, "title": "Stop agent", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "befd0d32-e02b-402d-ac23-ada8453807e3", "node_type": "4", "metadata": {"issue_id": 1084, "title": "Stop agent", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "ac36945f74ecdb46842db84fc3d56254c22c3fa3133882d4cc293fae891540bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Stop agent\n\nDescription: I want to be able to stop an agent mid run based on some external event. Stop will return a final answer that rhe agent was stopped by the user.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "26d27b82-d9b2-4490-9915-71bd0aaaac63", "embedding": null, "metadata": {"issue_id": 1080, "title": "Is pandas necessary as required dependency?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be89f396-f844-463e-a707-ad6e92c6bb6a", "node_type": "4", "metadata": {"issue_id": 1080, "title": "Is pandas necessary as required dependency?", "state": "closed", "labels": [], "type": "issue"}, "hash": "f660a5f59d0649236decb851c8482ab9ae2d10dd2ffc769d4512b4b253299ac5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Is pandas necessary as required dependency?\n\nDescription: I'm trying to bundle smolagents inside a Lambda function. As usual, one has to take care with dependencies, in order to let the bundle not grow too large. I've noticed that pandas is listed as a required dependency for smolagents. Searching for its use, it doesn't appear to be used in the main code, only referenced in authorized imports and examples: https://github.com/search?q=repo%3Ahuggingface%2Fsmolagents%20pandas&type=code If possible, I think pandas should be made optional. It bloats the distribution without being needed to implement agent functionality. Any thoughts?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45a60532-b805-4f8e-ad0f-f1035304f4bc", "embedding": null, "metadata": {"issue_id": 1079, "title": "[BUG] Open Deep Research's VisitTool hit 403 error for some URLs", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ad2b5a1-2a99-4a25-9ae7-f50050ffba2e", "node_type": "4", "metadata": {"issue_id": 1079, "title": "[BUG] Open Deep Research's VisitTool hit 403 error for some URLs", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "08b780aecc32baf0e3f8f82ef2d84d6d12b8df994aaef5cff19ffff43bf2b49f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Open Deep Research's VisitTool hit 403 error for some URLs\n\nDescription: **Describe the bug** I am using VisitTool from the Open Deep Research to fetch page content from URLs. But I find it may hit 403 error. Any way to fix it? Or how to fetch page content? And my follow-up question is how does the Open DR succeeds to fetch content from all URLs? or OpenDR hits such issue as well, and if hit, just skip the URL when generating final report or during analysis? **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** content **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 811, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f94d460-7e2a-4998-a727-a8036f77ffdd", "embedding": null, "metadata": {"issue_id": 1078, "title": "Provided `agent_from_any_llm.py` cannot run properly with error `Key tool_name_key='name' not found`", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a77fb2d-ebd0-413a-b8c4-50403db9241b", "node_type": "4", "metadata": {"issue_id": 1078, "title": "Provided `agent_from_any_llm.py` cannot run properly with error `Key tool_name_key='name' not found`", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "de56789a2a83ab689e86aa2b9dbd3bb0abe37ad546e1dbfd2cf880a7c2fe9968", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Provided `agent_from_any_llm.py` cannot run properly with error `Key tool_name_key='name' not found`\n\nDescription: Description I git clone your repo and found examples I tried the scripts examples/agent_from_any_llm.py, but failed with reason described as below part Error log the only difference is that I replace the llm init method into transformer as follow [CODE_BLOCK] Code [CODE_BLOCK] Error log [CODE_BLOCK] Expected behavior run normally Packages version [CODE_BLOCK] --- I found that tool_name is initialized when model is initialized I tried add arguments into model args and failed too I'd appreciated it if you could help, thx\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a0d3b56-bc6a-4246-aeea-87de2b28c09a", "embedding": null, "metadata": {"issue_id": 1073, "title": "[BUG] DocstringParsingException when creating a tool with types in docstring", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b40e1f7-9cfc-4f9b-87e4-0d5c76cd6970", "node_type": "4", "metadata": {"issue_id": 1073, "title": "[BUG] DocstringParsingException when creating a tool with types in docstring", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "2b97c4914cd1209cc4e0eb95eadfa2bbad15fd1020fab98f42a14aef32d9c1f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] DocstringParsingException when creating a tool with types in docstring\n\nDescription: **Describe the bug** When creating a new tool, I get an error \"DocstringParsingException: Cannot generate JSON schema for ... because the docstring has no description for the argument '...'\", if the type of arguments is given. **Code to reproduce the error** Example 1: [CODE_BLOCK] Or Example 2: [CODE_BLOCK] **Expected behavior** No error **Packages version:** smolagents==1.12.0 **Additional context** I think that the bug is due in smolagents/_function_type_hints_utils.py. I identified two problems: 1- The first is in [CODE_BLOCK] This explain why example 2 works if I replace book_title (str) : Titre du livre (ex. \"Genesis\", \"Berachot\") by book_title (str): Titre du livre (ex. \"Genesis\", \"Berachot\") The second is in: [CODE_BLOCK] Because for now we can't detect the next line as the next argument if it contains a type. This explain why the Example 1 cannot work even without the space between the type and the \":\".\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9798fa01-6563-42d9-9e36-064e3a9506e9", "embedding": null, "metadata": {"issue_id": 1071, "title": "[BUG] CodeAgent unable to use python built-in types \"bytes\"", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a88c2a9-f7e7-4d21-bd50-9ab1e6fbfd97", "node_type": "4", "metadata": {"issue_id": 1071, "title": "[BUG] CodeAgent unable to use python built-in types \"bytes\"", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "17ffa83f46a6994ed3ab50c71d27993f79e94ebbefa9f8e6b61224f28353a2e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] CodeAgent unable to use python built-in types \"bytes\"\n\nDescription: !Image When using smolagent's CodeAgent, I've discovered that the agent cannot use Python's built-in bytes type. Since bytes is a built-in type in Python (similar to int, str, list, etc.) and not part of any specific library, it cannot be imported through the conventional library import mechanism used by CodeAgent.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 478, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cc655486-aba9-4115-b8a4-c8296c009065", "embedding": null, "metadata": {"issue_id": 1066, "title": "Add step control to managed agents.", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00aea171-f0d3-4b49-bd58-70c04aa6e55b", "node_type": "4", "metadata": {"issue_id": 1066, "title": "Add step control to managed agents.", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "028c0820f2d5092d921d43301fffd40e74c9f0009cce6ceb9cb8594a450baea4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add step control to managed agents.\n\nDescription: There is a way to run the manager agent step by step. I want this level of control in managed agent as well.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 271, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a10f1ec-ff2e-454d-a711-4a24de82a2cf", "embedding": null, "metadata": {"issue_id": 1064, "title": "[Feature request] Handoff mode: allow managed agents to respond directly to the user", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b88a15b-147f-4f46-9955-daa56e6d5bed", "node_type": "4", "metadata": {"issue_id": 1064, "title": "[Feature request] Handoff mode: allow managed agents to respond directly to the user", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "d7e1c5c52219501203624eff98df86f71b7a55c230a5d66789ed14366304414f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Feature request] Handoff mode: allow managed agents to respond directly to the user\n\nDescription: **Is your feature request related to a problem? Please describe.** One problem I'm encountering with Smolagents is a scenario like this: 1. I have a primary \"assistant\" CodeAgent which has multiple managed_agents that can perform research. The primary \"assistant\" agent is only tasked with delegating to the correct managed_agent. 2. A user provides a task, and the CodeAgent correctly delegates to the proper managed_agent, and the managed_agent behaves well and returns a detailed output full of useful information 3. However, the top-level \"assistant\" agent will not return this detailed information verbatim (unless you ask it too, which is just a huge waste of tokens and time). So those ~200 tokens of great information are getting rephrased by the top-level agent to the user as a sentence. **Describe the solution you'd like** An common pattern with agents is a \"handoff\" where agents select a other agents to perform a task (exactly like managed_agents) but then that other agent can respond directly to the user. Currently, we have [CODE_BLOCK] I am imagining something like [CODE_BLOCK] For a user-provided task such as \"when was Elton John born\": 1. the main agent selects the research_agent for handoff 2. the research_agent performs its steps as usual 3. when the research_agent provides its final output via final_answer(...), this is shown directly to the user and added as a new message in the top-level conversation with reserach_agent I suspect there would need to be a new prompt added for this. Because the \"managed_agent\" responding prompt is geared towards \"model to model\" responses, but the user should be able to provide a prompt (or prompt template fragment) to help guide the output. **Is this not possible with the current options.** Make sure to consider if what you're requesting can be done with current abstractions. I have gone through some docs and features but haven't found a good way to implement this with current functionality. The best I could come up with was hooking into the step_callbacks and manually adjusting the history there, which feels very wrong. **Describe alternatives you've considered** A clear and concise description of any alternative solutions or features you've considered. **Additional context** Add any other context or screenshots about the feature request here.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2539, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "582a0dc5-749a-495b-9ce9-6dd696a54431", "embedding": null, "metadata": {"issue_id": 1062, "title": "[QUESTION] Does the agent implicitly does \"self-refinement\" ?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d32615d8-ff6a-4472-b115-8b13be71a2b8", "node_type": "4", "metadata": {"issue_id": 1062, "title": "[QUESTION] Does the agent implicitly does \"self-refinement\" ?", "state": "closed", "labels": [], "type": "issue"}, "hash": "c9194fca264d58b6e243987ed8d20b8dc0ff877cc17da37b0e5d446b39d2509c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [QUESTION] Does the agent implicitly does \"self-refinement\" ?\n\nDescription: @aymeric-roucher I found this paper which describes the process of Self-refinement by LLM where the LLM itself critiques its own response to improve it. I am wondering if the agent in smolagents does the same implicitly when we call agent.run() ? Link to the paper: https://arxiv.org/abs/2303.17651 .\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8e6b576-8635-47a8-8876-58bb969454d9", "embedding": null, "metadata": {"issue_id": 1061, "title": "[BUG] Managed Agent not working for hierarchy that involves two level.", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f793987-f41e-46c8-a1cb-7b0688c0e3ef", "node_type": "4", "metadata": {"issue_id": 1061, "title": "[BUG] Managed Agent not working for hierarchy that involves two level.", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "334f92ddbf088b3f4308da32a0519a0d46371453290d073e32e655d010dfd99e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Managed Agent not working for hierarchy that involves two level.\n\nDescription: **Describe the bug** CodeAgent offers managed_agent thus I create an agent that has two managed agents that is like a flat level 1 hierarchy wherein I am managing two agents with one supervisor kind of agent. But when I create the level managed agent also with some managed agents like level 2 hierarchy. Then on running my supervisor agent in a flow never reached the level 2 managed agents. It always stops or keeps on dwindling with the level 1 agents. **PseudoCode to illustrate the error** [CODE_BLOCK] The issue id while running the flow it never searches for the agents that are two levels below. **Additional context** Was looking forward to whether SmolAgents will implement this, or does it assumes a flat hierarchical structure.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 910, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc3ce08d-e41b-4c6d-a6ab-7c31a76c6409", "embedding": null, "metadata": {"issue_id": 1054, "title": "[BUG] Open Deep Research Example - serpapi import error", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81c70b6e-b717-403e-afbf-e15d20e8df71", "node_type": "4", "metadata": {"issue_id": 1054, "title": "[BUG] Open Deep Research Example - serpapi import error", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "10364a216d235ce7b6f3a3605f55ea4bdcc8a898273befac27a25b0dc838ed3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5954bce-05c3-4dbe-b4b9-555d7764c71a", "node_type": "1", "metadata": {}, "hash": "36fca4d51e4b51a5d8db6b2c443efba0ee1efbb16eb8fa9b6c7d7c8f7a5a1cbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Open Deep Research Example - serpapi import error\n\nDescription: **Describe the bug** Using python 3.10 with serpapi 0.1.5, in the Open Deep Research example, there is an import error in text_web_browser.py line 14: [CODE_BLOCK] Produces import error below: [CODE_BLOCK] This works: [CODE_BLOCK] **Code to reproduce the error** Using python 3.10 with **serpapi 0.1.5** Open the python console and enter: [CODE_BLOCK] Produces: [CODE_BLOCK] This works however: [CODE_BLOCK] **Error logs (if any)** See above. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. Package Version ---------------------------------------- ----------- accelerate 1.5.2 aiofiles 23.2.1 aiohappyeyeballs 2.6.1 aiohttp 3.11.14 aioitertools 0.12.0 aiosignal 1.3.2 aiosqlite 0.21.0 alembic 1.15.1 annotated-types 0.7.0 anthropic 0.49.0 anyio 4.9.0 arize-phoenix 8.17.1 arize-phoenix-client 1.1.0 arize-phoenix-evals 0.20.3 arize-phoenix-otel 0.8.0 asttokens 3.0.0 async-timeout 5.0.1 attrs 25.3.0 Authlib 1.5.1 beautifulsoup4 4.13.3 bio 1.7.1 biopython 1.85 biothings_client 0.4.1 cachetools 5.5.2 certifi 2025.1.31 cffi 1.17.1 charset-normalizer 3.4.1 chess 1.11.2 click 8.1.8 cobble 0.1.4 cryptography 44.0.2 datasets 3.4.1 decorator 5.2.1 defusedxml 0.7.1 Deprecated 1.2.18 dill 0.3.8 distro 1.9.0 dnspython 2.7.0 docker 7.1.0 duckduckgo_search 7.5.3 e2b 1.2.1 e2b-code-interpreter 1.1.1 email_validator 2.2.0 et_xmlfile 2.0.0 exceptiongroup 1.2.2 executing 2.2.0 fastapi 0.115.11 ffmpy 0.5.0 filelock 3.18.0 frozenlist 1.5.0 fsspec 2024.12.0 google-search-results 2.4.2 googleapis-common-protos 1.69.2 gprofiler-official 1.0.0 gradio 5.22.0 gradio_client 1.8.0 graphql-core 3.2.6 greenlet 3.1.1 groovy 0.1.2 grpc-interceptor 0.15.4 grpcio 1.71.0 h11 0.14.0 helium 5.1.1 httpcore 1.0.7 httpx 0.28.1 httpx-sse 0.4.0 huggingface-hub 0.29.3 idna 3.10 importlib_metadata 8.6.1 iniconfig 2.1.0 ipython 8.34.0 jedi 0.19.2 Jinja2 3.1.6 jiter 0.9.0 joblib 1.4.2 jsonref 1.1.0 jsonschema 4.23.0 jsonschema-specifications 2024.10.1 litellm 1.63.12 lxml 5.3.1 Mako 1.3.9 mammoth 1.9.0 markdown-it-py 3.0.0 markdownify 1.1.0 MarkupSafe 3.0.2 matplotlib-inline 0.1.7 mcp 1.5.0 mcpadapt 0.0.15 mdurl 0.1.2 mlx 0.23.1 mlx-lm 0.22.1 mpmath 1.3.0 multidict 6.2.0 multiprocess 0.70.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2359, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5954bce-05c3-4dbe-b4b9-555d7764c71a", "embedding": null, "metadata": {"issue_id": 1054, "title": "[BUG] Open Deep Research Example - serpapi import error", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81c70b6e-b717-403e-afbf-e15d20e8df71", "node_type": "4", "metadata": {"issue_id": 1054, "title": "[BUG] Open Deep Research Example - serpapi import error", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "10364a216d235ce7b6f3a3605f55ea4bdcc8a898273befac27a25b0dc838ed3f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc3ce08d-e41b-4c6d-a6ab-7c31a76c6409", "node_type": "1", "metadata": {"issue_id": 1054, "title": "[BUG] Open Deep Research Example - serpapi import error", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "95237cfb4b1489834530c18ac24e63bfe574c16fa5338291bd3780c93d0e7b28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "445975e3-d535-45e6-8d99-489d0c845466", "node_type": "1", "metadata": {}, "hash": "f8722aee8d1b237788078832ba0f990f85100732fd9bfdd373b6e5d30692d671", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0 ipython 8.34.0 jedi 0.19.2 Jinja2 3.1.6 jiter 0.9.0 joblib 1.4.2 jsonref 1.1.0 jsonschema 4.23.0 jsonschema-specifications 2024.10.1 litellm 1.63.12 lxml 5.3.1 Mako 1.3.9 mammoth 1.9.0 markdown-it-py 3.0.0 markdownify 1.1.0 MarkupSafe 3.0.2 matplotlib-inline 0.1.7 mcp 1.5.0 mcpadapt 0.0.15 mdurl 0.1.2 mlx 0.23.1 mlx-lm 0.22.1 mpmath 1.3.0 multidict 6.2.0 multiprocess 0.70.16 mygene 3.2.2 networkx 3.4.2 numexpr 2.10.2 numpy 2.2.4 nvidia-cublas-cu12 12.4.5.8 nvidia-cuda-cupti-cu12 12.4.127 nvidia-cuda-nvrtc-cu12 12.4.127 nvidia-cuda-runtime-cu12 12.4.127 nvidia-cudnn-cu12 9.1.0.70 nvidia-cufft-cu12 11.2.1.3 nvidia-curand-cu12 10.3.5.147 nvidia-cusolver-cu12 11.6.1.9 nvidia-cusparse-cu12 12.3.1.170 nvidia-cusparselt-cu12 0.6.2 nvidia-nccl-cu12 2.21.5 nvidia-nvjitlink-cu12 12.4.127 nvidia-nvtx-cu12 12.4.127 openai 1.68.2 openinference-instrumentation 0.1.24 openinference-instrumentation-smolagents 0.1.8 openinference-semantic-conventions 0.1.15 openpyxl 3.1.5 opentelemetry-api 1.31.1 opentelemetry-exporter-otlp 1.31.1 opentelemetry-exporter-otlp-proto-common 1.31.1 opentelemetry-exporter-otlp-proto-grpc 1.31.1 opentelemetry-exporter-otlp-proto-http 1.31.1 opentelemetry-instrumentation 0.52b1 opentelemetry-proto 1.31.1 opentelemetry-sdk 1.31.1 opentelemetry-semantic-conventions 0.52b1 orjson 3.10.15 outcome 1.3.0.post0 packaging 24.2 pandas 2.2.3 parso 0.8.4 pathvalidate 3.2.3 pdfminer 20191125 pdfminer.six 20240706 pexpect 4.9.0 pillow 11.1.0 pip 22.0.2 platformdirs 4.3.7 pluggy 1.5.0 pooch 1.8.2 primp 0.14.0 prompt_toolkit 3.0.50 propcache 0.3.0 protobuf 5.29.4 psutil 7.0.0 ptyprocess 0.7.0 PubChemPy 1.0.4 pure_eval 0.2.3 puremagic 1.28 pyarrow 19.0.1 pycparser 2.22 pycryptodome 3.22.0 pydantic 2.10.6 pydantic_core 2.27.2 pydantic-settings 2.8.1 pydub 0.25.1 Pygments 2.19.1 pypdf 5.4.0 PyPDF2 3.0.1 PySocks 1.7.1 pytest 8.3.5 pytest-datadir 1.6.1 python-dateutil 2.9.0.post0 python-dotenv 1.0.1 python-multipart 0.0.20 python-pptx 1.0.2 pytz 2025.1 PyYAML 6.0.2 rank-bm25 0.2.2 referencing 0.36.2 regex 2024.11.6 requests 2.", "mimetype": "text/plain", "start_char_idx": 1982, "end_char_idx": 4036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "445975e3-d535-45e6-8d99-489d0c845466", "embedding": null, "metadata": {"issue_id": 1054, "title": "[BUG] Open Deep Research Example - serpapi import error", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81c70b6e-b717-403e-afbf-e15d20e8df71", "node_type": "4", "metadata": {"issue_id": 1054, "title": "[BUG] Open Deep Research Example - serpapi import error", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "10364a216d235ce7b6f3a3605f55ea4bdcc8a898273befac27a25b0dc838ed3f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5954bce-05c3-4dbe-b4b9-555d7764c71a", "node_type": "1", "metadata": {"issue_id": 1054, "title": "[BUG] Open Deep Research Example - serpapi import error", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "19b41d1eedba5fa44803cddfc57e3d7a453e8c13dc815c8637bc6837a08d6e46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0.1 pycparser 2.22 pycryptodome 3.22.0 pydantic 2.10.6 pydantic_core 2.27.2 pydantic-settings 2.8.1 pydub 0.25.1 Pygments 2.19.1 pypdf 5.4.0 PyPDF2 3.0.1 PySocks 1.7.1 pytest 8.3.5 pytest-datadir 1.6.1 python-dateutil 2.9.0.post0 python-dotenv 1.0.1 python-multipart 0.0.20 python-pptx 1.0.2 pytz 2025.1 PyYAML 6.0.2 rank-bm25 0.2.2 referencing 0.36.2 regex 2024.11.6 requests 2.32.3 rich 13.9.4 rpds-py 0.23.1 ruff 0.11.2 safehttpx 0.1.6 safetensors 0.5.3 scikit-learn 1.6.1 scipy 1.15.2 selenium 4.30.0 semantic-version 2.10.0 sentencepiece 0.2.0 **serpapi 0.1.5** setuptools 59.6.0 shellingham 1.5.4 six 1.17.0 smolagents 1.13.0.dev0 sniffio 1.3.1 sortedcontainers 2.4.0 soundfile 0.13.1 soupsieve 2.6 SpeechRecognition 3.14.1 SQLAlchemy 2.0.39 sqlean.py 3.47.0 sse-starlette 2.2.1 stack-data 0.6.3 starlette 0.46.1 strawberry-graphql 0.262.5 sympy 1.13.1 threadpoolctl 3.6.0 tiktoken 0.9.0 tokenizers 0.21.1 tomli 2.2.1 tomlkit 0.13.2 torch 2.6.0 torchvision 0.21.0 tqdm 4.67.1 traitlets 5.14.3 transformers 4.50.0 trio 0.29.0 trio-websocket 0.12.2 triton 3.2.0 typer 0.15.2 typing_extensions 4.12.2 tzdata 2025.1 urllib3 2.3.0 uvicorn 0.34.0 wcwidth 0.2.13 websocket-client 1.8.0 websockets 15.0.1 wrapt 1.17.2 wsproto 1.2.0 xlrd 2.0.1 XlsxWriter 3.2.2 xxhash 3.5.0 yarl 1.18.3 youtube-transcript-api 1.0.2 zipp 3.21.0 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 3657, "end_char_idx": 5130, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a869896a-9b95-45d2-9a7e-f94faa3f4448", "embedding": null, "metadata": {"issue_id": 1046, "title": "[BUG] Can't get the MCP tools to work: RuntimeError: Event loop is closed", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a624970-6986-4d23-b13f-184e81fb1bfe", "node_type": "4", "metadata": {"issue_id": 1046, "title": "[BUG] Can't get the MCP tools to work: RuntimeError: Event loop is closed", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "3f6a7c148fd0b05d23de20fc452aa9d710cfc968b6c5e443de292fd9529ca264", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Can't get the MCP tools to work: RuntimeError: Event loop is closed\n\nDescription: **Describe the bug** I am trying to replace the normal tools by tools coming from a MCP server. My code is runnning inside a poetry venv. [CODE_BLOCK] gives me [CODE_BLOCK] In another mcp server, I can see that a log message coming from the server Processing request of type ListToolsRequest So the server is spawned, but once it tries to access the tool, I get the same error as above **Code to reproduce the error** See above. Running npx @modelcontextprotocol/inspector uvx mcp-server-time I can access the mpc server just fine. **Error logs (if any)** See above **Expected behavior** The agent calls the tool **Packages version:** smolagents==1.12.0 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 898, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8f334c0-10ea-4533-bdaa-ca287a0aeecf", "embedding": null, "metadata": {"issue_id": 1045, "title": "Support transformers 4.50.0", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "281949a3-cb31-40eb-85d3-3a3074db5106", "node_type": "4", "metadata": {"issue_id": 1045, "title": "Support transformers 4.50.0", "state": "closed", "labels": [], "type": "issue"}, "hash": "2ae43d20371075c4a0302e3ec5f06b1128c74f2edefe8c62d2f2d5d5f0c1facc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support transformers 4.50.0\n\nDescription: Support transformers 4.50.0 after the merge of: - 1044\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 118, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16871027-6c46-487d-8bc4-8575d0be352f", "embedding": null, "metadata": {"issue_id": 1043, "title": "CI test fails: AssertionError: Cannot use images with flatten_messages_as_text=True", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66fb727b-6535-454d-bfde-d407d9144fbc", "node_type": "4", "metadata": {"issue_id": 1043, "title": "CI test fails: AssertionError: Cannot use images with flatten_messages_as_text=True", "state": "closed", "labels": [], "type": "issue"}, "hash": "1c6785d9a52c08dbc5d14388dc4356d3051cacd506829b15ce7c5926fd7f6e31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: CI test fails: AssertionError: Cannot use images with flatten_messages_as_text=True\n\nDescription: CI test fails: https://github.com/huggingface/smolagents/actions/runs/13994614877/job/39186538691 [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49153932-5cd4-4902-8537-94302fae76ab", "embedding": null, "metadata": {"issue_id": 1039, "title": "[BUG]:litellm.exceptions.Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5972b418-f75c-41fc-babd-a67af58dc988", "node_type": "4", "metadata": {"issue_id": 1039, "title": "[BUG]:litellm.exceptions.Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "3afbc1c2bdf67054660dacfa65425563ad02cca8e0ae324d06f78f23366203ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]:litellm.exceptions.Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.\n\nDescription: **Describe the bug** LiteLLM Timeout while using a local LLM **Code to reproduce the error** python run.py --model-id \"o1\" \"Ask your question here.\" **Error logs (if any)** [CODE_BLOCK] **Expected behavior** I actually expect LiteLLM to timeout, what I do not expect is for smolagents to not have a way to set a custom LiteLLM timeout given that HF is all about providing models for self hosting. **Packages version:** [CODE_BLOCK] **Additional context** It would be great if there was a permitted setting that we can pass to LiteLLM to disable timeouts entirely when running local models, ie when the OpenAI host is localhost. Timeouts are meaningless in this context and entirely destructive.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 914, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ceb25346-6323-46ac-a7db-4e478957d4eb", "embedding": null, "metadata": {"issue_id": 1033, "title": "[BUG] Skipping analyzing \"smolagents\": module is installed, but missing library stubs or py.typed marker", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a6bb0f7-be00-44fe-9ba1-6932ec01e0c4", "node_type": "4", "metadata": {"issue_id": 1033, "title": "[BUG] Skipping analyzing \"smolagents\": module is installed, but missing library stubs or py.typed marker", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "6e9c23cd9a8d0eedb3800b772ec0cf5fb79def899445c1263f3cd6316ff17240", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Skipping analyzing \"smolagents\": module is installed, but missing library stubs or py.typed marker\n\nDescription: **Describe the bug** We use smolagents as part of our project that uses typing and mypy. It would be helpful if smolagents had a py.typed file as a marker for mypy or other typing tools as defined in PEP-561. **Code to reproduce the error** [CODE_BLOCK] Execute mypy <python file> **Error logs (if any)** error: Skipping analyzing \"smolagents\": module is installed, but missing library stubs or py.typed marker [import-untyped] **Expected behavior** Typing should be possible with the smolagents lib types. **Packages version:** smolagents==1.11.0 **Additional Context** https://mypy.readthedocs.io/en/stable/running_mypy.htmlmissing-imports\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 848, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e69bad15-8b68-4aff-a1ee-af3b06dcdfed", "embedding": null, "metadata": {"issue_id": 1032, "title": "PlanningSteps are not returned to the step callbacks nor from the stream", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f8ab4e5-2352-4fd7-bf7b-69bdd642ee31", "node_type": "4", "metadata": {"issue_id": 1032, "title": "PlanningSteps are not returned to the step callbacks nor from the stream", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "0c93563f54bd83996a504119be6f7bd99667171b944824630e0964382d9c874f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: PlanningSteps are not returned to the step callbacks nor from the stream\n\nDescription: **Is your feature request related to a problem? Please describe.** I am implementing reactions to the events in the UI and listening for the steps coming from the smolagents. I have planning steps enabled but i don't get the into the step callbacks or stream from agent.run. I had to fork the agents.py and do https://github.com/huggingface/smolagents/blob/main/src/smolagents/agents.pyL374-L378 In https://github.com/huggingface/smolagents/blob/main/src/smolagents/agents.pyL486-L512 **Describe the solution you'd like** PlanningStep is returned with the agent.run stream and to the step callbacks. **Is this not possible with the current options.** * It is possible if i change the code of the library myself, not really sustainable solution. **Describe alternatives you've considered** * I can dig into the memory of the agent and find it there, but it is not an event driven approach but rather a hack. * It is possible if i change the code of the library myself, not really sustainable solution. **Additional context** none\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "877b6847-d279-44b2-8545-d0c7796672c8", "embedding": null, "metadata": {"issue_id": 1028, "title": "Error \"Error in code parsing: Your code snippet is invalid...\" in a multi-agent system with manager CodeAgent", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70ec066d-ed7c-439a-9993-b11baf52f3eb", "node_type": "4", "metadata": {"issue_id": 1028, "title": "Error \"Error in code parsing: Your code snippet is invalid...\" in a multi-agent system with manager CodeAgent", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "hash": "bbfe57784226ebfb6e817976f3bdf55dfc457a6ca593128df20ab7de9adc2e6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Error \"Error in code parsing: Your code snippet is invalid...\" in a multi-agent system with manager CodeAgent\n\nDescription: I have multi-agent system with a manager agent, which is a CodeAgent itself, after calling other agents and getting their final answer the next step of the manager always throws this error: CODE_BLOCK?\\n(.*?)\\n[CODE_BLOCK]py Your python code here [CODE_BLOCK] I thought this could be related to the system prompt given to the CodeAgent but I tried to change it and didn't worked. Any ideas on how to fix this? I'm using OpenAIServerModel with gpt-4o.\n\nState: closed\n\nLabels: duplicate\n\nCategories: category-duplicate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a46dd38b-05d2-4c54-83ec-23ac66119d49", "embedding": null, "metadata": {"issue_id": 1025, "title": "Why is memory_step.action_output not assigned when tool_name != \"final_answer\" in ToolCallingAgent.step()?", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cd3ddd2-7055-4427-9c30-d7ef0e99dcfa", "node_type": "4", "metadata": {"issue_id": 1025, "title": "Why is memory_step.action_output not assigned when tool_name != \"final_answer\" in ToolCallingAgent.step()?", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "9f85c673500418dadde371106dca2c36e8b4e62ca1f2008cbf10d953f86b0243", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Why is memory_step.action_output not assigned when tool_name != \"final_answer\" in ToolCallingAgent.step()?\n\nDescription: In the ToolCallingAgent.step() method, when tool_name == \"final_answer\", memory_step.action_output is assigned the value of final_answer and is returned. However, when tool_name != \"final_answer\", memory_step.action_output is not assigned a value. [CODE_BLOCK]\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 496, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "deea4eac-c7b2-4801-aa0d-fa0a7defc161", "embedding": null, "metadata": {"issue_id": 1024, "title": "Support custom sandboxes", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4bc033c4-b23b-4bcd-8918-bb09b266ca3d", "node_type": "4", "metadata": {"issue_id": 1024, "title": "Support custom sandboxes", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "5ef8f8b2e6e7302b81ee41ebb458cf7875c8d88478751e014b3d0dc4f88cda23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support custom sandboxes\n\nDescription: **Is your feature request related to a problem? Please describe.** I'd like to be able to easily plug in a custom sandbox. However, the e2b and local python executor are currently hardcoded. **Describe the solution you'd like** I'd like to have a reusable abstraction for what a sandbox should support, so I could implement a custom sandbox and pass it as a parameter to the agent. **Is this not possible with the current options.** Currently the available sandboxes are hardcoded. **Describe alternatives you've considered** It's possible to hack together running the whole agent inside a custom sandbox, however this solution ties between the execution environment of the agent itself and the environment in which the tools run. It's a problem for example when it's needed to have a specific set of dependencies for a run.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f332152-2e63-410f-af03-453fd2ac57de", "embedding": null, "metadata": {"issue_id": 1020, "title": "Rendering LaTeX in GradioUI", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "117db8ad-dd6d-4a6c-8519-ad4bffe0f60f", "node_type": "4", "metadata": {"issue_id": 1020, "title": "Rendering LaTeX in GradioUI", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "1d6e7a4dc3df0eef09a93f522a5e3b253ce940d59fab0910db074f22ce113f76", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Rendering LaTeX in GradioUI\n\nDescription: At present (smolagents version 1.12.0), GradioUI does not render LaTeX formulas. What about enabling this feature? All it's required is changing in gradio_ui.py: [CODE_BLOCK] to: ``` Main chat interface chatbot = gr.Chatbot( label=\"Agent\", type=\"messages\", avatar_images=( None, \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/mascot_smol.png\", ), resizeable=True, scale=1, latex_delimiters = [ {\"left\": r'\\[', \"right\": r'\\]', \"display\": True}, {\"left\": r'\\(', \"right\": r'\\)', \"display\": False}, {\"left\": r'$$', \"right\": r'$$', \"display\": True}, {\"left\": r'$', \"right\": r'$', \"display\": False}, ], ) '''\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4dace83-98d3-4c26-9d0f-951d01de0219", "embedding": null, "metadata": {"issue_id": 1017, "title": "Add MCP.push_to_hub()", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "369d2d91-7740-413a-bd36-21beea446665", "node_type": "4", "metadata": {"issue_id": 1017, "title": "Add MCP.push_to_hub()", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "1dffeb297e1b00d50e26f470f642e40377bcef2d1d4f2086d47b4c4e8ef93773", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add MCP.push_to_hub()\n\nDescription: **Is your feature request related to a problem? Please describe.** Your Smolagents already supports MCP, and it's good to see that Smolagents' tools can be saved to hub. I wonder do you have any plans to support MCP server uploading? This might be an important feature since AFAIK there isn't any MCP server's hub for easily uploading and downloading yet. **Describe the solution you'd like** You guys know better how to do it. I'm thinking of an MCPHub class that supports push_to_hub etc. **Is this not possible with the current options.** I haven't seen any functionality in your library that supports MCP sever uploading. **Describe alternatives you've considered** Alternatives are MCP developers uploading separately to the places they prefer, such as GitHub. It could help further to standardise the verification for uploading and downloading. Besides, I really like HF's caching feature for models, data, Smolagents' tools, and more. **Additional context** I originally requested in the HF Hub's repo here, but was pointed out that here is a better place.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e70a173a-b2f2-467f-a683-1248faf93e59", "embedding": null, "metadata": {"issue_id": 1014, "title": "[BUG] Output is missing newlines which makes copying to clipboard suboptimal", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45635742-563c-4330-a0a5-6c353f8a78e9", "node_type": "4", "metadata": {"issue_id": 1014, "title": "[BUG] Output is missing newlines which makes copying to clipboard suboptimal", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "576044a908b2226cd35ff74cdb7d8a04cfe748e8cb036d54439468199d6e1607", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Output is missing newlines which makes copying to clipboard suboptimal\n\nDescription: **Describe the bug** When you copy the output of smolagents to clipboard and paste in a text editor, it is mangled because of missing newlines. It relies on the terminal wrapping long lines to simulate newlines instead of actually containing linefeed characters. **Code to reproduce the error** [CODE_BLOCK] **Expected behavior** It produces: [CODE_BLOCK] But it should paste with the same format as was displayed in the terminal: [CODE_BLOCK]` **Packages version:** Commit 83e971a9184ea9f91850f15481f65f4b31054d08 **Additional context** I tried poking around with Cursor and trying to add newlines or do different Rich layouts and couldn't figure out how to fix it without adding extra blank lines.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a8996192-d6a1-4600-b100-34fed1237e9a", "embedding": null, "metadata": {"issue_id": 1011, "title": "Opentelemetry traces should include agent names and the executed Python code", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2149c2a-a7c0-4cda-864d-f627bb530021", "node_type": "4", "metadata": {"issue_id": 1011, "title": "Opentelemetry traces should include agent names and the executed Python code", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "42b65167170da18d99771840c6da8a3ce4eb374a0a99f7b42be87f66714d852b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Opentelemetry traces should include agent names and the executed Python code\n\nDescription: The current OTEL instrumentation does not output the agent names, which makes understanding multi-agent traces difficult (although confusingly enough it does output managed_agents). Instead of outputting \"Step 1\", it would be better to have \"{agent.name} Step 1\", same for \"CodeAgent.run\" - better \"{agent.name} CodeAgent.run\". Based on a quick hack, it seems that this can be improved simply by replacing span_name assignments in _wrappers.py (and maybe also by adding it to _smolagent_run_attributes). Furthermore, given that Python execution errors are the most common type of errors occurring for CodeAgents during the runs, it is important that the actual Python code (not just error messages) is captured via OTEL. For that it seems like adding a _PythonExecutorWrapper similar to _ToolCallWrapper in _wrappers.py should be sufficient. (I got it to work with my LocalPythonExecutor, but I can't test it with other types, which is why I'm not providing a direct patch/PR.)\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d14231c-3857-4458-9eed-40aeac4c25f6", "embedding": null, "metadata": {"issue_id": 1005, "title": "[BUG]Incorrect description of the images parameter in MultiStepAgent.run", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f85ff9a0-3cda-4b2c-a6d6-93ed83353098", "node_type": "4", "metadata": {"issue_id": 1005, "title": "[BUG]Incorrect description of the images parameter in MultiStepAgent.run", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "0fa3fac133aca5d2ed20412f89638f4dcdb497af8f744a2ea61dbd39ed0ed149", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]Incorrect description of the images parameter in MultiStepAgent.run\n\nDescription: Description: The documentation for the images parameter in MultiStepAgent.run is inaccurate. The current docstring states that images accepts paths to images (list[str]), but in reality, the implementation does not support image file paths or URLs. Instead, it seems to require a different type of image input, such as preloaded image objects (e.g., PIL images, NumPy arrays, or Base64-encoded images). Current Docstring: [CODE_BLOCK] Issue: - The method does not handle image file paths or URLs. - If passed a file path, an error occurs because there is no internal loading mechanism. - Users may be misled into thinking they can provide image paths, causing unexpected failures.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 855, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b58006d2-3e5b-42a1-85bc-e0a40ddb7377", "embedding": null, "metadata": {"issue_id": 997, "title": "[BUG] Non-deterministic results because of CodeAgent's system prompt", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eed8d49b-2298-49e4-b306-1e8de70be75c", "node_type": "4", "metadata": {"issue_id": 997, "title": "[BUG] Non-deterministic results because of CodeAgent's system prompt", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "18fdb9b4ae0231ceaa7533fcfee53040bb753762a094f7b1ec9c852d0e554c70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Non-deterministic results because of CodeAgent's system prompt\n\nDescription: **Describe the bug** Currently, CodeAgent's default system prompt contains authorized_imports field which is collected during runtime. As of current version, authorized_imports is initialized dynamically using unordered sets: [CODE_BLOCK] This sometimes results in different system prompts between runs of code that creates CodeAgent, e.g.: [CODE_BLOCK] This may result in slightly different generation outputs even with temperature set to zero. **Code to reproduce the error** [CODE_BLOCK] and observe the difference between the imports order. **Expected behavior** I expect the system prompt to be the same. **Packages version:** smolagents==1.9.2\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 820, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05346153-eaeb-4c30-917c-ed0292de637a", "embedding": null, "metadata": {"issue_id": 996, "title": "\u89c6\u9891\u6f14\u793a\u8c03\u7528MCP", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b33f50bf-381e-48ff-a51e-56d1b93abf25", "node_type": "4", "metadata": {"issue_id": 996, "title": "\u89c6\u9891\u6f14\u793a\u8c03\u7528MCP", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "654a64a0517a0af9dce1b5a9ebcdfd81d116cf17800951267255a30e7dd81d19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: \u89c6\u9891\u6f14\u793a\u8c03\u7528MCP\n\nDescription: https://youtu.be/vYm0brFoMwA\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 140, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c5fb48c1-5c17-4588-a434-d68a7f9ad339", "embedding": null, "metadata": {"issue_id": 993, "title": "[BUG] `add_generation_prompt=False` when it should be `True`?", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a5b260c-cdaa-48a0-8091-8d7a8634dbe2", "node_type": "4", "metadata": {"issue_id": 993, "title": "[BUG] `add_generation_prompt=False` when it should be `True`?", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "c1f96536a34466ec7c4783e414f28dff813ed8a8deac852cb80c871915a0dadc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] `add_generation_prompt=False` when it should be `True`?\n\nDescription: **Describe the bug** CodeAgent and planning with ToolCallingAgent are not passing add_generation_template=True when applying the chat template to the input messages of the model. > [!NOTE] > Not sure if it is a bug or this is how you intended for it to work, but from what I saw it confuses the model and just seems wrong. If this is intended, can you please explain the logic? Note the <|endoftext|> token at the end of the prompt. [CODE_BLOCK] [CODE_BLOCK] Not sure if it is a bug or this is how you intended for it to work, but from what I saw it confuses the model and just seems wrong. The problem is that when calling the model, the condition to add the generation prompt is: [CODE_BLOCK] And the tools_to_call_from are not passed to the call when using CodeAgent or when planning. **Code to reproduce the error** [CODE_BLOCK] **Expected behavior** add_generation_prompt=True by default for all model calls? **Packages version:** smolagents==1.11.0 @albertvillanova saw you were involved in PRs around this issue, maybe you can shed some light on this matter. Thanks!\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "55f34740-0e79-4614-9c97-1d9d6d88bbc9", "embedding": null, "metadata": {"issue_id": 988, "title": "VertexAIServerModel Integration to Enable Openinference instrumentation with Phoenix", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cda0d58f-7fe0-4f22-be7d-8d799ef52077", "node_type": "4", "metadata": {"issue_id": 988, "title": "VertexAIServerModel Integration to Enable Openinference instrumentation with Phoenix", "state": "open", "labels": [], "type": "issue"}, "hash": "09977995fd24357c0b912503cb77b8143275f4fa2842c0de74047b101ef043fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: VertexAIServerModel Integration to Enable Openinference instrumentation with Phoenix\n\nDescription: I am not sure if this is a feature request or if I am just not understanding how to properly integrate vertex AI with smolagents and opentelemetry. I created a custom model class extending the base model and didn't have any issues until I tried to add openinference instrumentation with phoenix and SmolagentsInstrumentor I get an AttributeError because my custom model server isnt in smolagents. AttributeError: module 'smolagents' has no attribute 'VertexAIServerModel'. Did you mean: 'OpenAIServerModel'? I would like the ability to integrate VertexAIServerModel similar to the OpenAIServerModel to allow to select models from the model garden/registry in GCP. I added the custom model class to models.py and then I was able to use the model as expected and telemetry was successful in pheonix. Is this even a smolagents change? Sorry I am very new to this ecosystem. !Image code taken from this notebook: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/open-models/use-cases/vertex_ai_deepseek_smolagents.ipynb [CODE_BLOCK]\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1161, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b6237e1-4e48-435d-ace4-57a0519dc559", "embedding": null, "metadata": {"issue_id": 986, "title": "Add SearXNG to supported search engines", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "847d4c4e-3c92-46a3-b56f-4d2e22d6f1b2", "node_type": "4", "metadata": {"issue_id": 986, "title": "Add SearXNG to supported search engines", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "1d300b2c689cf0c490ded6c2297471079cdb515d5850a193d9564e6e0f2a2d02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add SearXNG to supported search engines\n\nDescription: **Is your feature request related to a problem? Please describe.** I run SearXNG locally because it is free **Describe the solution you'd like** Accept PR 677 that adds the SearXNG tool **Is this not possible with the current options.** No it's not possible. I had to write it myself. **Describe alternatives you've considered** Just using my own branch seems suboptimal **Additional context** Pretty please\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1bf79315-cd8f-4e91-8956-f53ddddd4f7f", "embedding": null, "metadata": {"issue_id": 980, "title": "Support `push_to_hub` in marimo notebooks", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a13ed9f4-57d6-4172-b78c-3c82bd51a6f1", "node_type": "4", "metadata": {"issue_id": 980, "title": "Support `push_to_hub` in marimo notebooks", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "044b568ad07ae21ebcc0f213b181240c0334bb6b8c6f769c68093145dd688bd1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support `push_to_hub` in marimo notebooks\n\nDescription: **Is your feature request related to a problem? Please describe.** In marimo notebooks (https://marimo.io/) the push_to_hub function fails because get_source function can not fetch the source. **Describe the solution you'd like** I have already written a patch https://github.com/kazemihabib/Huggingface-Agents-Course-Marimo-Edition/blob/marimo/patches/smolagents_patches.py that fixes this problem. **Is this not possible with the current options.** Current get_source function of smolagents, can fetch the source of function tools that are not prefixed by underscore but fails with classes. The reason is the following: 1)If obj is a Class, inspect.getsource(obj) will raise an error 2) If obj is a function, inspect.getsource(obj) will return the source code 3) If obj is a Class, in marimo inspect.getfile(obj) will return the path of the file 4) If obj is a function, in marimo inspect.getfile(obj) will return a non-useful path 5) If obj is local to cell (_ prefixed) obj.__name__ will return _cell_{cell_id}{obj orig name} so if obj is a function, the smolagents to_dict function (https://github.com/huggingface/smolagents/blob/c8f5322f7d8ea666f19f6b04459677f61071cebc/src/smolagents/tools.pyL243) attempts to replace the function name with forward in the source code, but fails because it can't find the prefixed name pattern. As a result, the uploaded tool lacks a forward function and will fail when executed on the server. **Additional context** My patch fixes this problem (https://github.com/kazemihabib/Huggingface-Agents-Course-Marimo-Edition/blob/marimo/patches/smolagents_patches.py) To fetch source I am doing the following: [CODE_BLOCK] because of the way I am fetching the source, writing tests for that is not easy, so I have not created a PR for this issue yet.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb45cfb9-cf47-4ea7-bcd9-483dc8c95af3", "embedding": null, "metadata": {"issue_id": 978, "title": "[BUG] executor_type Key Error when loading agents from_hub pushed with older smolagents versions", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e70af79-127b-4a12-bb53-4876d783cee4", "node_type": "4", "metadata": {"issue_id": 978, "title": "[BUG] executor_type Key Error when loading agents from_hub pushed with older smolagents versions", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f512d70e8c7c7d8d8a958e2edbf42a4c80ee785d3a608474c5c4bb6d2418e36a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] executor_type Key Error when loading agents from_hub pushed with older smolagents versions\n\nDescription: **Describe the bug** I suppose PR 733 creates compatibility issue. Impossible to load agent from_hub pushed with older versions of library. **Code to reproduce the error** The simplest code snippet that produces your bug. I use the code from notebook of Agents Course: https://huggingface.co/agents-course/notebooks/blob/main/unit2/smolagents/code_agents.ipynb [CODE_BLOCK] **Error logs (if any)** Provide error logs if there are any. [CODE_BLOCK] **Expected behavior** A clear and concise description of what you expected to happen. alfred_agent = agent.from_hub('sergiopaniego/AlfredAgent', trust_remote_code=True) is loaded without need to upgrade all older pushed agents. **Packages version:** Run pip freeze | grep smolagents and paste it here. [CODE_BLOCK] **Additional context** Add any other context about the problem here. Breaking for Hugging Face \"Agents Course\" alfred_agent = agent.from_hub('sergiopaniego/AlfredAgent', trust_remote_code=True) works great with smolagents 1.9.2\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1189, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54165b0d-75e1-4f3c-a450-74fbd5bde6a1", "embedding": null, "metadata": {"issue_id": 974, "title": "[BUG] Gemma3 Support in MLXModel", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5fe3eac6-66f0-4c4c-9196-993c726582a4", "node_type": "4", "metadata": {"issue_id": 974, "title": "[BUG] Gemma3 Support in MLXModel", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "8037800451c375d55b391b56d51965bd44780232ce5b0366afa6e146aca5329c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Gemma3 Support in MLXModel\n\nDescription: **Describe the bug** MLXModel doesn't support Gemm3 **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** See the command line output above. **Additional context** I can get the output through python -m mlx_vlm.generate --model mlx-community/gemma-3-4b-it-8bit --max-tokens 100 --temperature 0.0 --prompt \"How to make sushi?\" and I assume the problem was more on the smolagents' usage. **Packages version:** Run pip freeze | grep smolagents and paste it here. [CODE_BLOCK] **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f8603dd2-9868-4dea-a8f6-dc57b9488109", "embedding": null, "metadata": {"issue_id": 969, "title": "[BUG] quantization_config is not supported in TransformersModel", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "386aa1df-32be-4568-b85d-a2269448de3b", "node_type": "4", "metadata": {"issue_id": 969, "title": "[BUG] quantization_config is not supported in TransformersModel", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "4ddf95e0d78e97da8999e91ea0619aa9b3c7bfd2b7df0d864961af1e7a6fc5c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df1a8116-355d-4513-9409-37f23167275f", "node_type": "1", "metadata": {}, "hash": "5b6de14a5e00d156e86aa868f2caa44f80ccc493c3e14bcf2e890e1b056add9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] quantization_config is not supported in TransformersModel\n\nDescription: **Describe the bug** The following function supports quantization_config parameter that can be used to utilize quantization. [CODE_BLOCK] However the smolagents.TransformersModel() does not pass this parameter. Therefore quantization is not supported. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** There are no logs. I can see that model does not fit in my GPU memory, however it should using 8 bit quantization. **Expected behavior** The model should load in 8bit mode **Packages version:** accelerate==1.4.0 aiofiles==24.1.0 aiohappyeyeballs==2.5.0 aiohttp==3.11.13 aiosignal==1.3.2 altair==5.5.0 annotated-types==0.7.0 anyio==4.8.0 asgiref==3.8.1 async-timeout==4.0.3 attrs==25.1.0 backoff==2.2.1 bcrypt==4.3.0 beautifulsoup4==4.13.3 bitsandbytes==0.45.3 blinker==1.9.0 build==1.2.2.post1 cachetools==5.5.2 certifi==2025.1.31 cffi==1.17.1 chardet==5.2.0 charset-normalizer==3.4.1 chroma-hnswlib==0.7.6 chromadb==0.6.3 click==8.1.8 coloredlogs==15.0.1 cryptography==44.0.2 dataclasses-json==0.6.7 Deprecated==1.2.18 distro==1.9.0 docx2txt==0.8 duckduckgo_search==7.5.1 durationpy==0.9 emoji==2.14.1 eval_type_backport==0.2.2 exceptiongroup==1.2.2 fastapi==0.115.11 filelock==3.17.0 filetype==1.2.0 flatbuffers==25.2.10 frozenlist==1.5.0 fsspec==2025.3.0 gitdb==4.0.12 GitPython==3.1.44 google-auth==2.38.0 googleapis-common-protos==1.69.1 greenlet==3.1.1 grpcio==1.71.0 h11==0.14.0 html5lib==1.1 httpcore==1.0.7 httptools==0.6.4 httpx==0.28.1 httpx-sse==0.4.0 huggingface-hub==0.29.3 humanfriendly==10.0 idna==3.10 importlib_metadata==8.5.0 importlib_resources==6.5.2 Jinja2==3.1.6 jiter==0.9.0 joblib==1.4.2 jsonpatch==1.33 jsonpointer==3.0.0 jsonschema==4.23.0 jsonschema-specifications==2024.10.1 kubernetes==32.0.1 langchain==0.3.20 langchain-chroma==0.2.2 langchain-community==0.3.19 langchain-core==0.3.43 langchain-huggingface==0.1.2 langchain-text-splitters==0.3.6 langdetect==1.0.9 langsmith==0.3.13 lxml==5.3.1 markdown-it-py==3.0.0 markdownify==1.1.0 MarkupSafe==3.0.2 marshmallow==3.26.1 mdurl==0.1.2 mmh3==5.1.0 monotonic==1.6 mpmath==1.3.0 multidict==6.1.0 mypy-extensions==1.0.0 narwhals==1.30.0 nest-asyncio==1.6.0 networkx==3.4.2 nltk==3.9.1 numpy==1.26.4 nvidia-cublas-cu12==12.4.5.8 nvidia-cuda-cupti-cu12==12.4.127 nvidia-cuda-nvrtc-cu12==12.4.127 nvidia-cuda-runtime-cu12==12.4.127 nvidia-cudnn-cu12==9.1.0.70 nvidia-cufft-cu12==11.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "df1a8116-355d-4513-9409-37f23167275f", "embedding": null, "metadata": {"issue_id": 969, "title": "[BUG] quantization_config is not supported in TransformersModel", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "386aa1df-32be-4568-b85d-a2269448de3b", "node_type": "4", "metadata": {"issue_id": 969, "title": "[BUG] quantization_config is not supported in TransformersModel", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "4ddf95e0d78e97da8999e91ea0619aa9b3c7bfd2b7df0d864961af1e7a6fc5c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8603dd2-9868-4dea-a8f6-dc57b9488109", "node_type": "1", "metadata": {"issue_id": 969, "title": "[BUG] quantization_config is not supported in TransformersModel", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "5d60842f044f58260002095716382157fa73bab0ff787ace394ddce1cee4f221", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51839049-54bb-48b3-a223-0a38e5f3eb84", "node_type": "1", "metadata": {}, "hash": "523b7fb167bff0a722efe9005bdaa8482d67838071208b3134925b2705506f2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1.0 MarkupSafe==3.0.2 marshmallow==3.26.1 mdurl==0.1.2 mmh3==5.1.0 monotonic==1.6 mpmath==1.3.0 multidict==6.1.0 mypy-extensions==1.0.0 narwhals==1.30.0 nest-asyncio==1.6.0 networkx==3.4.2 nltk==3.9.1 numpy==1.26.4 nvidia-cublas-cu12==12.4.5.8 nvidia-cuda-cupti-cu12==12.4.127 nvidia-cuda-nvrtc-cu12==12.4.127 nvidia-cuda-runtime-cu12==12.4.127 nvidia-cudnn-cu12==9.1.0.70 nvidia-cufft-cu12==11.2.1.3 nvidia-curand-cu12==10.3.5.147 nvidia-cusolver-cu12==11.6.1.9 nvidia-cusparse-cu12==12.3.1.170 nvidia-cusparselt-cu12==0.6.2 nvidia-nccl-cu12==2.21.5 nvidia-nvjitlink-cu12==12.4.127 nvidia-nvtx-cu12==12.4.127 oauthlib==3.2.2 olefile==0.47 onnxruntime==1.21.0 openai==1.66.2 opentelemetry-api==1.30.0 opentelemetry-exporter-otlp-proto-common==1.30.0 opentelemetry-exporter-otlp-proto-grpc==1.30.0 opentelemetry-instrumentation==0.51b0 opentelemetry-instrumentation-asgi==0.51b0 opentelemetry-instrumentation-fastapi==0.51b0 opentelemetry-proto==1.30.0 opentelemetry-sdk==1.30.0 opentelemetry-semantic-conventions==0.51b0 opentelemetry-util-http==0.51b0 orjson==3.10.15 outcome==1.3.0.post0 overrides==7.7.0 packaging==24.2 pandas==2.2.3 pillow==11.1.0 posthog==3.19.1 primp==0.14.0 propcache==0.3.0 protobuf==5.29.3 psutil==7.0.0 pyarrow==19.0.1 pyasn1==0.6.1 pyasn1_modules==0.4.1 pycparser==2.22 pydantic==2.10.6 pydantic-settings==2.8.1 pydantic_core==2.27.2 pydeck==0.9.1 Pygments==2.19.1 pypdf==5.3.1 pypdfium2==4.30.1 PyPika==0.48.9 pyproject_hooks==1.2.0 PySocks==1.7.1 python-dateutil==2.9.0.post0 python-dotenv==1.0.1 python-iso639==2025.2.18 python-magic==0.4.27 python-oxmsg==0.0.2 pytz==2025.1 PyYAML==6.0.2 RapidFuzz==3.12.2 referencing==0.36.2 regex==2024.11.6 requests==2.32.3 requests-oauthlib==2.0.0 requests-toolbelt==1.0.0 rich==13.9.4 rpds-py==0.23.1 rsa==4.9 safetensors==0.5.3 scikit-learn==1.6.1 scipy==1.15.2 selenium==4.29.0 sentence-transformers==3.4.1 shellingham==1.5.4 six==1.17.0 smmap==5.0.2 smolagents==1.10.0 sniffio==1.3.1 sortedcontainers==2.4.0 soupsieve==2.6 SQLAlchemy==2.0.39 starlette==0.46.1 streamlit==1.43.2 sympy==1.13.1 tenacity==9.0.0 threadpoolctl==3.5.0 tokenizers==0.21.0 toml==0.10.2 tomli==2.2.", "mimetype": "text/plain", "start_char_idx": 2068, "end_char_idx": 4213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51839049-54bb-48b3-a223-0a38e5f3eb84", "embedding": null, "metadata": {"issue_id": 969, "title": "[BUG] quantization_config is not supported in TransformersModel", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "386aa1df-32be-4568-b85d-a2269448de3b", "node_type": "4", "metadata": {"issue_id": 969, "title": "[BUG] quantization_config is not supported in TransformersModel", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "4ddf95e0d78e97da8999e91ea0619aa9b3c7bfd2b7df0d864961af1e7a6fc5c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df1a8116-355d-4513-9409-37f23167275f", "node_type": "1", "metadata": {"issue_id": 969, "title": "[BUG] quantization_config is not supported in TransformersModel", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "f3865cd4d11a51a91dd66cb5c8964bf23e4883d13db015a88417dfca8c7b2024", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0.0 rich==13.9.4 rpds-py==0.23.1 rsa==4.9 safetensors==0.5.3 scikit-learn==1.6.1 scipy==1.15.2 selenium==4.29.0 sentence-transformers==3.4.1 shellingham==1.5.4 six==1.17.0 smmap==5.0.2 smolagents==1.10.0 sniffio==1.3.1 sortedcontainers==2.4.0 soupsieve==2.6 SQLAlchemy==2.0.39 starlette==0.46.1 streamlit==1.43.2 sympy==1.13.1 tenacity==9.0.0 threadpoolctl==3.5.0 tokenizers==0.21.0 toml==0.10.2 tomli==2.2.1 torch==2.6.0 tornado==6.4.2 tqdm==4.67.1 transformers==4.49.0 trio==0.29.0 trio-websocket==0.12.2 triton==3.2.0 typer==0.15.2 typing-inspect==0.9.0 typing-inspection==0.4.0 typing_extensions==4.12.2 tzdata==2025.1 unstructured==0.16.25 unstructured-client==0.31.1 urllib3==2.3.0 uvicorn==0.34.0 uvloop==0.21.0 watchdog==6.0.0 watchfiles==1.0.4 webencodings==0.5.1 websocket-client==1.8.0 websockets==15.0.1 wrapt==1.17.2 wsproto==1.2.0 yarl==1.18.3 zipp==3.21.0 zstandard==0.23.0 **Additional context** I want to fit the model into my GPU using 8 bit quantization because standard 16 bit float is too large.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 3806, "end_char_idx": 4901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aaa5c344-963a-42f8-9495-5c95f0fb6ede", "embedding": null, "metadata": {"issue_id": 967, "title": "List of Free Local LLM Models - Up and Running", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a8729cb6-ab29-45f0-9ac4-6ed5de1c277a", "node_type": "4", "metadata": {"issue_id": 967, "title": "List of Free Local LLM Models - Up and Running", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "31f23d8fbb410aa5d349dfab9158816357654983fa464d6c64dc0e53a67cf614", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: List of Free Local LLM Models - Up and Running\n\nDescription: **Is your feature request related to a problem? Please describe.** Can I get a list of free local LLM models I can run without the PRO subscription. **Describe the solution you'd like** A list of the args I can pass into HfApiModel() (most likely) so that I don't run into this error: [CODE_BLOCK] **Is this not possible with the current options.** I've tried DeepSeek model, which I thought would be free - but turns out even that is connected to the Together API. How can I get a local free LLM model up and running with smolagents? Maybe this code sample can be provided in the README section.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 772, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "699d56bf-d080-4595-a888-aefe2779b5c8", "embedding": null, "metadata": {"issue_id": 965, "title": "[BUG] Gemma3 Compatibility", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0840b0c2-b42f-4bb5-b19d-fd95126d1ac3", "node_type": "4", "metadata": {"issue_id": 965, "title": "[BUG] Gemma3 Compatibility", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "847ff550701417cb89d6fed6657c65441ef607c2f4c50aac0ac4d59349ce8c71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Gemma3 Compatibility\n\nDescription: Description I encountered an error when using gemma3 models with the smolagents library. - AttributeError: 'Gemma3Config' object has no attribute 'vocab_size' The Issue In src/smolagents/models.py, [CODE_BLOCK] I recommend adding an appropriate exception block, as shown in the example below. [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94a8f603-2921-4fb2-a465-0e0a70f831ff", "embedding": null, "metadata": {"issue_id": 960, "title": "[BUG] Prompt never properly triggers invocation of managed agent", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d8c13c2-24ce-41a0-a399-c61896dc1f47", "node_type": "4", "metadata": {"issue_id": 960, "title": "[BUG] Prompt never properly triggers invocation of managed agent", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "b5451ce7a987bba0cb7648885a6ca47dc8432d3c78144a05c8745d95266e210f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Prompt never properly triggers invocation of managed agent\n\nDescription: **Describe the bug** I have a simple multi-agent setup as in the example (see below). No matter what question I ask it I can never get the model to trigger the managed agent. I am using the o3-mini reasoning model from Azure OpenAI. **Code to reproduce the error** [CODE_BLOCK] **Additional context** Strangely if I copy/paste the same system prompt and user prompt into the same exact Azure OpenAI model, it behaves as expected !Image !Image\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 607, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b113ae4d-f818-40cf-a444-94ce10de0aa7", "embedding": null, "metadata": {"issue_id": 955, "title": "[BUG] Contradiction between comments and implementation in plan updating logic", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c47f0e68-e8e2-4396-9d30-6a1a4f417342", "node_type": "4", "metadata": {"issue_id": 955, "title": "[BUG] Contradiction between comments and implementation in plan updating logic", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "e5480fb47317a2f52e757dba8731d63acbec5c93d484ba2cd646766eac9bd11a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Contradiction between comments and implementation in plan updating logic\n\nDescription: Description In the _generate_updated_plan method of the MultiStepAgent class. There's a direct contradiction between the code comments and the actual implementation, which likely causes the planning functionality to behave differently than intended. The Issue In agents.py, [CODE_BLOCK] However, when examining how write_memory_to_messages() and PlanningStep.to_messages() work together, I discovered that setting summary_mode=False (the default when not specified) actually includes previous plans rather than excluding them. Looking at memory.py, the PlanningStep.to_messages() method shows: [CODE_BLOCK] This means the condition is the exact opposite of what the comment suggests: when summary_mode=False, plans ARE included, not excluded\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 922, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd0fdb7b-e52e-4d37-be95-4bc956e663ff", "embedding": null, "metadata": {"issue_id": 953, "title": "Gradio UI fails to run with unnamed agents", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28937e90-9bd1-4f8a-a3e4-a0c880e8d1fe", "node_type": "4", "metadata": {"issue_id": 953, "title": "Gradio UI fails to run with unnamed agents", "state": "closed", "labels": [], "type": "issue"}, "hash": "fed602d27988435a87d879961da95e0707a2d913772e971b56ca30b37a5605bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Gradio UI fails to run with unnamed agents\n\nDescription: When a GradioUI instance is created with an unnamed agent, the following error is raised: [CODE_BLOCK] This is caused by line: https://github.com/huggingface/smolagents/blob/812c2d2e798701024d0259e3d46ab4f45a228185/src/smolagents/gradio_ui.pyL267\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 325, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0ac3d3ee-487b-4f41-aaac-f4090f99c530", "embedding": null, "metadata": {"issue_id": 952, "title": "Add Couchbase as a RAG example", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1c7cced-0338-4177-aa61-833ab2bff13a", "node_type": "4", "metadata": {"issue_id": 952, "title": "Add Couchbase as a RAG example", "state": "open", "labels": [], "type": "issue"}, "hash": "eef365e7d86ea5aa5ed0cd2529324686cfdc9143688e59f3a61e24492fb70588", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add Couchbase as a RAG example\n\nDescription: Couchbase is an award-winning NoSQL database with built-in vector search capabilities, and I'd love to add it as a RAG example to smolagents under the examples directory! I want to clarify if it's alright to add a jupyter notebook for a step-by-step walkthrough.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ad240fe-9b90-4689-b05b-02a86e76e910", "embedding": null, "metadata": {"issue_id": 945, "title": "Enhanced memory module with integrations", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db5ac989-b1db-43c9-82a2-ca04f8976bae", "node_type": "4", "metadata": {"issue_id": 945, "title": "Enhanced memory module with integrations", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "959234bcfaf009392746fa59a08f8680440bdf9938de4be1ea749209c27917c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Enhanced memory module with integrations\n\nDescription: **Is your feature request related to a problem? Please describe.** Right now memory is only in-memory. We would like to store memory in a storage solution, so agents can resume and replay conversations **Describe the solution you'd like** Either having a pluggable memory module or integrating something like mem0. A tight interface would allow extending and injecting it into agents. **Is this not possible with the current options.** No This feature can also help with cross agent context conversations using RAG\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "709aa062-7bd9-44ac-9b7c-db289496ea53", "embedding": null, "metadata": {"issue_id": 944, "title": "[BUG] Error in generating model output: Failed to deserialize the JSON body into the target type", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "343b63d9-d9eb-425f-a259-bf3f39ea2857", "node_type": "4", "metadata": {"issue_id": 944, "title": "[BUG] Error in generating model output: Failed to deserialize the JSON body into the target type", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "bce5765ec24dc1ccd91c16ff0167edb7aea14aa94ce7c106d3ef8502f9fead55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Error in generating model output: Failed to deserialize the JSON body into the target type\n\nDescription: <img width=\"812\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/fe16f02b-95ff-47f5-8488-f179198c4775\" /> **Describe the bug** When running a text-to-SQL query using the DeepSeek model with the SmolAgent tool, the model correctly executes the first step, which returns a valid result ('Woodrow Wilson'), but fails to return the final answer in subsequent steps. The error logs indicate a Failed to deserialize issue, where the system expects a string but receives an invalid type (sequence). **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** Execution logs: [CODE_BLOCK] **Expected behavior** The model should return the final answer (in this case, the name of the client with the most expensive receipt) after executing the query. **Packages version:** openinference-instrumentation-smolagents==0.1.6 smolagents==1.10.0 **Additional context** None\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1078, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba649b71-374f-4722-865e-296f3dced341", "embedding": null, "metadata": {"issue_id": 940, "title": "[BUG] \"error: metadata-generation-failed\" when installing smolagents[transformers]", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce005d63-5116-4fdf-ba3d-aaa7ddc47578", "node_type": "4", "metadata": {"issue_id": 940, "title": "[BUG] \"error: metadata-generation-failed\" when installing smolagents[transformers]", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "bc1c72e96b9c985878fcf7c748f34b92ef705f615f0dcded3ed9d85f4798a653", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] \"error: metadata-generation-failed\" when installing smolagents[transformers]\n\nDescription: **Describe the bug** Running \"pip install smolagents[transformers]\" on my mac failed. The error I received was \"error: metadata-generation-failed\" ... debugging found error is triggered when trying to build install the \"accelerate\" package. I have fixed the problem by installing smolagents[torch] first, then smolagents[transformers]. So I propose to update smolagents pyproject.toml to install these dependencies in such order. I'm making a PR with such proposal for your review. I'm running in a newly python virtual environment using venv. Python version 3.13. MacOS Sequoia 15.3.1. **Code to reproduce the error** Steps to reproduce: 0. Make sure your don't have Pytorch installed globally 1. Start with a new Python virtual environment 2. Install smolagents[all] (ie. pip install smolagents[all]) 3. You should see the error come up when trying to insteall \"accelerate\" python package **Error logs (if any)** (ai_agent) MacBook-Pro-8:~ USER$ pip install smolagents[transformers] Requirement already satisfied: smolagents[transformers] in ./.python_venv/ai_agent/lib/python3.13/site-packages (1.10.0) ....[redacted for conciseness]... Collecting accelerate (from smolagents[transformers]) Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB) Collecting transformers<4.49.0,>=4.0.0 (from smolagents[transformers]) Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB) ....[redacted for conciseness]... INFO: pip is looking at multiple versions of accelerate to determine which version is compatible with other requirements. This could take a while. Collecting accelerate (from smolagents[transformers]) Using cached accelerate-1.3.0-py3-none-any.whl.metadata (19 kB) Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB) Using cached accelerate-1.2.0-py3-none-any.whl.metadata (19 kB) Using cached accelerate-1.1.1-py3-none-any.whl.metadata (19 kB) Using cached accelerate-1.1.0-py3-none-any.whl.metadata (19 kB) Using cached accelerate-1.0.1-py3-none-any.whl.metadata (19 kB) Using cached accelerate-1.0.0-py3-none-any.whl.metadata (19 kB) INFO: pip is still looking at multiple versions of accelerate to determine which version is compatible with other requirements. This could take a while. Using cached accelerate-0.34.2-py3-none-any.whl.metadata (19 kB) Using cached accelerate-0.34.1-py3-none-any.whl.metadata (19 kB) Using cached accelerate-0.34.0-py3-none-any.whl.metadata (19 kB) Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB) Collecting numpy>=1.26.0 (from pandas>=2.2.3->smolagents[transformers]) Using cached numpy-1.26.4.tar.gz (15.8 MB) Installing build dependencies ... done Getting requirements to build wheel ... done Installing backend dependencies ... done Preparing metadata (pyproject.toml) ... error error: subprocess-exited-with-error \u00d7 Preparing metadata (pyproject.toml) did not run successfully. \u2502 exit code: 1 \u2570\u2500> [342 lines of output] ....[redacted for conciseness]... note: This error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed \u00d7 Encountered error while generating package metadata. \u2570\u2500> See above for output. **Expected behavior** I expect smolagents[all] and smolagents[transformers] to install successfully. **Packages version:** smolagents==1.3.0 **Additional context** I'm running in a newly python virtual environment using venv. Python version 3.13. MacOS Sequoia 15.3.1. I have found a potential solution by switching the order of package installations in pyproject.toml. I'll open a PR for your consideration.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3a9b3cea-dd6c-4de0-957d-9b84f0081d00", "embedding": null, "metadata": {"issue_id": 939, "title": "Passing arguments to tools when using remote executor", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42d65fde-525d-4686-859e-59e16d676806", "node_type": "4", "metadata": {"issue_id": 939, "title": "Passing arguments to tools when using remote executor", "state": "open", "labels": [], "type": "issue"}, "hash": "b69e778040632881e765250dd3512de5944ea5d5ed1760faf4249320615871e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Passing arguments to tools when using remote executor\n\nDescription: What is the best way to pass parameters to tools when using remote executor like docker? For example if I had a custom tool which needs an api key, how would I pass that to the tool? When I run the agent code locally without docker I can create an instance of the tool with parameters and pass the instance to the CodeAgent like this: [CODE_BLOCK] However, when using executor_type=\"docker\" this does not work. I believe this is the case because the instance of the class is not passed to the remote executor but the tool code instead.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6c468277-a73d-4e24-8469-33789b7a0200", "embedding": null, "metadata": {"issue_id": 938, "title": "[BUG] Sequential tool calls unreliable with LiteLLM ollama_chat", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ddabcb2d-2b19-4140-97a8-89b647da4031", "node_type": "4", "metadata": {"issue_id": 938, "title": "[BUG] Sequential tool calls unreliable with LiteLLM ollama_chat", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "cdacc155ec80a1342865008d8ca976e7fbefdd7e97074f737d2dfb44f37e4d9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Sequential tool calls unreliable with LiteLLM ollama_chat\n\nDescription: **Describe the bug** I'm trying to create an example of using ToolCallingAgent to solve a puzzle that requires multiple function calls to solve. I haven't been able to determine exactly why it does not work, here are my observations: After the first or second tool call, I start getting the following error for most of the remaining tool calls [CODE_BLOCK] Looking into the logs, I see entries like [CODE_BLOCK] The only thing that looks wrong about this, to me, is the prepending of 'Calling tools:\\n'. The first thing I tried was a custom tool_parser, as I noticed that parse_json_tool_call doesn't handle this prefix. However, the custom tool parser appears to never get called. Since the model seems to be learning this format from the chat history, I also tried removing the prefix in ActionStep.to_messages, this fixed the extra prefix, but the tool call is still being returned as a string under ChatMessage(content=. I got the same error regardless of whether the function accepts arguments. I am trying tools without args to narrow down the issue. I noticed that the system prompt warns against calling a function multiple times with the same argument, this is not relevant for my use case, so I tried removing that part, but the above issues remain. I also tried CodeAgent, but it will only succeed if I set planning_interval, otherwise the LLM writes an invalid solution and lies about succeeding. However, if I set planning_interval, then ToolCallingAgent is also able to make a correct plan, it just fails to execute it. **Code to reproduce the error** I've seen the exact same issue with multiple models including qwen2.5:7b/14b, qwen2.5-coder:7b/14b, and mistral-nemo:12b [CODE_BLOCK] **Error logs (if any)** Provide error logs if there are any. **Expected behavior** - The failed tool call message should be more detailed - A model that can call a tool successfully once, should be able to keep calling tools* - Changing the formatting of the tool call history shouldn't affect how the model calls tools* - tool_parser should warn that it doesn't do anything *I understand these two might fall under \"LLM is dumb\", but I don't have enough information to determine if the issue is with the model, smolagents, or LiteLLM. **Packages version:** [CODE_BLOCK] **Additional context** I have only tested this with LiteLLM ollama_chat, I don't know if the issue exists with other providers.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e51c222a-5ee0-4f2b-ae9c-d87f26637788", "embedding": null, "metadata": {"issue_id": 934, "title": "Inconsistency with summary_mode for MultiStepAgent._generate_updated_plan", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ce48392-ca84-4877-ac35-17579a6a1049", "node_type": "4", "metadata": {"issue_id": 934, "title": "Inconsistency with summary_mode for MultiStepAgent._generate_updated_plan", "state": "closed", "labels": [], "type": "issue"}, "hash": "2c658697e982821cc686a265ccb1eba1e3709871a11ad509408e8059784d3c3f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Inconsistency with summary_mode for MultiStepAgent._generate_updated_plan\n\nDescription: In MultiStepAgent._generate_updated_plan it mentions that summary_mode is False so as to not extract the previous planning steps from memory. https://github.com/huggingface/smolagents/blob/2f300dca915d395a24da76e26508115293338961/src/smolagents/agents.pyL432-L435 However, in PlanningStep.to_messages, the previous plan is added to the message history if summary_mode is False. https://github.com/huggingface/smolagents/blob/2f300dca915d395a24da76e26508115293338961/src/smolagents/memory.pyL160-L166 The comment in the above snippet also references hiding the step from a model writing a plan, which means that when writing a plan, summary_mode should be set to True. Am I missing something out here?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d9a497b5-a080-4ea8-9fed-0ebc67837713", "embedding": null, "metadata": {"issue_id": 932, "title": "Broken link in README", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29ff11fd-d2c9-4dca-9a65-4f2400c5d52b", "node_type": "4", "metadata": {"issue_id": 932, "title": "Broken link in README", "state": "closed", "labels": [], "type": "issue"}, "hash": "db1be84eb7630dd0c5a1c24888b90bc647e9852fe846ebbc573ef93db5ac0dea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Broken link in README\n\nDescription: **Describe the bug** The link for benchmarking code is broken in the README **Expected behavior** Provide correct link - https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/run.py\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "929428de-77b6-41f8-a69d-4e530a662164", "embedding": null, "metadata": {"issue_id": 928, "title": "[BUG]ImportError: cannot import name 'ManagedAgent' from 'smolagents'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "525aea68-ad37-4669-9cce-75949de1a9d3", "node_type": "4", "metadata": {"issue_id": 928, "title": "[BUG]ImportError: cannot import name 'ManagedAgent' from 'smolagents'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "5fa0736331e42500aa561f044e2768455ec754b939a8e85c7b00ddce69cfc844", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]ImportError: cannot import name 'ManagedAgent' from 'smolagents'\n\nDescription: **Describe the bug** A clear and concise description of what the bug is. **Code to reproduce the error** The simplest code snippet that produces your bug. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6d94f10d-f780-452b-86d7-243a59fdb04e", "embedding": null, "metadata": {"issue_id": 925, "title": "[BUG] Duplicate Name Error in CodeAgent", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b8723b4-9dd1-45bf-80e6-fd4a0b199682", "node_type": "4", "metadata": {"issue_id": 925, "title": "[BUG] Duplicate Name Error in CodeAgent", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "6197b4abe9886a22bfae9fa33561578a5912f01114e99a5a150f12c99cbe46b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Duplicate Name Error in CodeAgent\n\nDescription: \ud83d\udee1\ufe0f Bug Report: Duplicate Name Error in CodeAgent Summary Creating a CodeAgent using tools or managed agents with duplicate names results in a confusing error. This typically happens when a tool like DuckDuckGoSearchTool() uses a default name that unintentionally conflicts with the name given to the agent. Error Message [CODE_BLOCK] Code to Reproduce [CODE_BLOCK] Expected Behavior - Either automatically rename conflicting components (e.g., append a suffix), **or** - Raise a more helpful error message that clearly states which component (tool vs agent) has a conflict. Workaround Manually provide unique names to each agent and tool: [CODE_BLOCK] Suggested Fix - Improve the error message to specify which component names are clashing. - Optionally, allow automatic name disambiguation if a flag like auto_rename=True is enabled. Environment - smol-agents version: 1.10.0 - Python version: 3.10+ - OS: Windows 10 Let me know if you'd like a PR to help improve the error handling!\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1125, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3078ef3d-22f6-43e7-968b-94d203fd4dfa", "embedding": null, "metadata": {"issue_id": 921, "title": "\"Reached Max Steps\" Error in Reasoning model", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2d765da-7d2a-4c40-acab-b77c802e98db", "node_type": "4", "metadata": {"issue_id": 921, "title": "\"Reached Max Steps\" Error in Reasoning model", "state": "closed", "labels": [], "type": "issue"}, "hash": "db203a99cbd1eb9aa8b213b5c403ee74ce79e23d7798ac082da175bbe5bb92d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: \"Reached Max Steps\" Error in Reasoning model\n\nDescription: For LLMs with reasoning capabilities (such as QwQ-32B), it is easy to encounter an error stating \"reached max steps\" before reaching the designated max_steps. However, the memory stores a lot of redundant information from QwQ's reasoning. Is it possible to avoid saving the information within the <think><\\think> tags to the memory at each step?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74370c4f-b7fd-4156-a90d-5afd36a04d89", "embedding": null, "metadata": {"issue_id": 920, "title": "[BUG] Secure Code Agents Documentation Dockerfile not working", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4164ee38-53ce-4605-8c6f-9018a4e2d343", "node_type": "4", "metadata": {"issue_id": 920, "title": "[BUG] Secure Code Agents Documentation Dockerfile not working", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "83e555d1960adfe72b49e9f890202ab1e48c32051a54180914152ec145e1ef54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Secure Code Agents Documentation Dockerfile not working\n\nDescription: **Describe the bug** The secure code agents example about docker didn't work for me The Dockerfile was not enough to build an image **Code to reproduce the error** Copy paste the Dockerfile and code from the last example and try to run. Didn't work **Error logs (if any)** Errors were multiple but mainly smolagents requires libraries that needs to build and the image were missing build ing tools like cargo, cmake and clang **Expected behavior** I would expect that if I copy paste some code especially about Docker, which by default is for easy reproducibility it should work out of the box **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** I am not the most expert in dockerize python so I have used some help from ChatGPT to get a final working image Working Dockerfile [CODE_BLOCK] maybe make sense to update the documentation thank you\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "571b7c79-1bd8-408e-93cb-49bdcfdef264", "embedding": null, "metadata": {"issue_id": 919, "title": "Add more detail to code execution documentation", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "930c4bf9-d477-4deb-bfc7-bd2858e37658", "node_type": "4", "metadata": {"issue_id": 919, "title": "Add more detail to code execution documentation", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "c73e43b1b4c217b15385ccdd45064b8e66bcdbb2d7eba1ae1e49309ef06bdfd1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add more detail to code execution documentation\n\nDescription: We could add more detail about the two alternatives for sandboxing: 1. Use a built-in executor by passing executor_type upon agent initialization 2. Executing everything in remote <img width=\"1243\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1a7cb0d3-eea7-4d49-a352-d274d8241acd\" /> Adding this graph could help: https://excalidraw.com/json=c1Kki8Qahyo8SPDCCX_Hl,YA1hRJkygHSLVhMWkuekrQ\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 578, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5075b087-3279-469a-988e-a211ad8f51b6", "embedding": null, "metadata": {"issue_id": 917, "title": "Memory/checkpointer to remember past interactions?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2646a40d-84f8-4fa1-b58f-ba58d70cc54b", "node_type": "4", "metadata": {"issue_id": 917, "title": "Memory/checkpointer to remember past interactions?", "state": "closed", "labels": [], "type": "issue"}, "hash": "7fb81daa88ebf62124e90f56a8941d7c809d641ed148f848045e653810bd5de5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Memory/checkpointer to remember past interactions?\n\nDescription: Hey all, coming from langgraph, wondering what the equivalent of memory checkpointing is here. Meaning, do we have a way of doing the following [CODE_BLOCK] Thank you, and please let me know if there is a more appropriate place.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 315, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d73bf814-8f79-4b0f-99c2-06cae955dcd3", "embedding": null, "metadata": {"issue_id": 915, "title": "Python 3.9 Support?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "166cd993-2ac3-42e9-849e-07e36fef2357", "node_type": "4", "metadata": {"issue_id": 915, "title": "Python 3.9 Support?", "state": "closed", "labels": [], "type": "issue"}, "hash": "d3e840e73a95011b5dd9a4a0cab42d8c4c064b3330ab27906d68801dfadc2022", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Python 3.9 Support?\n\nDescription: Hello, I'm the author of txtai. A couple releases ago I added support for the Transformers Agents framework and it's been working well. I recently saw that this is going to be deprecated in favor of smolagents. I have no issue migrating as smolagents looks basically the same but it's been actively developed with some new features. The only issue is that I've always followed the same pattern as Transformers with supporting the oldest but supported Python version, which is 3.9. This library though requires Python 3.10. So there really isn't an upgrade path unless I force txtai to require 3.10 which seems heavy handed. Thoughts?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58872c41-f0c6-462b-932e-5df75e288cd1", "embedding": null, "metadata": {"issue_id": 913, "title": "[BUG] Tool validation with executor type defined", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11f45c45-3562-494e-a2de-5467de6c198f", "node_type": "4", "metadata": {"issue_id": 913, "title": "[BUG] Tool validation with executor type defined", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "b0862fcf5d28aff37a946f5d842105780d4b332c433ce039d6074ddd0cda360f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Tool validation with executor type defined\n\nDescription: **Describe the bug** Currently on the version v1.10.0. I can run an agent without the executor_type set and all the tools I need. Once I use executor_type to docker or e2b, I get errors. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** This work if the line executor_type='docker', is removed from the agent, so expectation is that it also works with the executor. **Packages version:** \"smolagents[docker,e2b]>=1.10.0\", **Additional context** None\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42a26701-4d95-4c3b-8062-77ad0caabf36", "embedding": null, "metadata": {"issue_id": 912, "title": "[BUG] AttributeError in tool calling", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e4e8355-29f7-414b-9bd5-2850d56e4b83", "node_type": "4", "metadata": {"issue_id": 912, "title": "[BUG] AttributeError in tool calling", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f72468624e16fafb4cf53149fd99f38fc86d2d4ec74ae7abc12ae758d7dcca64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] AttributeError in tool calling\n\nDescription: Minimum reproducible example: [CODE_BLOCK] This raises: [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d764c2de-4ea3-496f-ac17-1fe038afca10", "embedding": null, "metadata": {"issue_id": 908, "title": "[BUG] OpenAIServerModel doesn't work with vLLM serve properly", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28663047-ee1f-4a20-9f95-32ec8584b917", "node_type": "4", "metadata": {"issue_id": 908, "title": "[BUG] OpenAIServerModel doesn't work with vLLM serve properly", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "f7db5db9a88cca7108e99381fef46dba4de952818901e48a96593e4750ceecdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] OpenAIServerModel doesn't work with vLLM serve properly\n\nDescription: **Describe the bug** I have deployed the Qwen 2.5 32B model using vllm serve. I created an OpenAIServerModel and am trying to run an agent on it. However, I get an error on the first and subsequent steps **Code to reproduce the error** model = OpenAIServerModel( model_id=\"large\", api_base=llm_api_url, api_key=os.getenv(\"LLM_API_KEY\"), temperature=0.2, ) agent = CodeAgent( tools=[], add_base_tools=False, model=model, max_steps=10, additional_authorized_imports=[ \"json\", \"pandas\", \"sqlite3\", ], ) agent.run(\"How to make pizza ?\") **Error logs (if any)** Error in generating model output: Error code: 500 - {'message': 'Internal server error'} **Expected behavior** I expect that i have no error :)) **Packages version:** smolagents==1.9.2 **Additional context** I figured out how to fix this. The error occurs due to the **flatten_messages_as_text** parameter in the models.py script. If it is manually set to True by default, the issue is resolved. However, I haven't found a way to set it via kwargs or other methods :(\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "493bc527-07d5-445d-ae00-f28ab8bb1b2a", "embedding": null, "metadata": {"issue_id": 902, "title": "How to populate custom variables in prompt template?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eeb659fd-f3f8-42e6-9b41-65bc9f169197", "node_type": "4", "metadata": {"issue_id": 902, "title": "How to populate custom variables in prompt template?", "state": "closed", "labels": [], "type": "issue"}, "hash": "9f586e6c90311e0cdaa14da7ccabf33e7aa6a033591212493afa80069faddb62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to populate custom variables in prompt template?\n\nDescription: I'm trying to configure custom template variables in my system prompt. **Current Implementation:** 1. I have a system prompt template with custom variables: [CODE_BLOCK] 2. Agent creation and configuration: [CODE_BLOCK] 3. Calling the agent: [CODE_BLOCK] **Questions:** 1. What's the correct way to populate template variables like {{ bot_name }} and {{ formatting_guidelines }} in the system prompt? 2. How do I handle dynamic variables like conversation_history that change with each request? **Environment:** - smolagents v1.10.0 - Python 3.10+ - FastAPI integration\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 658, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d450d29a-96c1-4428-86a3-35e9f0684a7e", "embedding": null, "metadata": {"issue_id": 901, "title": "[Feature] Agent memory/history consolidation after a number of interactions", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67b338bd-b8c7-4580-8220-23c8acbf623a", "node_type": "4", "metadata": {"issue_id": 901, "title": "[Feature] Agent memory/history consolidation after a number of interactions", "state": "open", "labels": [], "type": "issue"}, "hash": "c2f452faa58bb6685c47e62d31c184e4242cf0946b60b15b3622c2da1172602c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Feature] Agent memory/history consolidation after a number of interactions\n\nDescription: In the current version it appears that agents maintain a history of every interaction and in some manner propagete it in the prompt as time goes on, effectively having memory. Over time this results in a growing context size, and in applications with a lot of communication it will evenutally exceed the context window size. The management of memory, thus context size, is important because it translates limitations in terms of cost, speed, model choice etc. Other agentic LLM applications like Claude Code deal with this by summarizing the context every few steps or remembering only a limited history. To be able to do appropriate management of memory we would need some memory related tooling exposed in smolagents. Right now this is a limitation for more advanced agent applications.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 898, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73d884e0-83de-4ddd-a791-8930c5e96e2a", "embedding": null, "metadata": {"issue_id": 899, "title": "[BUG] Error during jinja template rendering: UndefinedError: 'tool_descriptions' is undefined", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46579d9d-9a84-4747-9368-a7334e77361e", "node_type": "4", "metadata": {"issue_id": 899, "title": "[BUG] Error during jinja template rendering: UndefinedError: 'tool_descriptions' is undefined", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "17c7a7d95104b9533b8c09b221cb7df64fe6d94734bd6d3787ccb546cba6ba9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Error during jinja template rendering: UndefinedError: 'tool_descriptions' is undefined\n\nDescription: **Describe the bug** I have the message \" Error during jinja template rendering: UndefinedError: 'tool_descriptions' is undefined \" when i customize the prompt **Code to reproduce the error** [CODE_BLOCK] **Expected behavior** Should run correctly **Packages version:** smolagents==1.10.0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1fe925be-3c67-488a-80a3-7478b7fe4ba0", "embedding": null, "metadata": {"issue_id": 897, "title": "Support transformers 4.49.0", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abf28f45-a426-42af-b4cd-226a66b02335", "node_type": "4", "metadata": {"issue_id": 897, "title": "Support transformers 4.49.0", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "c0bad84336b3c3eb718a952898719b78bb9bcc33babf3b30dd10f20ec01fcde5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support transformers 4.49.0\n\nDescription: **Is your feature request related to a problem? Please describe.** Currently, we pin transformers<4.49.0 - 693 to avoid an error raised when using VLMs: TypeError: LlavaProcessor: got multiple values for keyword argument 'images' - 692 **Describe the solution you'd like** We should investigate the root cause and fix it, so we can unpin transformers.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6bb7af32-7ad8-4571-b340-109810984be5", "embedding": null, "metadata": {"issue_id": 896, "title": "[BUG]VLM reports \"got multiple values for keyword argument 'images'\"", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "031fe0e2-c8cc-4dcd-9b17-d9baa1c78860", "node_type": "4", "metadata": {"issue_id": 896, "title": "[BUG]VLM reports \"got multiple values for keyword argument 'images'\"", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "hash": "ebe24b6173c70d82fc2dc81939ed026f3d77834758d62c41726f9cf2d7db1cb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]VLM reports \"got multiple values for keyword argument 'images'\"\n\nDescription: **Describe the bug** When I run VLM ,there is a error \"**got multiple values for keyword argument 'images'**\" My code is: [CODE_BLOCK] **Code to reproduce the error** I debuged that this error is caused from model.py --> TransformersModel.__call__ origin code is: [CODE_BLOCK] I find that the error maybe caused by this line \"**images=images,**\",when I delete this line, it works well **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Packages version is 1.9.2 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug duplicate\n\nCategories: category-bug category-duplicate\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 848, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f153352-024e-487d-aaad-ccdd79bd2a4f", "embedding": null, "metadata": {"issue_id": 884, "title": "[BUG]  When running open deep research, final_answer output is empty", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ccaf41b-6adf-4e89-81ae-0d5cf5c435e2", "node_type": "4", "metadata": {"issue_id": 884, "title": "[BUG]  When running open deep research, final_answer output is empty", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "9b3b3024ae918f9a00eb720b63992d52d686e228cbda522b98d9eadd6a9d49b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]  When running open deep research, final_answer output is empty\n\nDescription: **Describe the bug** I ran run.py in the open deep research folder, but there is no final_answer output. I changed run.py to use for stock portfolio analysis, and placed 3 sub-managed agents under 1 manager agent. If you look at the log where the managed agents are running, there is no problem. However, when the manager agent calls final_answer at the end, the value is empty. This problem did not occur when only 2 managed_agents were used. (I verified that there is no problem with each managed_agents.) If you look at the error log below, there is an output token, but I don't know why final_answer is empty. And if you look at the error log, I set the max step of the manager agent to 12, and final_answer was called in the 4th step, so it did not reach the max step. For reference, we use claude-3.7-sonnet as a model. **Code to reproduce the error** manager_agent = ToolCallingAgent( model=model, tools=[visualizer, TextInspectorTool(model, text_limit)], max_steps=12, verbosity_level=2, planning_interval=10, managed_agents=[ stock_market_agent, market_trends_agent, industry_status_agent ], ) **Error logs (if any)** Provide error logs if there are any. --- </summary_of_work> [Step 3: Duration 255.25 seconds| Input tokens: 82,410 | Output tokens: 18,197] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Calling tool: 'final_answer' with arguments: {} \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Final answer: {} [Step 4: Duration 133.81 seconds| Input tokens: 170,420 | Output tokens: 26,389] Got this answer: {} **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** pip freeze | findstr smolagents openinference-instrumentation-smolagents==0.1.6 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a008e461-8719-4908-b75e-c574e1cff9c2", "embedding": null, "metadata": {"issue_id": 881, "title": "[BUG] removing tools does not work for agent with reset=False", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10bd6948-cf8b-4d6d-82da-4fab8d4d5949", "node_type": "4", "metadata": {"issue_id": 881, "title": "[BUG] removing tools does not work for agent with reset=False", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "b894ee1cda6e6cd0b6ffc941b033d917f2ca20c3b2b0506d26d0bd5b5ff330c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] removing tools does not work for agent with reset=False\n\nDescription: **Describe the bug** As mentioned in https://github.com/huggingface/smolagents/issues/149, tools update/remove can be done with agent.tools dict. But when I set reset=False, the agent does not think I have removed any tools. Adding tools is fine. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** no **Expected behavior** The agent should list the latest tool set. **Packages version:** v1.9.2 **Additional context** I doubt that the agent or llm think on the old memory, and the update may also not work if we just replace the exsisting tool.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 726, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c20c4f6a-c3be-4824-8e79-af18d885212c", "embedding": null, "metadata": {"issue_id": 880, "title": "[Update Required] mcpadapt now support SSE MCP servers", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f172ece5-2be4-40a3-bd63-932c87c1accd", "node_type": "4", "metadata": {"issue_id": 880, "title": "[Update Required] mcpadapt now support SSE MCP servers", "state": "closed", "labels": [], "type": "issue"}, "hash": "d8b39d367445fe2af43fc24b34c4c844305a41987588cc48be6eed9906f91343", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Update Required] mcpadapt now support SSE MCP servers\n\nDescription: Hey, Starting from version 0.0.13 of mcpadapt we now support SSE MCP servers which was a feature request here as well 350. This is a backward compatible change so no big hurry to make the switch but basically we need to update the documentation and the typing of ToolCollection.from_mcp to reflect the fact that users can now either pass StdioServerParams or a dict parameter that will be used to connect the sse client. I will make a PR asap.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "55688939-fdcb-4132-b8dc-d83c2884278f", "embedding": null, "metadata": {"issue_id": 875, "title": "manager agent pipeline", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ecca4cc-6bbe-4901-b309-8eb3f1194fdc", "node_type": "4", "metadata": {"issue_id": 875, "title": "manager agent pipeline", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "bafbdf60399a43a9aad9b9dc57c14ab329e03778d8139d3a046aa9ca65035294", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: manager agent pipeline\n\nDescription: How can I implement some logic in inside the manager agent? For example, an order berween two managed agents, or a human in the loop after final answer of specific agents?\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2abd8e12-e5ec-452b-a0fc-9542422810ac", "embedding": null, "metadata": {"issue_id": 874, "title": "Richer GradioUI for debugging and vsualization", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "696c6343-d1fb-434d-98cd-795f9466161f", "node_type": "4", "metadata": {"issue_id": 874, "title": "Richer GradioUI for debugging and vsualization", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "1d3fda96779e314b53244a1e8bf49dd14273f02ab6e489cc4ff8dc28e46b746b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Richer GradioUI for debugging and vsualization\n\nDescription: Throughout the agent's run, I print several key remarks that I want to display in the Gradio UI. How can I connect them to the Gradio UI?\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b1553c48-d68d-4823-90b4-1ffbca20b084", "embedding": null, "metadata": {"issue_id": 873, "title": "Suggested change to the LiteLLMModel class documentation", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a431c27-0d7a-4251-8e24-d7aff22a0df8", "node_type": "4", "metadata": {"issue_id": 873, "title": "Suggested change to the LiteLLMModel class documentation", "state": "closed", "labels": [], "type": "issue"}, "hash": "91a468a08600dc3c81fec99b4e419f7e98431bd2fa4a3e690e367d3eb17e7636", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Suggested change to the LiteLLMModel class documentation\n\nDescription: Hello! This is a suggested change to the LiteLLMModel class documentation. There are no bugs reported here. LiteLLMModel code: https://github.com/huggingface/smolagents/blob/8849b95df75ca563401f8609d1ae1b268cacf790/src/smolagents/models.pyL822 Smolagents documentation says \"[LiteLLMModel] connects to LiteLLM as a gateway\". However, it is not connecting to a \"LiteLLM Proxy Server\". LiteLLMModel seems to use the \"LiteLLM Python SDK\" to connect directly to the LLM providers (like OpenAI, Azure and even Ollama running locally). See: https://docs.litellm.ai/docs/litellm-python-sdk. So if this is correct, my suggestion for LiteLLMModel documentation would be something like: \"This model uses the LiteLLM Python SDK as a Python client to call hundreds of LLMs.\" Additionally, parameter 'api_base' must point to \"the URL of the target provider\", instead of \"the base URL of the OpenAI-compatible API server\". Related code: https://github.com/huggingface/smolagents/blob/8849b95df75ca563401f8609d1ae1b268cacf790/src/smolagents/models.pyL828 Thanks for the great work!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1159, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d950b5d3-667e-44eb-b1da-e03139382ae6", "embedding": null, "metadata": {"issue_id": 872, "title": "[BUG]Error in generating tool call with model: ChatMessageToolCallDefinition.__init__() got an unexpected keyword argument 'parameters'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d7b26f2-33da-421e-9d33-99c484eb4ef0", "node_type": "4", "metadata": {"issue_id": 872, "title": "[BUG]Error in generating tool call with model: ChatMessageToolCallDefinition.__init__() got an unexpected keyword argument 'parameters'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "28b758f617d1f9f7139abb64f6b0a79aee067015dc8cd601685d4e897af08d88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]Error in generating tool call with model: ChatMessageToolCallDefinition.__init__() got an unexpected keyword argument 'parameters'\n\nDescription: **Describe the bug** When I run open_deep_research run.py , get this error . **Code to reproduce the error** model = OpenAIServerModel( model_id=\"gpt-4o\", api_base=f\"https://oneapi.gptnb.ai/v1/\", api_key=api_key ) document_inspection_tool = TextInspectorTool(model, text_limit) browser = SimpleTextBrowser(**BROWSER_CONFIG) WEB_TOOLS = [ SearchInformationTool(browser), VisitTool(browser), PageUpTool(browser), PageDownTool(browser), FinderTool(browser), FindNextTool(browser), ArchiveSearchTool(browser), TextInspectorTool(model, text_limit), ] text_webbrowser_agent = ToolCallingAgent( model=model, tools=WEB_TOOLS, max_steps=20, verbosity_level=2, planning_interval=4, name=\"search_agent\", description=\"\"\"A team member that will search the internet to answer your question. Ask him for all your questions that require browsing the web. Provide him as much context as possible, in particular if you need to search on a specific timeframe! And don't hesitate to provide him with a complex search task, like finding a difference between two webpages. Your request must be a real sentence, not a google search! Like \"Find me this information (...)\" rather than a few keywords. \"\"\", provide_run_summary=True, ) text_webbrowser_agent.prompt_templates[\"managed_agent\"][\"task\"] += \"\"\"You can navigate to .txt online files. If a non-html page is in another format, especially .pdf or a Youtube video, use tool 'inspect_file_as_text' to inspect it. Additionally, if after some searching you find out that you need more information to answer the question, you can use final_answer with your request for clarification as argument to request for more information.\"\"\" manager_agent = CodeAgent( model=model, tools=[visualizer, document_inspection_tool], max_steps=12, verbosity_level=2, additional_authorized_imports=AUTHORIZED_IMPORTS, planning_interval=4, managed_agents=[text_webbrowser_agent], ) answer = manager_agent.run(\"2022\u65af\u8bfa\u514b\u4e16\u9526\u8d5b\u51a0\u519b\u622a\u81f3\u76ee\u524d\u6709\u591a\u5c11\u4e2a\u6392\u540d\u8d5b\u51a0\u519b?\") **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** smolagents version 1.9.2 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2446, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bcda06a-1dc1-4123-82ad-60c075c9cf8d", "embedding": null, "metadata": {"issue_id": 871, "title": "[BUG] Agents can create infinite loops using functions", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea748136-2464-494d-accd-01d3a82640f1", "node_type": "4", "metadata": {"issue_id": 871, "title": "[BUG] Agents can create infinite loops using functions", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "c4160ed15bdde90755d02a6d18404758728a861b3169488e2f8feccf18ba5efa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Agents can create infinite loops using functions\n\nDescription: Hello, there is a nice protection against infinite loops in the local python interpreter, but unfortunately, it doesn't work if an agent creates a double loop with a second loop in a function. **Code to reproduce the error** Here is an example of a correct interruption at 10000000 operations: [CODE_BLOCK] And the following code proceeds till the end: [CODE_BLOCK] **Expected behavior** Expected InterpreterError: Reached the max number of operations of 10000000. Maybe there is an infinite loop somewhere in the code, or you're just asking too many calculations. in the second case as well. **Packages version:** Affected main\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c2dd7960-35d8-40f5-8194-6d594d878113", "embedding": null, "metadata": {"issue_id": 870, "title": "[BUG] structured output of the final answer.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2c7e589-be5a-4ae0-b0c5-8fb3a0cadfc8", "node_type": "4", "metadata": {"issue_id": 870, "title": "[BUG] structured output of the final answer.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "7ee074a421eedd277382886059d96584832d2d379c7e60e68653bff3358b3929", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] structured output of the final answer.\n\nDescription: I want to get a structured output ofbthe final answer, lets say a number. If i limit the model with grammar i am killing the reasoning of the agent. How can i get both a react agent and structured final answer? Is thi possible for bith code and tool calling agents?\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 412, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf599a82-2fd0-4011-8e68-8c186221a3b5", "embedding": null, "metadata": {"issue_id": 867, "title": "[BUG] CI is broken: AssertionError with message role in test_action_step_to_messages", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "682fff80-0461-4018-860d-4fdd6d276229", "node_type": "4", "metadata": {"issue_id": 867, "title": "[BUG] CI is broken: AssertionError with message role in test_action_step_to_messages", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "5bbffea2d77f0f112d25c67a051b36662d536eba6adcef16f103f49e6ca291a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] CI is broken: AssertionError with message role in test_action_step_to_messages\n\nDescription: **Describe the bug** CI is broken: https://github.com/huggingface/smolagents/actions/runs/13647944821/job/38150144811?pr=861 [CODE_BLOCK] I think the CI was broken with the merge of: - 779\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e91addfb-6c02-4953-89e4-edf5033c1a7e", "embedding": null, "metadata": {"issue_id": 864, "title": "[Feature] Stream Tool execution log", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57221784-4c40-4f65-b8b7-146cf5a65bf6", "node_type": "4", "metadata": {"issue_id": 864, "title": "[Feature] Stream Tool execution log", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "7913d44d79ef5b19c89bd9d3b8e996f90dc9248f745413396e6a523ced36789b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Feature] Stream Tool execution log\n\nDescription: **Describe the bug** It would be helpful to be able to stream the outputs from tool(s) execution related issue: https://github.com/huggingface/smolagents/issues/850 **Code to reproduce the error** The simplest code snippet that produces your bug. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7b3e06f-fbc2-4fba-8c89-14b040e7b9ca", "embedding": null, "metadata": {"issue_id": 860, "title": "[BUG] Error in code parsing", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abca6d74-73bf-4142-b924-81f76bd4b862", "node_type": "4", "metadata": {"issue_id": 860, "title": "[BUG] Error in code parsing", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "0c49022da9e5bb4d4d1c42b7aad944a3d10c33f7740efa2ea85e243d20458f48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Error in code parsing\n\nDescription: **Describe the bug** For CodeAgents, this internal \"error\" seems to be consistent: CODE_BLOCK?\\n(.*?)\\n[CODE_BLOCK] However, it does not look like it should be an error. **Code to reproduce the error** This happens almost every time using CodeAgent. Basically the python execution returns the string with the final answer but the main agent complains bc it does not have that regex\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "64bf3201-6755-4e18-a1a5-74879fc08e9e", "embedding": null, "metadata": {"issue_id": 859, "title": "[BUG] Not able to add custom system prompt", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32e33539-695c-429b-aef1-a313e84da737", "node_type": "4", "metadata": {"issue_id": 859, "title": "[BUG] Not able to add custom system prompt", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "hash": "d975f8a38bd02ee01267d5543223895da271690b077b3179e8d50db71e2bd767", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Not able to add custom system prompt\n\nDescription: **Describe the bug** Hi, I'm trying to add a custom system prompt along with existing one. But no documentation on how to add that in v.1.9.2. Old documents has system_prompt parameter in CodeAgent but looks like code changed to use PromptTemplates. As a workaround i'm adding my custom information during agent.run(\"my_query\", additional_args={\"key_information\":CUSTOM_PROMPT}) **Note:** Instead of 'key_information' if i say 'key_instruction', azure content filter blocks it (default threshold) in Azure. Also, not able to provide the 'key_information' when using **GradioUI**. GradioUI(agent).launch() **Code to reproduce the error** The simplest code snippet that produces your bug. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A cleaner way to add our own custom instruction to adapt to the domain. **Packages version:** Run pip freeze | grep smolagents and paste it here. 1.9.2 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug duplicate\n\nCategories: category-bug category-duplicate\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1164, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d6c18a7-75bc-43a9-a94e-94d7e6b8d62b", "embedding": null, "metadata": {"issue_id": 856, "title": "V1.30", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "384b1996-76a8-48ba-883f-bd49ec8fc169", "node_type": "4", "metadata": {"issue_id": 856, "title": "V1.30", "state": "closed", "labels": [], "type": "issue"}, "hash": "2e203ae69e9ea26458721f8eeb0365db6af7e698ba18afcac1292116b01b31e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: V1.30\n\nDescription: Could you create a toggle to revert the data return method to its state in version 1.3?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "012a225f-5473-479f-8570-fb54928d3f45", "embedding": null, "metadata": {"issue_id": 852, "title": "[BUG] ipython dependency missed", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "acc1542f-1fe2-40b4-b63f-02a8986e8767", "node_type": "4", "metadata": {"issue_id": 852, "title": "[BUG] ipython dependency missed", "state": "closed", "labels": [], "type": "issue"}, "hash": "fc3702c28257c0ccc03fd1e1bd7cd6360a6192ae03f525215dd88eb5866e4ac6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] ipython dependency missed\n\nDescription: \"ipython>=8.31.0\" is set only as test dependency with the comment for interactive environment tests. IPthon is used in utils.py in src directory, so I believe the dependency should be in main project dependencies\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 280, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "446c9516-1f37-486a-b0fc-abe079c2de92", "embedding": null, "metadata": {"issue_id": 850, "title": "[BUG] Managed Agents Stream", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6df63af1-0300-44a8-9e9c-42b5e7b1b947", "node_type": "4", "metadata": {"issue_id": 850, "title": "[BUG] Managed Agents Stream", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "cbab975a6c3c1cb635cd25a643afb7de6e4c0af8e6b55e109768379caeac2f06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Managed Agents Stream\n\nDescription: **Describe the bug** It's not possible to stream step logs from managed agents. This is a huge limitation for multi agent setups (similar to open deep research) which translates in waiting 60+ seconds for an answer without having a clue on what's going on internally. This should be easy to fix since the managed agents' logs are streamed in the terminal. There should be an easy way to access those and stream them. **Code to reproduce the error** The simplest code snippet that produces your bug. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f11194fe-f0c0-4a9a-9b6a-6b2c53ff7c5c", "embedding": null, "metadata": {"issue_id": 844, "title": "Enable Mypy check", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ef59e74f-447b-40d0-941d-95cdf6e3c9a8", "node_type": "4", "metadata": {"issue_id": 844, "title": "Enable Mypy check", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "hash": "c359500fcb3e9970e2c4ada6a7c88b125a6916edfc90d05697529a537da4e2e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Enable Mypy check\n\nDescription: Adding MyPy to the project could be useful for : - Preventing type-related bugs - Enhancing IDE auto-completion - Providing better documentation through type hints - Finding deeper logical issues I'm going to submit a PR to add support for it in Makefile, but adding a specific target. It could be enabled as part of \"quality\" target after errors are fixed (at the moment it found 400+ errors, that need evaluation and fixes step by step and cannot be part of a single PR). I'd be happy (well, not happy but someone need to do this dirty job if we want type checking) to go through errors and submit PRs fixing them. It seems a good way to get more familiar with the whole code.\n\nState: closed\n\nLabels: duplicate\n\nCategories: category-duplicate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 783, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21dbfed1-d3cf-4271-967e-239ab118401c", "embedding": null, "metadata": {"issue_id": 842, "title": "How to pass custom type variables to tools", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5432f9d7-4db7-4979-8903-56466a04042d", "node_type": "4", "metadata": {"issue_id": 842, "title": "How to pass custom type variables to tools", "state": "closed", "labels": [], "type": "issue"}, "hash": "52d17481f0431cd01084cc82cf68029c96339fa6c90e57aeb651630e4209f70e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to pass custom type variables to tools\n\nDescription: I\u2019m working on a Telegram bot and using the smolagents library to create agents that handle reminders. The issue I\u2019m facing is related to passing the context object (which is specific to each message received by the bot) to a tool function (add_reminder). The context object is required to access the job_queue for scheduling reminders. Problem: Even though I\u2019m passing the context variable through the additional_args argument in agent.run, the agent doesn\u2019t seem to pass this variable directly to the code interpreter. Instead, it redefines the variable as None, which causes the rest of the code to fail. Here\u2019s the relevant part of the code: [CODE_BLOCK] When the agent runs, it generates code like this: [CODE_BLOCK] However, the context variable is redefined as None by the agent, resulting in the following error: [CODE_BLOCK] Workaround: The only workaround I\u2019ve found is to define the add_reminder tool inside the function where the context is available. This works but is not ideal because I have to redefine the tool every time I want to use it in different agents. Example: [CODE_BLOCK] Request: Is there a better way to pass custom type variables like context to tools in smolagents? Ideally, I\u2019d like to define the tool once and pass the context object dynamically when the tool is executed. Additional Context: - The context object is specific to the python-telegram-bot library and is required to access the job_queue for scheduling tasks. - The add_reminder tool needs to be reusable across multiple agents without redefining it each time. Any guidance or suggestions would be greatly appreciated!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9636a608-3867-4df4-9090-d5805a2c617c", "embedding": null, "metadata": {"issue_id": 841, "title": "[BUG] VLM doesn't recognize images in TransformersModel due to processor differences", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f7531afa-df5e-43a2-b454-001036491685", "node_type": "4", "metadata": {"issue_id": 841, "title": "[BUG] VLM doesn't recognize images in TransformersModel due to processor differences", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "3ad2af685663766eec060ad8edbe584a6af8a1ccc5f7e52c91201bceca6a9187", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] VLM doesn't recognize images in TransformersModel due to processor differences\n\nDescription: **Describe the bug** I'm unable to get a VLM, specificially HuggingFaceTB/SmolVLM-Instruct, to describe the content of an image due to misalignment between the processor usage in Smolagent's TransformersModel and the \"intended\" usage of the processor. It seems that the processor usage in Smolagent's TransformersModel may only work for specific VLMs instead of all VLMs. **Code to reproduce the error** First, to demonstrate what I would expect as output for a given prompt, I run inference on HuggingFaceTB/SmolVLM-Instruct based on the \"How To Get Started\" code. [CODE_BLOCK] Executing this code cell produces the following output: [CODE_BLOCK] Great, this looks as something that I'd expect. Now, I try something similar with Smolagent's TransformersModel wrapped by the CodeAgent. [CODE_BLOCK] Executing this code cell produces the following output: !Image Note that subsequent steps produce similar outputs. I dove a bit deeper into the code and it seems that the model is extrapolating based on the default system prompt, which seems to cover similar examples about this 55 year old lumberjack called John Doe (see, e.g., https://github.com/huggingface/smolagents/blob/main/src/smolagents/prompts/code_agent.yamlL22-L27). After diving deeper, I inspected the prompt_tensor that gets fed into the model. This tensor did not contain the pixel values and its corresponding mask that you'd typically find while working with images in VLMs. Hence, it seems that the processing of the image didn't happen correctly. After further inspection, this seems to be the case due to the misalignment between the processor in the TransformersModel and what is expected by the model. It is expected to be processed as: [CODE_BLOCK] while it gets processed as: [CODE_BLOCK] By modifying how the processor is used in the TransformersModel (e.g., first apply the chat template on the text and then call the processor on the text and images), I'm able to get a similar response as the one I get when using the model in a standalone manner. However, the generated output is now a textual description of the image without the corresponding Python code blob that we'd expect for the CodeAgent. !Image I have two questions related to the behavior that I observe in this case: 1. How to make Smolagent's TransformersModel compatible with any VLM that may use the processor differently from how it is currently implement in Smolagents. 2. How to further continue my adventure w.r.t. enabling SmolVLM within a CodeAgent; specifically, how to get it to generate a Python blob that returns me the description that it is currently already generating. Could this be due the system prompt not being recognize by the chat template of SmolVLM? Please let me know if anything is unclear regarding this question. I'd be happy to provide further detail if helpful. **Error logs (if any)** N/A **Expected behavior** I would have expected this application to work out-of-the-box and would be curious how to make it work for a variety of VLMs with different processing steps. **Packages version:** I am using a locally installed version of https://github.com/huggingface/smolagents/commit/82e647abb03781358925e0c64635e288bdc7c77b.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3382, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1040168b-249b-4056-9216-26233b20b8ff", "embedding": null, "metadata": {"issue_id": 839, "title": "[BUG] Minor issue with double assign", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45bafa4a-dbc0-45a1-b9f5-fc12291675b0", "node_type": "4", "metadata": {"issue_id": 839, "title": "[BUG] Minor issue with double assign", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "bedfc48e55909058c38397cf5e8d2fd5fa949c9f95ad0c691c7f8c150ed4396d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Minor issue with double assign\n\nDescription: Hello, I am trying to use agents on scientific problems and sometimes Qwen model outputs a weird, but a valid code like a = b = 1, and it fails in the executor with the following exception: [CODE_BLOCK] **Code to reproduce the error** The simplest code snippet that produces the bug: [CODE_BLOCK] **Packages version:** '1.9.2' and main **Possible solution** Remove if check here: https://github.com/huggingface/smolagents/blob/eef2c17c3ec8cb05acba3881454017014043a2f6/src/smolagents/local_python_executor.pyL535 **Expected behavior** The code is valid and the agent should not fail\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "be3f07d3-a14d-48da-85e2-b5fb366e7708", "embedding": null, "metadata": {"issue_id": 838, "title": "[BUG] Regex import in VisitWebpageTool", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "72566c77-b125-4b48-8fbd-34cc2b72bcf8", "node_type": "4", "metadata": {"issue_id": 838, "title": "[BUG] Regex import in VisitWebpageTool", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "91d1d7a2393378b9b8bfac945c1a4dd6a1d7b955ca57f3b1c8030e2c7850e640", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Regex import in VisitWebpageTool\n\nDescription: **Describe the bug** The VisitWebpageTool uses the regex module here but I think it is not available in the sandbox as you can see here in the logging of the agents course\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7cec4e91-b61c-4d2d-9fb0-5b90cb5e0e2e", "embedding": null, "metadata": {"issue_id": 836, "title": "webagent and browser script is broken", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bdb5c89-8032-4cfa-a03b-db421dfacac6", "node_type": "4", "metadata": {"issue_id": 836, "title": "webagent and browser script is broken", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "bc957ad7bbd3b3b271effd0a5d63dc6c386f3da9a7cc9d875447464f960120bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: webagent and browser script is broken\n\nDescription: I have a bit of a backlog to melt until I get to this but changes in memory has broken the browser and thus webagent. easy repro (this used to work) [CODE_BLOCK] what I get (easy fix, perhaps we should also get them from env vars): [CODE_BLOCK] when I fixed this I started getting following one, I was working on releases during memory changes so I don't have a visibility over them, will take a look and fix. [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d78582e-ed3c-41ee-a3cb-1f2a838bf741", "embedding": null, "metadata": {"issue_id": 835, "title": "[BUG] Error reporting message strips away useful error type", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfa9c7c9-a918-42e2-8564-db62a1217099", "node_type": "4", "metadata": {"issue_id": 835, "title": "[BUG] Error reporting message strips away useful error type", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "d3aa6bf130bd2c52a101bca06b8817233766aac064460a5cb85924bf2d92fbef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Error reporting message strips away useful error type\n\nDescription: **Describe the bug** I tried to intentionally tell CodeAgent to output code with syntax error in its tool calling code. It did show the error message, but it omitted the informative error/exception type, i.e. SyntaxError. This is just an illustration, as it could as well be KeyError, NotImplementedError, ImportError, etc. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** SyntaxError: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1) **Packages version:** [CODE_BLOCK]\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 699, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "98f4444d-0ba5-4cc9-9e26-5669dc83eaa9", "embedding": null, "metadata": {"issue_id": 832, "title": "[BUG]Faild rag_using_chromadb.py on Windows 10", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aea9595c-1b3d-46ee-862c-fd2ee480029e", "node_type": "4", "metadata": {"issue_id": 832, "title": "[BUG]Faild rag_using_chromadb.py on Windows 10", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "2757f14c2342bcfb41bb98b834b706d76e65bfea4163bb0881c6522c6af8fdb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]Faild rag_using_chromadb.py on Windows 10\n\nDescription: **Describe the bug** when run rag_using_chromadb.py on Windows 10, quit app at Chroma.from_document **Code to reproduce the error** rag_using_chromadb.py [CODE_BLOCK] **Error logs (if any)** no logs,just quit **Packages version:** smolagents==1.9.2 chroma-hnswlib==0.7.6 chromadb==0.6.3 langchain-chroma==0.2.2 on windows 10 **Additional context** It works only if the chroma_db folder does not exist AND the batch_size is larger than the document size. [CODE_BLOCK] I believe this bug is caused by Chromadb. However, Chromadb's codebase is too complex for me to investigate further. I've given up on examining the code myself. I hope this report will be helpful for others who might encounter the same issue.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 856, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05e131b1-29f8-470f-8098-cfb1089948d8", "embedding": null, "metadata": {"issue_id": 830, "title": "Make system prompt examples consistent with the agent\u2019s actual logs", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7f26971-5ec5-44f9-b4ef-a83d2ac9df44", "node_type": "4", "metadata": {"issue_id": 830, "title": "Make system prompt examples consistent with the agent\u2019s actual logs", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "34686d0a7c278f4acbebfb35885a75f790aa59347c43010f1806684c3b92a946", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Make system prompt examples consistent with the agent\u2019s actual logs\n\nDescription: I\u2019ve noticed a small discrepancy that might affect the model\u2019s performance. In the system prompt examples (code_agent.yaml), each step\u2019s output is shown simply as: [CODE_BLOCK] However, in actual usage the agent output is more detailed, including lines such as: [CODE_BLOCK] This difference can slightly reduce clarity for the large language model, because it doesn\u2019t consistently see the same format it was \u201ctaught\u201d to use in the system prompt examples. **Suggestion** Two possible approaches: 1. **Update the system prompt examples** to show the additional lines, including Call id, Execution logs, etc. 2. **Simplify the actual agent output** so it only presents the simpler \u201cObservation\u201d style, exactly as in the system prompt examples. Either option would help the LLM follow the desired output style more reliably.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7d181f5-c795-45ee-86e3-de01406f6505", "embedding": null, "metadata": {"issue_id": 825, "title": "[BUG] Use FinalAnswerTool to submit final answer after reaching max steps.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9f718e9-a360-49bf-bea4-b6ba46756783", "node_type": "4", "metadata": {"issue_id": 825, "title": "[BUG] Use FinalAnswerTool to submit final answer after reaching max steps.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "a9286c04a892a6aaf9f13d401d77851f886950671fe3ccc9655587748182d2f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Use FinalAnswerTool to submit final answer after reaching max steps.\n\nDescription: **Describe the bug** After the agent reaches max_steps, it calls to provide_final_answer to force the agent to write an answer, but it doesn't use the FinalAnswerTool. This makes the final answer contain many explanations from the model, rather than simply the answer itself. **Expected behavior** Use FinalAnswerTool in when providing final answers. **Packages version:** 1.9.2\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f16a621-fe29-4a1d-bbf0-9137aabcc199", "embedding": null, "metadata": {"issue_id": 819, "title": "Implementation for using deepseek-r1 for GAIA benchmark", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c2b075e-e8c8-4919-9e9c-d968c2bb853e", "node_type": "4", "metadata": {"issue_id": 819, "title": "Implementation for using deepseek-r1 for GAIA benchmark", "state": "closed", "labels": [], "type": "issue"}, "hash": "6b793a313fe95a851816dd53470caf53faa20ce82b7211e97d01f6dbe53dc8ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Implementation for using deepseek-r1 for GAIA benchmark\n\nDescription: Thanks for the great work! I found there are results for deepseek-r1 on benchmarks such as GAIA shown in the section [CODE_BLOCK] , but I can't found how deepseek-r1 is used in benchmark.ipynb, when I try to use it, I got the error that deepseek-r1 does not support tool calling. Do you have any example code to show how to use deepseek-r1?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd28cea8-2fd8-499f-9d1f-d011115488f9", "embedding": null, "metadata": {"issue_id": 816, "title": "Add Podman remote executor as an alternative to Docker", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47284796-8494-4d8e-92a6-efb3656dcb11", "node_type": "4", "metadata": {"issue_id": 816, "title": "Add Podman remote executor as an alternative to Docker", "state": "open", "labels": [], "type": "issue"}, "hash": "ea853be7a4e2dbc0e1774ed4bfef60c3b8e534cc15b32b0c0a227618bde46257", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add Podman remote executor as an alternative to Docker\n\nDescription: Discussed in 761\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1eebcab3-8d01-41ec-88ab-018f6a1e3f72", "embedding": null, "metadata": {"issue_id": 815, "title": "[BUG] Test failures for Remote Docker executor", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b375567-ddcd-4ab7-a758-db2c68848073", "node_type": "4", "metadata": {"issue_id": 815, "title": "[BUG] Test failures for Remote Docker executor", "state": "closed", "labels": [], "type": "issue"}, "hash": "985c0c57bdb55b9d48e2e6f859aacbaa1521c274e1e924a034affa73183ed330", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Test failures for Remote Docker executor\n\nDescription: I got test failures for Docker executor. The bug seems related to how the pattern matching for final_answer is running. I'll provide a PR soon fixing it\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71ab2b1e-e494-4014-beb6-e97e2d8652b3", "embedding": null, "metadata": {"issue_id": 810, "title": "ImportError: cannot import name 'is_soundfile_availble' from 'transformers.utils' in smolagents.types", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b96681fe-bee6-401d-bc54-8eff1e01ff74", "node_type": "4", "metadata": {"issue_id": 810, "title": "ImportError: cannot import name 'is_soundfile_availble' from 'transformers.utils' in smolagents.types", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "84037be953be73b09ee66004dade0b1d3a381f90bfc7e931c9b7895189eb4ce1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: ImportError: cannot import name 'is_soundfile_availble' from 'transformers.utils' in smolagents.types\n\nDescription: **Describe the bug** When importing smolagents, an ImportError occurs due to a typo in smolagents/types.py. The code attempts to import is_soundfile_availble (misspelled) from transformers.utils, when it should be is_soundfile_available. This prevents the module from loading correctly. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** I expected the smolagents module to import successfully without errors, allowing me to use its features like CodeAgent. **Packages version:** - smolagents==0.1.0 **Additional context** - **Python Version**: 3.12 - **Environment**: Miniconda3 on a Linux system - **Workaround**: Manually editing smolagents/types.py (line 23) to change is_soundfile_availble to is_soundfile_available resolves the issue locally. - **Suggested Fix**: Update the smolagents source code to correct the typo in smolagents/types.py. The correct function name in transformers.utils is is_soundfile_available, as suggested by the error message.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06cbaf9b-0980-4cca-b567-b8ce6ddb8a03", "embedding": null, "metadata": {"issue_id": 808, "title": "[BUG] \"o1\" model id isn't found", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44352df4-ec04-4cc9-90a1-6d5b779a25ba", "node_type": "4", "metadata": {"issue_id": 808, "title": "[BUG] \"o1\" model id isn't found", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "cfd948ae273c09aeed8655de5dc4940704204e60b3ec4ef64ced88c463e1f83d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] \"o1\" model id isn't found\n\nDescription: **Describe the bug** It appears that I either don't have access to an OpenAI model with id \"o1\" or it doesn't exist. I can probably pick another model, but not sure why the instructions (https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research) specify \"o1\": litellm.exceptions.NotFoundError: litellm.NotFoundError: OpenAIException - Error code: 404 - {'error': {'message': 'The model o1 does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}} I confirmed lack of \"o1\" model by running (venv) m@M-MacBook-Pro-2 open_deep_research % curl https://api.openai.com/v1/models -H \"Authorization: Bearer $OPENAI_API_KEY\" [CODE_BLOCK] **Code to reproduce the error** Installed and ran Open Deep Search using this page: https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research I created a venv --> edited the activation script to add SERPER_API_KEY, OPENAI_API_KEY, My run command is: python run.py --model-id \"o1\" \"$(cat /path/file.txt)\" where my path is to a file containing the prompt text **Error logs** [CODE_BLOCK] **Expected behavior** I expected Open DeepResearch to run and produce results in the form of OpenAI Deep Research using the specified OpenAI model. **Packages version:** [CODE_BLOCK] **Additional context** N/A\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db8319e1-02fa-4159-aafd-5a3625f7fcb6", "embedding": null, "metadata": {"issue_id": 807, "title": "[BUG] HTTP 401 on https://huggingface.co/HuggingFaceM4/idefics2-8b-chatty/resolve/main/chat_template.json", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e86c1d41-61a1-484d-abfe-6c78b727d49a", "node_type": "4", "metadata": {"issue_id": 807, "title": "[BUG] HTTP 401 on https://huggingface.co/HuggingFaceM4/idefics2-8b-chatty/resolve/main/chat_template.json", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "64a2ef9a8db441bf59dbb661082ee3ee2812d7385d7463602f7922837ac814a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] HTTP 401 on https://huggingface.co/HuggingFaceM4/idefics2-8b-chatty/resolve/main/chat_template.json\n\nDescription: **Describe the bug** Trying to run this example: https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Packages version:** Clone at 26th Feb - Git hash: [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d6b84c93-6199-4bee-bf33-18250d779557", "embedding": null, "metadata": {"issue_id": 805, "title": "userinputtool() Prompts Not Displaying in Gradio Interface", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b732937-d096-4c90-8870-ace0b5b5645a", "node_type": "4", "metadata": {"issue_id": 805, "title": "userinputtool() Prompts Not Displaying in Gradio Interface", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "689e27e6616d2ef49406bade843afcca3e89b364a44cb32059358173431e438a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: userinputtool() Prompts Not Displaying in Gradio Interface\n\nDescription: **Describe the bug** I have developed a solution using SmolAgents that incorporates an agent with multiple tools (Python functions). Specifically, I utilize UserInputTool() to request missing parameters when executing a function. This approach ensures that the agent doesn't fabricate or assume parameters. However, I've encountered an issue where the prompts from UserInputTool() are displayed in the terminal instead of the Gradio interface, this is beacause this tool uses input(). As a result, users interacting with the agent through the Gradio UI cannot see or respond to these prompts, which disrupts the intended user experience. Source code (https://github.com/huggingface/smolagents/blob/v1.9.2/src/smolagents/default_tools.py): [CODE_BLOCK] **Code to reproduce the error** The code simplified would be something like this: [CODE_BLOCK] **Expected behavior** When a function requiring additional parameters is invoked, the UserInputTool() prompts should appear within the Gradio interface. **Packages version:** smolagents==1.9.2\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1335212d-fb44-4757-baa9-ceaa3671ea3a", "embedding": null, "metadata": {"issue_id": 804, "title": "Opentelemetry instrumentation splits transactions into steps when streaming=True in codeAgent", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4aefa7a-fe07-444d-aee7-675809b0f4d0", "node_type": "4", "metadata": {"issue_id": 804, "title": "Opentelemetry instrumentation splits transactions into steps when streaming=True in codeAgent", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "2b3f1a133435b96c9c0a1712bc20cac28c1ddb0869d197c50c104516031a1b9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Opentelemetry instrumentation splits transactions into steps when streaming=True in codeAgent\n\nDescription: My app writes instrumentations via Opentelemtry-sdk==1.29.0, opentelemtry-exporter-otlp==1.29.0 and openinference-instrumentation-smolagents==0.1.4 My smolagents version is 1.9.2 my_agent = CodeAgent(model=my_model, tools=my_tools_list) my_agent.run(\"say hello\", stream=True) causes each span to be its own step instead of all step being gathered under the same trace Thank you for your help\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8e7bb51c-0e73-43aa-9fff-c64cd8e8d51b", "embedding": null, "metadata": {"issue_id": 801, "title": "[BUG] You passed these duplicate names: ['inspect_file_as_text', 'inspect_file_as_text'] when running Open Deep Research example", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7821e39b-fcf2-47f2-abc9-4e7411326568", "node_type": "4", "metadata": {"issue_id": 801, "title": "[BUG] You passed these duplicate names: ['inspect_file_as_text', 'inspect_file_as_text'] when running Open Deep Research example", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "hash": "6f2b6dba1672405bcd30d7c0109f0336bba152b22fea7d09feacbb87cc4f7237", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] You passed these duplicate names: ['inspect_file_as_text', 'inspect_file_as_text'] when running Open Deep Research example\n\nDescription: **Describe the bug** Trying to run open deep research example - however, when I do, I get the error message ValueError: Each tool or managed_agent should have a unique name! You passed these duplicate names: ['inspect_file_as_text', 'inspect_file_as_text'] **Code to reproduce the error** - Follow instructions here: https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research **Error logs (if any)** [CODE_BLOCK] **Expected behavior** The agents to come up with a nice answer :) **Packages version:** Fresh clone and pip install today (feb 26th)\n\nState: closed\n\nLabels: bug duplicate\n\nCategories: category-bug category-duplicate\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2e69ce7c-0f3b-4e10-a4a5-36a3b96aa5a8", "embedding": null, "metadata": {"issue_id": 800, "title": "step_callbacks custom parameters are required", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c336cfe6-290b-44fb-8be2-008b0478a403", "node_type": "4", "metadata": {"issue_id": 800, "title": "step_callbacks custom parameters are required", "state": "closed", "labels": [], "type": "issue"}, "hash": "408c2d14aaba90c44d8478b0be26a2e21352ab64d92d1e3188f3c2cc6b52d752", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: step_callbacks custom parameters are required\n\nDescription: \n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 82, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2407ccc-359b-4fcc-9db4-34f785c15b8a", "embedding": null, "metadata": {"issue_id": 799, "title": "[Question] Inquiry About text_webbrowser_agent Implementation in open_deep_search", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "147e4b2c-3759-4fc6-bf11-ee0aacae168a", "node_type": "4", "metadata": {"issue_id": 799, "title": "[Question] Inquiry About text_webbrowser_agent Implementation in open_deep_search", "state": "closed", "labels": [], "type": "issue"}, "hash": "764af90f9d5ef919f6888a210488cea6e99e7dbdfed0af70c0b8b2db079c6570", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Question] Inquiry About text_webbrowser_agent Implementation in open_deep_search\n\nDescription: I recently came across the implementation of the text_webbrowser_agent within the open_deep_search project. https://github.com/huggingface/smolagents/blob/9498094f86dd1839c6f75aefde637cc2f265bb31/examples/open_deep_research/run.pyL111-L128 I noticed that the text_webbrowser_agent is implemented using **ToolCallingAgent**. Could you please clarify the rationale behind using **ToolCallingAgent** for the text_webbrowser_agent instead of **CodeAgent**? I recall that your blog highlighted several advantages of CodeAgent in blog.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d4efedb-1310-4214-9572-51942c577afe", "embedding": null, "metadata": {"issue_id": 795, "title": "[BUG] ValueError: Each tool or managed_agent should have a unique name! You passed these duplicate names ['inspect_file_as_text', 'inspect_file_as_text']", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8b82be4-7117-4e52-b60f-9c99c734a720", "node_type": "4", "metadata": {"issue_id": 795, "title": "[BUG] ValueError: Each tool or managed_agent should have a unique name! You passed these duplicate names ['inspect_file_as_text', 'inspect_file_as_text']", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "453246081dbdadae8b290a7c53bde048ac044c3613a129e36eb8a0f150c9a4fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] ValueError: Each tool or managed_agent should have a unique name! You passed these duplicate names ['inspect_file_as_text', 'inspect_file_as_text']\n\nDescription: **Describe the bug** ValueError: Each tool or managed_agent should have a unique name! You passed these duplicate names: ['inspect_file_as_text', 'inspect_file_as_text'] **Code to reproduce the error** Installed and ran Open Deep Search using this page: https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research EXCEPT, I used this instead for installing smolagents from the main branch as instructed in this Issue: : pip install -e ../../.[dev] My run command is: python run.py --model-id \"o1\" \"$(cat /path/file.txt)\" where my path is to a file containing the prompt text **Error logs** [CODE_BLOCK] **Expected behavior** _I expected Open DeepResearch to run and produce results in the form of OpenAI Deep Research. I certainly expected it to be able to do google searches as part of that process._ **Packages version:** [CODE_BLOCK] **Additional context** N/A\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1139, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "14b96155-5122-4632-b02b-93ff2c69ff5b", "embedding": null, "metadata": {"issue_id": 791, "title": "[BUG] Rich markup causes crash during error logging", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3a3dfad-b5b3-430b-8773-064e4c26ec00", "node_type": "4", "metadata": {"issue_id": 791, "title": "[BUG] Rich markup causes crash during error logging", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "d24501115eb2d7546973ef01fedc9c961cc953bf4e6af62d37baba8b30f13742", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Rich markup causes crash during error logging\n\nDescription: **Describe the bug** Rich markup logging interferes with agent output errors when logging to the console. [CODE_BLOCK] This is problematic because it causes continuous crashes and terminates execution. The issue occurs when message contains Rich markup syntax, such as closing tags. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** It should log error as usual without crashing. **Packages version:** smolagents==1.9.2 **Additional context** Debugging, this is an example of the content of {message} that is generating the issue. [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af4e9c66-b4f4-49fd-8a99-31e1492b7ba2", "embedding": null, "metadata": {"issue_id": 789, "title": "[BUG] GradioUI cannot be reloaded", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db206d7c-133c-41b2-8c03-f67d9632ee35", "node_type": "4", "metadata": {"issue_id": 789, "title": "[BUG] GradioUI cannot be reloaded", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "fe875e3ceda2ec5f6009575375713337866285f52a2fa6b40439e1c5d2ae9411", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] GradioUI cannot be reloaded\n\nDescription: **Describe the bug** GradioUI is not a real gradio interface so it cannot be hot reloaded when executing gradio agent_chat.py **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** the reload functionality should work, same as with normal gradio interfaces. **Packages version:** smolagents==1.9.2 **Additional context** n/a\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 509, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3dce52c0-71cd-4888-a9f1-903560e260bc", "embedding": null, "metadata": {"issue_id": 787, "title": "[BUG] smolagent CLI uses OPENAI_API_KEY for LiteLLMModel when no API key is provided", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ebb1a5c1-4532-43a1-b625-41e4a0f56651", "node_type": "4", "metadata": {"issue_id": 787, "title": "[BUG] smolagent CLI uses OPENAI_API_KEY for LiteLLMModel when no API key is provided", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "0165c365ead19d5784c810c7a3ab4006e1a84a8888d18c832f33468800d3bcfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] smolagent CLI uses OPENAI_API_KEY for LiteLLMModel when no API key is provided\n\nDescription: **Describe the bug** When using the smolagent CLI with --model-type \"LiteLLMModel\" and without specifying an --api-key, it passes the OPENAI_API_KEY to LiteLLM, even when running models from other providers. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** When using smolagent with --model-type \"LiteLLMModel\" and no explicit --api-key, it should allow LiteLLM to select the appropriate API key from environment variables based on the model id. **Packages version:** [CODE_BLOCK] **Additional context** When passing None as the api_key parameter to LiteLLM, it automatically determines which API key to use from environment variables based on the model (e.g., ANTHROPIC_API_KEY for Anthropic models).\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "880e41b0-628a-4a3f-a5c9-59a8fe81bc82", "embedding": null, "metadata": {"issue_id": 784, "title": "How do I add a system prompt?", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bb2b2ab-ce03-452d-b90f-d577413d2e84", "node_type": "4", "metadata": {"issue_id": 784, "title": "How do I add a system prompt?", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "hash": "5f125344e3912ea067bc68b820354952a4b1358b9ea581c3dac95789ac227000", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How do I add a system prompt?\n\nDescription: I remember earlier probably around december I used to be able to add system prompt using CODE_SYSTEM_PROMPT but now when I wanted to run my older code I'm getting a error that says: cannot import name 'CODE_SYSTEM_PROMPT' from 'smolagents' (/usr/local/lib/python3.11/dist-packages/smolagents/__init__.py) So it's clearly no longer available so how should I add system prompts?\n\nState: closed\n\nLabels: duplicate\n\nCategories: category-duplicate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 493, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d67eb5d8-22ab-40d6-8507-e398c66f5856", "embedding": null, "metadata": {"issue_id": 782, "title": "Add Kubernetes as an alternative container for sandbox for secure running agents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "778993da-ced9-4361-924d-e767a35380e6", "node_type": "4", "metadata": {"issue_id": 782, "title": "Add Kubernetes as an alternative container for sandbox for secure running agents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "738cfd589dca3037f725c464daa7fcca7d3c777bf15062c8ff1e59d049d1ac65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add Kubernetes as an alternative container for sandbox for secure running agents\n\nDescription: It would be great to be able to use a kubernetes cluster to run smolagents in sandbox. The enahancement here is about creating an example and updating documentation. For the example purpose a loca cluster running on minikube could be fine\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 446, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "be970075-b0da-4126-aba4-314c3c1a2810", "embedding": null, "metadata": {"issue_id": 781, "title": "Results on the GAIA compared to browser-use", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abfd0d3c-00b3-40c3-b564-61a1e0088540", "node_type": "4", "metadata": {"issue_id": 781, "title": "Results on the GAIA compared to browser-use", "state": "closed", "labels": [], "type": "issue"}, "hash": "9ade4b798659557811d46329bcf82e908e3e05a9221968ec6f9fbb0884116135", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Results on the GAIA compared to browser-use\n\nDescription: Has Anyone Successfully Run browser-use on GAIA? I would like to know if anyone has successfully run the full browser-use experiment on the GAIA dataset. This is very important to me. https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/visual_vs_text_browser.ipynb\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c2251777-5c43-4f27-8378-1a56dd7a54ad", "embedding": null, "metadata": {"issue_id": 777, "title": "[BUG] TypeError: GoogleSearchTool.__init() got an unexpected keyword argument 'provider'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76e6b280-98ba-421c-9654-d45c174702c2", "node_type": "4", "metadata": {"issue_id": 777, "title": "[BUG] TypeError: GoogleSearchTool.__init() got an unexpected keyword argument 'provider'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "54c721cf66ae1c743c4ca3ee980a8a36b46c224e31ce2388eb7d229968f9c2a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] TypeError: GoogleSearchTool.__init() got an unexpected keyword argument 'provider'\n\nDescription: **Describe the bug** TypeError thrown as Open DeepResearch attempts to do a Google Search. **Code to reproduce the error** Installed and ran Open Deep Search using this page: https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research My run command is: python run.py --model-id \"o1\" \"$(cat /path/file.txt)\" where my path is to a file containing the prompt text (it's a fairly long prompt). **Error logs** [CODE_BLOCK] **Expected behavior** _I expected Open DeepResearch to run and produce results in the form of OpenAI Deep Research. I certainly expected it to be able to do google searches as part of that process._ **Packages version:** Running pip freeze | grep smolagents gives me the following: olagents openinference-instrumentation-smolagents==0.1.6 smolagents==1.9.2 **Additional context** N/A\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1013, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39d2f44d-c112-46cf-8b76-174c18319b4e", "embedding": null, "metadata": {"issue_id": 775, "title": "Add Podman as an alternative container for sandbox for secure running agents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ad7683-b97b-4230-85de-ff209c4cd04b", "node_type": "4", "metadata": {"issue_id": 775, "title": "Add Podman as an alternative container for sandbox for secure running agents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "4d399481e6e7289e33ee1d21ea9958b325635b289a7a5a8a98c3b34d3832f6a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add Podman as an alternative container for sandbox for secure running agents\n\nDescription: Podman is an alternative to Docker, which presents a few advantages in terms of security because it has been designed to be daemon-less, and it's able to run containers root-less.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e246852b-5e74-4b92-a4e6-91c9d5ea234f", "embedding": null, "metadata": {"issue_id": 774, "title": "[BUG] SmolAgent ERROR on to \"import sklearn\"", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7572713-53fb-42ce-88c2-e9fe8a557ecc", "node_type": "4", "metadata": {"issue_id": 774, "title": "[BUG] SmolAgent ERROR on to \"import sklearn\"", "state": "closed", "labels": ["bug", "duplicate"], "type": "issue"}, "hash": "b72be97a519f4fd5eeae2121283cbe76201f2e647e9f02f969b7e4ef968d2138", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] SmolAgent ERROR on to \"import sklearn\"\n\nDescription: **Describe the bug** Smolagents throws error on \"import of sklearn\" Final answer: Failed to import sklearn due to a SciPy error ('module 'scipy.sparse._coo' has no attribute 'upcast'). Direct package management is not available in this environment. **Code to reproduce the error** huggingface's own example: https://huggingface.co/learn/cookbook/en/agent_data_analyst agent = CodeAgent( tools=[], model=model, additional_authorized_imports=[ \"numpy\", \"pandas\", \"matplotlib.pyplot\", \"seaborn\", \"sklearn\", ], max_iterations=12, ) output = agent.run( \"\"\"You are an expert machine learning engineer. Please train a ML model on \"titanic/train.csv\" to predict the survival for rows of \"titanic/test.csv\". Output the results under './output.csv'. Take care to import functions and modules before using them! \"\"\", additional_args=dict(additional_notes=additional_notes + \"\\n\" + analysis), ) **Error logs (if any)** Final answer: Failed to import sklearn due to a SciPy error ('module 'scipy.sparse._coo' has no attribute 'upcast'). **Expected behavior** I verfied in multiple location colab, aws, upgrad of scipy. It works in normal python kernel and notebook but just basic \"import sklearn\" . fails in smolagent **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug duplicate\n\nCategories: category-bug category-duplicate\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1523, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ea85258-3a7c-49f1-b0aa-f63e7d70e241", "embedding": null, "metadata": {"issue_id": 770, "title": "Secure code execution wrong title in docs TOC", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50403402-7317-4fa2-abc5-d03343ca6293", "node_type": "4", "metadata": {"issue_id": 770, "title": "Secure code execution wrong title in docs TOC", "state": "closed", "labels": [], "type": "issue"}, "hash": "89103806a7d0d9828d42d2eb39ff8ec27f6c43072ffd5b819fc2bddf0f118039", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Secure code execution wrong title in docs TOC\n\nDescription: Minor issue: the title of the page is \"Secure code execution\", but TOC still have \"Secure your code execution with E2B\"\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17ec1393-6ad5-4c39-aaa2-a47826e1ed14", "embedding": null, "metadata": {"issue_id": 767, "title": "Agents with vLLM framework", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d7df947d-bad1-467d-9e4c-12ba37447c0e", "node_type": "4", "metadata": {"issue_id": 767, "title": "Agents with vLLM framework", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "9da4d76420e159b5cc4a3bd5aa278d84de60466056706fc466bf39a53bbd9173", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Agents with vLLM framework\n\nDescription: Hello, I saw that the MLXModel model has been added and I would like to propose to add vLLM support. I have a use case where I need to try vLLM framework locally with an agent and, perhaps, other people would like to use it as well. You can find below a model class that uses vLLM inference and it can be used with CodeAgent directly like any other model. If this is interesting, I can create an MR and we can iterate on the code. **Describe the solution you'd like** [CODE_BLOCK]\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 636, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb36d6b2-22ac-4cfe-b605-f0fa251fd6f6", "embedding": null, "metadata": {"issue_id": 760, "title": "Usage instructions for Open Deep Research are incomplete", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3a928b6-d8ff-4c10-89fd-dd1d3832ad33", "node_type": "4", "metadata": {"issue_id": 760, "title": "Usage instructions for Open Deep Research are incomplete", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "53e2a5f71d9ad355a12237b6c27b80584fcd7ea3cba82c1001108afed62ef018", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Usage instructions for Open Deep Research are incomplete\n\nDescription: So I was excited to find out about Open Deep Research and decided to give it a go myself. I am of course talking about the Open Deep Research example here: https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research According to the README.md all you have to do is install the requirements and the smolagents[dev] and bam, you're good to go. The author of this page clearly hasn't followed that description. Because obviously it doesn't just work. First of all you need a HF token and the run.py script will prompt for it every single time you run it. If there is a way to pass it in, it's not explained anywhere. Secondly you need an OpenAI key which must be passed in as an environment variable. This is not explained anywhere. Thirdly, I couldn't get the \"o1\" model to work with OpenAI. It just kept returning an error that the o1 model was not available. Is it even available for API use for non-pro subscribers? Fourth, you can't use the Open Deep Research example with a non-reasoning model like gpt-4o because it passes in a reasoning_effort parameter that those models don't support. It just doesn't work. I had to edit the run.py code and comment out the line. Fifth, and once you resolve all these issues, you finally think you can ask the agent a question. Well, joke's on you when you look at the \"Missing SerpAPI key\" that is mostly all you see in the output. This is where I gave up. I really want to see this project succeed and as a software developer I did my best to make things work. But my roots are in java and have only basic understanding of Python and it's runtime environment. So while maybe most of my issues above may seem pretty obvious to developers in the Python community, they sure as heck aren't to a guy like me. Can the README.md file please get some hints for newbies like me?\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "940be1f6-feae-4227-9dd3-4827b02ab303", "embedding": null, "metadata": {"issue_id": 759, "title": "[BUG] MultiStepAgent silently overwrites parameter-provided FinalAnswerTool", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "365df268-9020-497c-ba95-0e2cfe6cd203", "node_type": "4", "metadata": {"issue_id": 759, "title": "[BUG] MultiStepAgent silently overwrites parameter-provided FinalAnswerTool", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f48a25cf61bc685f024c7664c78041f94085042aaddb5d12dab6989713605306", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] MultiStepAgent silently overwrites parameter-provided FinalAnswerTool\n\nDescription: **Describe the bug** MultiStepAgent silently overwrites the parameter-provided FinalAnswerTool. The agent receives a list of tools trough its constructor, which may include a 'FinalAnswerTool' that terminates the Thought-Action-Observation cycle of the agent. However agent.py, in line 255 silently overrides a user-provided FinalAnswerTool by instantiating the default implementation. [CODE_BLOCK] This behavior violates the Principle of Least Surprise and can lead to hard to track bugs. It also creates a hard dependency on the library-povided FinalAnswerTool class and makes it impossible to provide a modified FinalAnswerTool by a user of the API. Release v. 1.9.2 **Additional context** Originally reported by user Julien in the huggingface Discord.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 933, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5268053-9a99-481a-8642-256519fd9f0b", "embedding": null, "metadata": {"issue_id": 758, "title": "smolagents.GradioUI rendering matplotlib or plotly charts", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f813bf4-ecd6-4b4e-a2a6-62f4e882bde3", "node_type": "4", "metadata": {"issue_id": 758, "title": "smolagents.GradioUI rendering matplotlib or plotly charts", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "a4f3448d0f34a402050130d32815a59b48c610ac99fb0e6a7f69e57133eb436f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: smolagents.GradioUI rendering matplotlib or plotly charts\n\nDescription: I am trying to build data analyser agent and couldn't make it render the resulting chart. Could you improve the GradioUI class to include gr.LinePlot(), gr.ScatterPlot and other types of plots, that captures matplotlib or plotly plots?\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "26f26919-fd2b-4eb6-8383-192bec00fe56", "embedding": null, "metadata": {"issue_id": 757, "title": "[BUG] Problem with the user and assistant roles when using the Mistarl api.", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02e5e935-1a6c-49ae-8356-5aa519359e14", "node_type": "4", "metadata": {"issue_id": 757, "title": "[BUG] Problem with the user and assistant roles when using the Mistarl api.", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "0ce6b3f0dee391ad0abeea7210c4cf7099fe295954f25a28531d921f0cfc1e4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Problem with the user and assistant roles when using the Mistarl api.\n\nDescription: **Describe the bug** When using the Mistarl API via LiteLLM or OpenAIServerModel, an error occurs due to the fact that the Mistral API expects a message with the User or Tool role, and receives it with the Assistant role. **Code to reproduce the error** I used this code: model_mistral = LiteLLMModel( model_id=\"mistral/mistral-large-latest\", api_key=mistral_api_key, timeout=29, ) and such model_mistral_openAI = OpenAIServerModel( model_id=\"mistral-large-latest\", api_base=\"https://api.mistral.ai/v1\", api_key=mistral_api_key, ) **Error logs (if any)** Error: Error in generating model output: litellm.BadRequestError: MistralException - Error code: 400 - {'object': 'error', 'message': 'Expected last role User or Tool (or Assistant with prefix True) for serving but got assistant', 'type': 'invalid_request_error', 'param': None, 'code': None} **Expected behavior** Perhaps you should immediately pass the Assistant role with prefix True? **Packages version:** smolagents==1.9.2 **Additional context**\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bb282a76-a145-4782-a5a1-2ab2115307f4", "embedding": null, "metadata": {"issue_id": 755, "title": "Add LATS and Reflexion", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3aed9041-3900-4ef2-a8a5-a03aa9bf2d91", "node_type": "4", "metadata": {"issue_id": 755, "title": "Add LATS and Reflexion", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "67ac2ac31635097d5a640b39b526bc1be392d761e33cd911e054fa82be18c33a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add LATS and Reflexion\n\nDescription: **Is your feature request related to a problem? Please describe.** I use the CodeAgent in my work and am amazed by its performance. It is mainly based on ReAct. However, I feel it is more than that, but still, there are some limitations. One of the things that I am missing is the revision step. The other thing is the search (like MCTS) capability and the ability to generate multiple chains of thoughts. I want to start building some of the more advanced agents like LATS and Reflexion. **Describe the solution you'd like** I want to add some agents with the revision and MCTS capability like LATS and Reflexion. **Is this not possible with the current options.** I think with some improvements, it is possible to have these new agents. I am thinking of some new classes for these agents inherited from CodeAgent with more functionality or changing the logic and workflow. **Describe alternatives you've considered** I have found llamaindex and langgraph implementations of these new agents: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/lats/lats.ipynb https://docs.llamaindex.ai/en/stable/examples/agent/lats_agent/ https://langchain-ai.github.io/langgraph/tutorials/reflexion/reflexion/\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58d41bc8-8612-403f-a92a-0637ff4ee5db", "embedding": null, "metadata": {"issue_id": 754, "title": "[BUG] Issue running agents with Llama 3 series models.", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23cf3eec-e995-4bcb-8d64-ace478ba2d0b", "node_type": "4", "metadata": {"issue_id": 754, "title": "[BUG] Issue running agents with Llama 3 series models.", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "d4be70d8a374b00145045aec4d5b92783da7c9f2b7ab273aa8896ff4fa6b2fc1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Issue running agents with Llama 3 series models.\n\nDescription: **Describe the bug** When running Llama 3.3 and 3.1 70b from both fireworks and together, through litellm proxy, directly, and even through HF as an inference provider, when attempting to run an agent, I continually get this error: [CODE_BLOCK] **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** above. **Expected behavior** When using the exact setup, primarily litellm proxy as an openai compatible server, and using the openaiserver in smolagents, I simply change the model to any model from openai, anthropic, or grok, and it works perfectly fine. If I change it to any model from Fireworks or Together, or even Ollama running it myself, with the Deepseek and Llama models, I get this error. No clue why. I tested it with multiple providers with multiple methods, and confirmed they all worked with cURLs. **Packages version:** smolagents==1.9.2 **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1091, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac60a869-4348-47b7-89ab-262ce7e2e0b1", "embedding": null, "metadata": {"issue_id": 752, "title": "Customize `final_answer` output", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f629fd73-290b-4d8f-beea-698f4c0b68dc", "node_type": "4", "metadata": {"issue_id": 752, "title": "Customize `final_answer` output", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "5109951404071bbeab6c3a0428f463fb0a3516ebce71c0b2ea38fef2a7a55801", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Customize `final_answer` output\n\nDescription: **Is your feature request related to a problem? Please describe.** When using smolagents for multi-step tasks, the final output lacks detail and does not follow a specific format. This happens because final_answer outputs a raw string without customization options. **Describe the solution you'd like** I propose making final_answer a customizable step that allows specifying a format or output instructions (possibly using a different model) while still leveraging the agent\u2019s memory. **Is this not possible with the current options?** I attempted to include output formatting instructions in the system prompt, but it didn\u2019t work well due to the presence of other instructions that the agent also needs to follow. **Describe alternatives you've considered** The other alternative is to make final_answer a dedicated tool for output. However, it would require the model to pass all context and reconstruct the task, which can be overcomplicated.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a7b659a-6cf4-437e-a0b5-9f2c6d154425", "embedding": null, "metadata": {"issue_id": 751, "title": "[BUG] Rich removes all [] in errors", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12510df2-4912-4c06-a78a-d83c02c73c40", "node_type": "4", "metadata": {"issue_id": 751, "title": "[BUG] Rich removes all [] in errors", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "4c9f5f7c64752e0d11ffd4348fc5cde69c2437388b1b99bb90ad91fc06aade7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Rich removes all [] in errors\n\nDescription: **Describe the bug** When code execution has an error and displays the offending code in red, any non-quoted string in a [] in the code line is not shown. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** The log should be: [CODE_BLOCK] **Packages version:** smolagents==1.9.2 I think this is due to Rich recognizing \"[any_string]\" pattern as tags\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 539, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dae90bff-03fb-45ff-b5d0-c9194444fa66", "embedding": null, "metadata": {"issue_id": 748, "title": "Do advanced tools support multiple operations?", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b24ae3a-0b8f-4252-abbe-4ff98cb9a0fb", "node_type": "4", "metadata": {"issue_id": 748, "title": "Do advanced tools support multiple operations?", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "1de7dea5463fc18699045dec3de491679320084d9bf5266036d446c0aa73052a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Do advanced tools support multiple operations?\n\nDescription: For example: [CODE_BLOCK] it's old: [CODE_BLOCK] what should i do\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94c69119-8bc1-40ea-b2a9-a9202aec0c22", "embedding": null, "metadata": {"issue_id": 746, "title": "[BUG] ModuleNotFoundError: No module named 'audioop'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1af54fc3-7e64-445d-b15e-1864d32a445d", "node_type": "4", "metadata": {"issue_id": 746, "title": "[BUG] ModuleNotFoundError: No module named 'audioop'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "2bde67a1d7ddc5123211648526b4058fae282f0358cd8c56f9213e7bf459e302", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] ModuleNotFoundError: No module named 'audioop'\n\nDescription: Describe the bug As reported by @KoLLchIK, the GradioUI raises: > ModuleNotFoundError: No module named 'audioop' [CODE_BLOCK] Code to reproduce the error [CODE_BLOCK] Sub-issue of: - 715\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "96bc15bc-2735-464b-85f0-9ee56cbecfd1", "embedding": null, "metadata": {"issue_id": 742, "title": "Could open deep research be executed in a streaming", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "742e8ffd-489e-40e2-8381-c97b624d71be", "node_type": "4", "metadata": {"issue_id": 742, "title": "Could open deep research be executed in a streaming", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "7e8323908f7eb9defe657944a57af8a0e7d688b40c4d9eca878bc936a2bc1f98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Could open deep research be executed in a streaming\n\nDescription: Could open deep research be executed in a streaming where I can check the intermediate steps? My PC environment isn\u2019t very good, so each step takes several minutes to complete, and it would be a bit less frustrating if I could see tokens one by one along the way.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b96be9f9-1e70-4263-b1f8-d97466434601", "embedding": null, "metadata": {"issue_id": 738, "title": "CI tests sometimes raise DuckDuckGoSearchException: 202 Ratelimit", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dba39c9b-d8b3-44f8-98f2-bfbc52c96cb9", "node_type": "4", "metadata": {"issue_id": 738, "title": "CI tests sometimes raise DuckDuckGoSearchException: 202 Ratelimit", "state": "closed", "labels": [], "type": "issue"}, "hash": "53b52727f2faac83312aba405f51859a4d4bee5f67f3aefec6817420fd395a04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: CI tests sometimes raise DuckDuckGoSearchException: 202 Ratelimit\n\nDescription: CI tests sometimes raise DuckDuckGoSearchException: 202 Ratelimit See: https://github.com/huggingface/smolagents/actions/runs/13438187557/job/37545615433?pr=731 > FAILED tests/test_search.py::DuckDuckGoSearchToolTester::test_exact_match_arg - duckduckgo_search.exceptions.DuckDuckGoSearchException: https://lite.duckduckgo.com/lite/ 202 Ratelimit [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 461, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "81aea191-0ee2-4ccb-a7da-4b2ba271591a", "embedding": null, "metadata": {"issue_id": 736, "title": "[BUG] Completions.create() got an unexpected keyword argument 'max_retries' ", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd65e8ef-e696-452d-9182-6b3854d4db17", "node_type": "4", "metadata": {"issue_id": 736, "title": "[BUG] Completions.create() got an unexpected keyword argument 'max_retries' ", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "b366dd9aaabbc6a3b1fc9473e11fdb4da4ce0296ceaead2297b9d52d05086d93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Completions.create() got an unexpected keyword argument 'max_retries' \n\nDescription: **Describe the bug** While Using the OpenAIServerModel class i am getting Completions.create() got an unexpected keyword argument 'max_retries' **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** max_retries is an argument of OpenAI class so it shouldn't produce any error **Packages version:** 1.10.0.dev0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "621c2815-b43f-4be3-815b-acba8868c889", "embedding": null, "metadata": {"issue_id": 723, "title": "Add --base-url to CLI tools (smolagent / webagent)", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "039bae05-5444-424f-8306-e1057e328a01", "node_type": "4", "metadata": {"issue_id": 723, "title": "Add --base-url to CLI tools (smolagent / webagent)", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "4ec9e70d44c5325e8dc37c01619207f0f79e70c943888e4461ea67353d147c34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add --base-url to CLI tools (smolagent / webagent)\n\nDescription: **Is your feature request related to a problem? Please describe.** Allow users to specify the base URL for their OpenAIServerModel in the CLI invocation. For example, if they are hosting their own OpenAI API compatible server, they could use that instead of the current hard-coded Fireworks.ai . **Describe the solution you'd like** Add --base-url to the CLI tools smolagent and webagent, so that the user-provided value gets used for the api_base parameter here. **Is this not possible with the current options.** The base URL is currently hard-coded for CLI usage. **Describe alternatives you've considered** Changing this myself. **Additional context** Add any other context or screenshots about the feature request here.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 904, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6f1dde2-04c2-46ef-8b84-4fef1b8a5314", "embedding": null, "metadata": {"issue_id": 718, "title": "[BUG]ImportError: cannot import name 'CODE_SYSTEM_PROMPT' from 'smolagents.prompts' (unknown location)", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cfa604a-3651-43d0-b416-e7c134e0e04b", "node_type": "4", "metadata": {"issue_id": 718, "title": "[BUG]ImportError: cannot import name 'CODE_SYSTEM_PROMPT' from 'smolagents.prompts' (unknown location)", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "43c3fc1a9894762d7bc7bac5b34c270f045beea99a6348cb9e6248733d29329d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]ImportError: cannot import name 'CODE_SYSTEM_PROMPT' from 'smolagents.prompts' (unknown location)\n\nDescription: **Describe the bug** Attempting to import the default system prompt using: [CODE_BLOCK] results in an ImportError. The error message indicates that the name CODE_SYSTEM_PROMPT cannot be imported from the smolagents.prompts module. This suggests that the constant might be missing, renamed, or misconfigured in the repository. As a result, developers are unable to modify the built-in system prompt when initializing agents, which disrupts the expected, smooth setup for creating and customizing AI agents. **Code to reproduce the error** [CODE_BLOCK] **Error logs** from smolagents.prompts import CODE_SYSTEM_PROMPT Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ImportError: cannot import name 'CODE_SYSTEM_PROMPT' from 'smolagents.prompts' (unknown location) **Expected behavior** The expected behavior is that importing CODE_SYSTEM_PROMPT from the smolagents.prompts module should succeed without any errors. The constant should contain the default system prompt template used for initializing agents, allowing developers to either use it as-is or modify it to suit their custom needs. This ensures that the process of setting up and running agents remains consistent with the documentation and intended design of the library. **Packages version:** smolagents==1.9.2\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1498, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a222e35b-de1b-42b0-94dc-8b9b0e3a3b5d", "embedding": null, "metadata": {"issue_id": 717, "title": "DatasetNotFoundError in benchmark", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "636a5ebe-f940-4c63-adb7-fcb364b165b9", "node_type": "4", "metadata": {"issue_id": 717, "title": "DatasetNotFoundError in benchmark", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "eedf1cb289a6281bf07bc844fb1357c536cacc9fb1476fd7719e6b86d18b4583", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: DatasetNotFoundError in benchmark\n\nDescription: I'm getting a DatasetNotFoundError for 'smolagents-benchmark/benchmark-v1' while running benchmark.ipynb, even though I've already entered my access token.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 291, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ec25225-aa47-4b68-aa9b-cb2398ff8788", "embedding": null, "metadata": {"issue_id": 716, "title": "[BUG] from openinference.instrumentation.smolagents import SmolagentsInstrumentor", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5c4ff1a-de00-4618-b4b9-48989e2194e7", "node_type": "4", "metadata": {"issue_id": 716, "title": "[BUG] from openinference.instrumentation.smolagents import SmolagentsInstrumentor", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "917bc134aacecfdc8b1f1c95a399cc6240a7d3ca4daed9879a0391380b914050", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] from openinference.instrumentation.smolagents import SmolagentsInstrumentor\n\nDescription: ModuleNotFoundError: No module named 'openinference.instrumentation.smolagents' can you check this\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 282, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d19d9c54-f348-4404-9472-70267991ac01", "embedding": null, "metadata": {"issue_id": 715, "title": "[BUG] UnicodeEncodeError: 'charmap' codec can't encode characters", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "260b3e21-2b53-4a04-85b2-17e6a47d43f1", "node_type": "4", "metadata": {"issue_id": 715, "title": "[BUG] UnicodeEncodeError: 'charmap' codec can't encode characters", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "add3890b09aa04d138a47703c1bc0753a3ed04f2bde7f1b792ac9b2b2d54542e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] UnicodeEncodeError: 'charmap' codec can't encode characters\n\nDescription: Doesn't work GradioUI and also don't work with requests on Russian [CODE_BLOCK] For GradioUI: [CODE_BLOCK] For request on Russian: [CODE_BLOCK] smolagents==1.9.2\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 329, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b02304b0-acdd-4d29-bd3d-ad7ab8a0599d", "embedding": null, "metadata": {"issue_id": 712, "title": "The execution of the action sequence fails when an agent and a tool have the same name", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c672172d-0255-40c4-a924-31a7ecef025b", "node_type": "4", "metadata": {"issue_id": 712, "title": "The execution of the action sequence fails when an agent and a tool have the same name", "state": "closed", "labels": [], "type": "issue"}, "hash": "eb3d2f248502e836cc4c1901d16addb8ae61a4fa239470e9e81be17d1446a76f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: The execution of the action sequence fails when an agent and a tool have the same name\n\nDescription: Problem Description + When the name of the agent handling subtasks is the same as that of a tool, according to the current prompt, the LLM may be confused when choosing whether to use the tool or the agent in the returned action, leading to code execution failure. Reproduction Code + Here is a code snippet of DuckDuckGoSearchTool: [CODE_BLOCK] + As you can see, DuckDuckGoSearchTool uses \"web_search\" as its name. + Now, let's define and execute the web_agent. Note that the name of web_agent is also \"web_search\". [CODE_BLOCK] + Let's start executing the code. When the first step is executed, the code sequence returned by the LLM is as follows: [CODE_BLOCK] + Obviously, during the thinking process, the LLM preferentially selects the \"web_search\" from the tool to generate the code sequence. However, when the Python interpreter executes this code, it actually hits the __call__ method of the manager_agent. Therefore, it notice that the task parameter is required instead of the query parameter, which causes confusion. + I have currently thought of two solutions, but I'm not sure if they are the optimal ones. I hope to discuss them with the developers. + The first solution is to check for duplicate names between managed_agents and tools when initializing the agent and throw an error for duplicate names. Here is the pseudocode: [CODE_BLOCK] + The second solution is to modify the prompt by adding something like If there are an agent and a tool with the same name, you should give priority to invoking the agent. The following is an example of this situation. and then supplement the above in - situ code as a case. + Looking forward to your replies.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "737cdc14-6452-458d-82ff-083cc475a2bf", "embedding": null, "metadata": {"issue_id": 709, "title": "How can I  use smolagents in python3.8", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c2ec7aa-a1eb-4828-b9cd-4049ce16fd85", "node_type": "4", "metadata": {"issue_id": 709, "title": "How can I  use smolagents in python3.8", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "hash": "1d2d39184c39d3170e6f43481c022a479668afa8226857cdcd57fe2dac5c0978", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How can I  use smolagents in python3.8\n\nDescription: hi, how can I use smolagents in python3.8?\n\nState: closed\n\nLabels: duplicate\n\nCategories: category-duplicate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "018819c7-03fa-4534-b713-3eb7a056e5c3", "embedding": null, "metadata": {"issue_id": 707, "title": "[BUG] Tool.from_space() missing 2 required positional arguments: 'name' and 'description.", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7231459a-f224-488d-b1af-31f9be52324b", "node_type": "4", "metadata": {"issue_id": 707, "title": "[BUG] Tool.from_space() missing 2 required positional arguments: 'name' and 'description.", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "162534a691314948319851dc6e20992c5c52ef8a2d67c7180f6079d650548f45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Tool.from_space() missing 2 required positional arguments: 'name' and 'description.\n\nDescription: **Describe the bug** Running smolagent using cli and getting following error: Tool.from_space() missing 2 required positional arguments: 'name' and 'description' **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** name and description are required parameters so we have to pass it to Tool.from_space **Packages version:** 1.10.0.dev0\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 575, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "be34892d-488f-41a0-bb16-9631a0705600", "embedding": null, "metadata": {"issue_id": 704, "title": "[BUG] The \"translation\" tool does not exist", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26e6c868-cc46-4866-ae5b-8bb00edc4afc", "node_type": "4", "metadata": {"issue_id": 704, "title": "[BUG] The \"translation\" tool does not exist", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "fc0ae8326416e3108fb7ebf45f8f60a0d7cd2e3b436c4d0efdd378a192e4b07a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] The \"translation\" tool does not exist\n\nDescription: **Describe the bug** smolagent cli.py does not recognize tool \"translation\". **Code to reproduce the error** smolagent \"Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7.\" --model-type \"HfApiModel\" --model-id \"Qwen/Qwen2.5-Coder-32B-Instruct\" --imports \"pandas numpy\" --tools --tools \"translation\" **Error logs (if any)** smolagents/cli.py\", line 109, in main raise ValueError(f\"Tool {tool_name} is not recognized either as a default tool or a Space.\") ValueError: Tool translation is not recognized either as a default tool or a Space. **Expected behavior** Expected that the translation tool be known and used. **Packages version:** smolagents==1.9.2 **Additional context** This example is from the main repo page for cli example. The example shows --tools \"web_search translation\". To troubleshoot this further each option was tested separately. When using only \"translation\" the error appears.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1056, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "691891cd-52a9-4611-99c8-8c457d4fc76f", "embedding": null, "metadata": {"issue_id": 703, "title": "SmolAgents AgentMemory is kept in Gradio global state and cannot be turned off.", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4079f84-416f-4296-8c99-35c465855949", "node_type": "4", "metadata": {"issue_id": 703, "title": "SmolAgents AgentMemory is kept in Gradio global state and cannot be turned off.", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "942bf3dc5e39743e0ef066dff7d07b6131110e27e1599dd68a31ee86efcae0a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: SmolAgents AgentMemory is kept in Gradio global state and cannot be turned off.\n\nDescription: **Describe the bug** When launching the agent in Gradio GradioUI(agent).launch(), the agent is kept in the global state of Gradio, meaning all users share the same agent. Although messages seem to be stored in Gradio Session state, some internal agent state like AgentMemory is saved globally I would imagine that it would be better to scope everything on Session State. There is a reset_agent_memory flag on SmolAgents that is False by default, but there is no way to override it. **Code to reproduce the error** Open a chat session in a gradio space and keep talking until you hit the context window limit. Open a new chat session in a different browser. You will not be able to have a conversation due to the context window limit error that was triggered by the first chat session. The global agent memory will consume all of the context size. From that point on the space will be broken for everyone and nobody will be able to start conversations again. **Error logs (if any)** All users will experience an error like this [CODE_BLOCK] **Expected behavior** I would expect that there are proper user scoped sessions. And that one user session cannot influence the behavior of another session. **Packages version:** smolagents==1.9.2 **Additional context** This issue became clear during the HuggingFace Agents Course, when students were asked to promote their Agents through their spaces. Because the underlying LLM had a small context window, as soon as 1 person hit that max context size the space was ruined for everyone. HuggingFace provided a template https://huggingface.co/spaces/agents-course/First_agent_template for students to create agents and tools. That template contains a copy of Gradio_UI.py where we can manipulate the reset_agent_memory flag, but I assume you want to keep Gradio_UI.py a part of SmolAgents and not have people create copies of it. Otherwise the only way to fix the issue is to restart the entire space\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2ea3e52-12f9-487a-b427-f5a2a405354e", "embedding": null, "metadata": {"issue_id": 699, "title": "[Feature request] Enable Tool arguments to process pydantic schemas properly", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "883c005c-52ac-45c3-9b76-dac342ffa577", "node_type": "4", "metadata": {"issue_id": 699, "title": "[Feature request] Enable Tool arguments to process pydantic schemas properly", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "a676fd447d931bc54541911fa552f72abcf453891419f8ae07cea1f71b1a25da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Feature request] Enable Tool arguments to process pydantic schemas properly\n\nDescription: **Is your feature request related to a problem? Please describe.** Tools don't properly pass pydantic schema for the objects in their functions. For example, my tool takes the following as its input parameter [CODE_BLOCK] If I call ReportConfig.model_json_schema() to be passed into, say, OpenAI, [CODE_BLOCK] I get a correct call ReportConfig(sheet_name='ABC', requested_start_date=datetime.date(2024, 1, 1), requested_end_date=datetime.date(2024, 1, 31), report_type='report_1') with the correct tool call {'properties': {'sheet_name': {'description': 'Name of the sheet', 'title': 'Sheet Name', 'type': 'string'}, 'requested_start_date': {'description': 'Start date', 'format': 'date', 'title': 'Requested Start Date', 'type': 'string'}, 'requested_end_date': {'description': 'End date', 'format': 'date', 'title': 'Requested End Date', 'type': 'string'}, 'report_type': {'description': 'Type of the report', 'enum': ['report_1', 'report_2'], 'title': 'Report Type', 'type': 'string'}}, 'required': ['sheet_name', 'requested_start_date', 'requested_end_date', 'report_type'], 'title': 'ReportConfig', 'type': 'object'} However, when I use LiteLLMModel with a tool with the following inputs: inputs = { \"report_config\": { \"type\": \"object\", \"description\": \"Configuration for the new report.\", \"properties\": ReportConfig.model_json_schema()['properties'], }, } I get the call with {'report_config': {'sheet_name': 'ABC', 'requested_start_date': '2024-01-01', 'requested_end_date': '2024-01-31'}} So first, required fields are not passed, which results in an incomplete request. Moreover, the enum is not properly passed as get_json_schema function in ..._hint_utils.py actually uses strange choice way of defining enums **Describe the solution you'd like** I want to be able to specify ReportConfig.model_json_schema() in my tool input as a parameter, for a complex nested model. This will ensure much more consistent tool calling **Is this not possible with the current options.** At least I am not aware how to pass the schema differently **Describe alternatives you've considered** The only alternative I see is to write my own Tool class and overwrite most of the functions there, but it's possible this won't be enough as the LiteLLMModel might work with tools in a very specific way. **Additional context** Basically, it would be good to have at least parity with openai/anthropic tool calls capabilities in tool arguments\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "005bba6a-1647-4990-9264-7b2678079776", "embedding": null, "metadata": {"issue_id": 694, "title": "Real Memory summary for on-going conversations (avoid LLM size limits)", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e9cf61e-b4fb-4fb3-9d45-14956e63ca4f", "node_type": "4", "metadata": {"issue_id": 694, "title": "Real Memory summary for on-going conversations (avoid LLM size limits)", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "ec2c86839766a78445d3879dc18028789ebbd4302e93d41680e23ae48017321d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Real Memory summary for on-going conversations (avoid LLM size limits)\n\nDescription: **Is your feature request related to a problem? Please describe.** the framework has basic truncation and message removal, but NO built-in summarization to manage long-term memory growth. It will eventually break if the memory exceeds the LLM's context window. This is a significant area for improvement. **Describe the solution you'd like** Memory management capable of creating long-term memories (convert older messages into a short summary/facts version). Limit the memory to only last e.g. 100 memories. For older memories, use RAG to retrieve any relevant memory if at all. **Is this not possible with the current options.** this kind of memory management is not possible natively in smolagents **Describe alternatives you've considered** I've developed my own external memory management for long-term memories. but it's a pain to keep it in sync with the main repo version **Additional context** somehow similar issue here: https://github.com/huggingface/smolagents/issues/531\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4061cbaa-b5cb-4c45-8c9f-a0c4310394be", "embedding": null, "metadata": {"issue_id": 692, "title": "[BUG] CI test fails: TypeError: LlavaProcessor: got multiple values for keyword argument 'images'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c0a975f-ee54-400c-a825-745306dd62ba", "node_type": "4", "metadata": {"issue_id": 692, "title": "[BUG] CI test fails: TypeError: LlavaProcessor: got multiple values for keyword argument 'images'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f9e070af38b71b225848247bc1dc1962ae9271edb7b1037e064c43b4837c2ff6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] CI test fails: TypeError: LlavaProcessor: got multiple values for keyword argument 'images'\n\nDescription: **Describe the bug** CI test fails: TypeError: LlavaProcessor: got multiple values for keyword argument 'images' > FAILED tests/test_models.py::ModelTests::test_transformers_message_vl_no_tool - TypeError: LlavaProcessor: > ... > got multiple values for keyword argument 'images' **Error logs (if any)** [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 516, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c72d5197-93a8-4d60-a96c-03a90c104864", "embedding": null, "metadata": {"issue_id": 690, "title": "[BUG] Link in docs to open in Colab does not work", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb5fc18d-976b-4975-9b66-edf838df1c3a", "node_type": "4", "metadata": {"issue_id": 690, "title": "[BUG] Link in docs to open in Colab does not work", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f82c4379f7b721cf487d0275f05876b092acfeeaee006113537f3e495fbf5a66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Link in docs to open in Colab does not work\n\nDescription: **Describe the bug** Link in docs to open in Colab does not work, even after adding the notebook_folder arg to Build docs, done in PR: - 671 Related issues: - 647 - 211 - 113 **Code to reproduce the error** Click \"Open in Colab\" link. **Error logs (if any)** > Notebook not found > > There was an error loading this notebook. Ensure that the file is accessible and try again. > https://github.com/huggingface/notebooks/blob/main/smolagents_doc/en/secure_code_execution.ipynb In the GH Action to build the docs, it appears the message: https://github.com/huggingface/smolagents/actions/runs/13374673277/job/37351097439 > Notebooks creation was not enabled.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "052285ce-d032-4292-942f-396eb09292c0", "embedding": null, "metadata": {"issue_id": 685, "title": "Python logging missing console output (solved: workaround with Rich)", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46b3aec9-ead4-414e-af15-88b0e866a633", "node_type": "4", "metadata": {"issue_id": 685, "title": "Python logging missing console output (solved: workaround with Rich)", "state": "open", "labels": [], "type": "issue"}, "hash": "788693cc76e107fbfc2e3d1c0a9a92e52cc70abcf58f4260e49144832985e8d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Python logging missing console output (solved: workaround with Rich)\n\nDescription: Description: I ran into an issue while trying to capture console output into a log.log file for manual inspection. I initially assumed that interfacing with the Python logging module in the library would be sufficient, but I noticed that most of the console output\u2014particularly the \"thinking\" process\u2014was missing from my logs. Only a few API calls and minimal details were being logged. After investigating, I realized that most of the console output is handled by Rich (monitoring.py) rather than the standard logging module. As a result, simply configuring logging didn\u2019t capture all the relevant information. Workaround: Dual Console Logging with Rich To solve this, you can create a dual-output system using Rich\u2019s Console class. This setup allows logging to both log.log and the terminal while maintaining proper formatting. I needed this solution because I'm still getting familiar with LLM/agentic frameworks and am not yet ready to dive into an observability platform. Just needed a simple way to test things. Here is a full working script for reference (simple web search agent writing to both console and log.log file): [CODE_BLOCK]\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1245, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48444fa0-ed80-4d5a-8913-b63c5071641e", "embedding": null, "metadata": {"issue_id": 683, "title": "DAG Agents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e10e60d-4d44-431e-8638-07fe2b7aaa92", "node_type": "4", "metadata": {"issue_id": 683, "title": "DAG Agents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "14a4bd44623809b81ac53c1d727b82b02cb171a2a40bd5151727d46036db8579", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: DAG Agents\n\nDescription: Dear SmolAgents Team, I have been exploring the library and really appreciate its capabilities and potential. My question is the following: is there currently a way to create a graph-based structure for organizing agents or tools workflow. We believe adding support for a Directed Acyclic Graph (DAG) could be a useful enhancement. Indeed, we plan to use structure like DAG to constraint how agents and tools collaborate in order to handle tasks where such a priori knowledge about process is available. Looking forward to your insights! Best, Anton Conrad - Data Scientist @Epitech\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7cc07bb0-d79e-4dee-bba5-7165717608b6", "embedding": null, "metadata": {"issue_id": 681, "title": "[BUG] Tool defined in class triggers TypeHintParsingException", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "026b6d25-1f0c-47fe-a723-c3f31ce42036", "node_type": "4", "metadata": {"issue_id": 681, "title": "[BUG] Tool defined in class triggers TypeHintParsingException", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "d5f0b862f73cfd724e54f200ea2116ede7b678b0aaa5be41b374068359e222ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Tool defined in class triggers TypeHintParsingException\n\nDescription: **Describe the bug** Defining class instance methods as a tool raises: smolagents._function_type_hints_utils.TypeHintParsingException: Argument self is missing a type hint in function test_tool **Code to reproduce the error** example: [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** I am trying to define tools inside classes, so I have access to other methods and properties **Packages version:** 1.9.2 **Additional context** This happens with both ToolCalling and Code agents. I briefly looked inside the docs to see if this case is already covered, but could find anything related except the e2b_example, which is not so close to what I want. If I modified my example like this def test_tool(self: Any .... then I get: [CODE_BLOCK] That happens because line new_parameters = [inspect.Parameter(\"self\", inspect.Parameter.POSITIONAL_ONLY)] + list( original_signature.parameters.values() ) Is already prepending a \"self\" parameter (declared as positional only) to the list of parameters from the original function signature.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2fa3bec-1932-4576-9882-46bd98ffdf8e", "embedding": null, "metadata": {"issue_id": 680, "title": "A/B testing", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07529848-d048-4021-9a83-cbc89a5e5afb", "node_type": "4", "metadata": {"issue_id": 680, "title": "A/B testing", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "cf516e95d0c30efc4f01f87ac8a5ec44e8e7599f27736ce2d877c84fe1e07ced", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: A/B testing\n\nDescription: Problem: with such wild variability in output based on not only the LLMs but the prompts, small changes can result in quite significant differences. Solution: ability to specify a list of prompt variations and a list of different LLMs to try. You could use Optuna for efficient evaluation (cf DSPy), along with argilla the human evaluation.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40c9789b-c051-43f4-9436-d97afb8ec488", "embedding": null, "metadata": {"issue_id": 679, "title": "Open Deep Research deep dive", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8485ff8f-6579-41a3-941e-efa419513ef3", "node_type": "4", "metadata": {"issue_id": 679, "title": "Open Deep Research deep dive", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "7d5ca86877dcfc47b22bc26439b0a8003db6ade3cdb33be27fa579d262f54b43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Open Deep Research deep dive\n\nDescription: Open Deep Research is an exciting launch opportunity. It'd be really great to have a blow by blow account of how it was designed: eg 1. How do you decide which external imports are allowed? 2. How do you decide which tools to use? Naively, a deep research agent only needs an LLM, web search and web page extractor yet this agent has a fascinating array of custom tools.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "171e3f0a-dc55-4680-b434-5955a457e032", "embedding": null, "metadata": {"issue_id": 676, "title": "[BUG] Deep Research: Can't use reasoning_effort=high", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "064a6d06-fb0f-4683-acd6-cce3a8d4cdf5", "node_type": "4", "metadata": {"issue_id": 676, "title": "[BUG] Deep Research: Can't use reasoning_effort=high", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "a15b42bbc65e235582c066c5e558838750ed1736caddd3729bd3854afb24e15c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Deep Research: Can't use reasoning_effort=high\n\nDescription: **Describe the bug** I don't seem to have access to \"o1\" and \"o1-mini\" seemed to fail so I tried claude-3.5-sonnet-xxxx and had to turn reasoning_effort=high off and set token output to 8192 and I got some\u2026 modest output. Anyone able to get reasoning_effort=high working? For which models? I have ollama so I can use that too. **Code to reproduce the error** Just run.py in the deep research example **Error logs (if any)** [CODE_BLOCK] **Expected behavior** That the example would use a model that is widely available. I assume o1 is tier-restricted? **Packages version:** [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "409ee7dd-cde3-46c1-9eee-ef1fe72914b0", "embedding": null, "metadata": {"issue_id": 672, "title": "Why smolagents open_deep_research can surpass auogen magnet one on Gaia benchmark", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "906b202a-35e4-42fb-a7e5-f69f07500634", "node_type": "4", "metadata": {"issue_id": 672, "title": "Why smolagents open_deep_research can surpass auogen magnet one on Gaia benchmark", "state": "closed", "labels": [], "type": "issue"}, "hash": "e5985ecbe45d986734636035b819c68f33dd9171969ec87ea1c124ed34b7ab26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Why smolagents open_deep_research can surpass auogen magnet one on Gaia benchmark\n\nDescription: I am researching agents architecture among these open-source agents. I see that smolagents open_deep_research surpasses auogen magnet one on Gaia benchmark. Smolagents and autogen can both use web and coding tools. Compared to Autgen, why smolagents can lead so much ahead.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "263b35f0-1faa-40dc-905f-a3a31d8c4fdd", "embedding": null, "metadata": {"issue_id": 663, "title": "\"Skipping dangerous attribute\" check", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "735f0181-b048-4fe6-be6a-7845b9bce6b2", "node_type": "4", "metadata": {"issue_id": 663, "title": "\"Skipping dangerous attribute\" check", "state": "closed", "labels": [], "type": "issue"}, "hash": "95f438856f79bb1fdc71fd3d44d01a6c8f712b6d2055082e9266c07131d0d200", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: \"Skipping dangerous attribute\" check\n\nDescription: Logs issue I've wrapped smolagents.CodeAgent in a FastAPI app and noticed that when running the generated code with imports, thousands of INFO-level logs are produced by LocalPythonInterpreter, such as: [CODE_BLOCK] Full log from single run in the attached app.log. Would it be possible to adjust the logger level for these messages to DEBUG? This would help reduce log clutter during regular FastAPI execution. Here's minimal example: [CODE_BLOCK] Run it locally - Terminal 1: uvicorn minimal_example_agent:app --reload - Terminal 2: curl -X 'GET' 'http://0.0.0.0:8000/execute' Additional question Lastly, I\u2019m curious about how dangerous_patterns are applied at any levels. As seen in the logs, modules like pandas.io.formats.printing.Any are being excluded simply because their path contains 'io', even though they don\u2019t actually use the io library. Would it be possible to modify the logic to check only the top-level module against dangerous_patterns, rather than filtering based on any part of the path?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ec33b79-44d0-433f-9137-cd4d7b057688", "embedding": null, "metadata": {"issue_id": 662, "title": "[BUG] Images not displaying with GradioUI even when passed as the final answer", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d7ddcfa-230d-4029-b7ba-7dd49af3c626", "node_type": "4", "metadata": {"issue_id": 662, "title": "[BUG] Images not displaying with GradioUI even when passed as the final answer", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "49c15cf324d7c9a38b25f0984049ce848e0d51b69ef27d5ad60c4387ab6d879d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Images not displaying with GradioUI even when passed as the final answer\n\nDescription: The code in gradio_ui.py code suggests that if a png filename is passed as the final answer it will display the image. elif isinstance(final_answer, AgentImage): yield gr.ChatMessage( role=\"assistant\", content={\"path\": final_answer.to_string(), \"mime_type\": \"image/png\"}, ) I passed this prompt: \"Plot sin(x) from 0 to 1. Then save the plot as my_plot.png. Then give my_plot.png for the final answer so that the image displays.\" I used a code agent with GradioUI code: from smolagents import CodeAgent, GradioUI, HfApiModel, LiteLLMModel AUTHORIZED_IMPORTS = [ \"numpy\", \"matplotlib\", \"seaborn\", ] agent = CodeAgent(tools=[], model=HfApiModel(), max_steps=20, verbosity_level=2,additional_authorized_imports=AUTHORIZED_IMPORTS) GradioUI(agent).launch() See the screenshot below. The agent submitted my_plot.png as the final answer. But as the screenshot shows it did not display the png. Thanks. !Image\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1080, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5c2cae8-b2fe-4cb5-bd3d-bdbda926a08f", "embedding": null, "metadata": {"issue_id": 661, "title": "Display plots in Gradio", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd91cf56-6de0-48df-bd43-ff9ba72ade02", "node_type": "4", "metadata": {"issue_id": 661, "title": "Display plots in Gradio", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "10a9ad904596d24179ee04e88fe3ae74b7c1550623051a505f0307c3f4e2207b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Display plots in Gradio\n\nDescription: **Is your feature request related to a problem? Please describe.** If I make a simple CodeAgent with numpy and matplotlib it should have all the imports to create a plot. If I run it in GradioUI such as: from smolagents import CodeAgent, GradioUI, HfApiModel AUTHORIZED_IMPORTS = [ \"numpy\", \"matplotlib\", \"seaborn\", ] agent = CodeAgent(tools=[], model=HfApiModel(), max_steps=20, verbosity_level=1,additional_authorized_imports=AUTHORIZED_IMPORTS) GradioUI(agent).launch() It gives a nice GUI interface. If I then give the prompt: \"Plot sin(x) from x=0 to x=1\" It can use numpy and matplotlib to generate a plot, and sometimes it will even save the plot as say myplot.png. But it never seems to display that plot in the gradio gui. If it could that would be awesome. Looking at the code, it seems like AgentImage is a return type for the GradioUI. I'm not sure why this never seems to display the image, but it would be nice if it would.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1090, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7573ea06-de3e-42e0-8bf8-bbaaea2e4dc2", "embedding": null, "metadata": {"issue_id": 658, "title": "[BUG] 413 error when sharing from Gradio Chatbot component", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5bd7d77c-cc58-4b16-8c9a-a7397a73e974", "node_type": "4", "metadata": {"issue_id": 658, "title": "[BUG] 413 error when sharing from Gradio Chatbot component", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "0b029c39bbf448db16c2c4c7d8721c00aa17aa9e131ce778a8119fcd48ada27a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] 413 error when sharing from Gradio Chatbot component\n\nDescription: click on the share button and 413 err pops up the grey button on top right of chat result box !Image !Image **Code to reproduce the error** The simplest code snippet that produces your bug. **Error logs (if any)** Provide error logs if there are any. **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42eabdb0-2430-4ffe-bba8-3b2386871c58", "embedding": null, "metadata": {"issue_id": 657, "title": "[BUG] Guided Tour text and code refers to class ManagedAgent which has been removed", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6305c028-140c-4157-b878-48d16d2aabf8", "node_type": "4", "metadata": {"issue_id": 657, "title": "[BUG] Guided Tour text and code refers to class ManagedAgent which has been removed", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "b23c927f05446ade28e896fef9d35dd716cca25e52ddfab92cf718890f72c835", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Guided Tour text and code refers to class ManagedAgent which has been removed\n\nDescription: **Describe the bug** https://huggingface.co/docs/smolagents/guided_tourmulti-agents refers to a class ManagedAgent: \"To do so, encapsulate the agent in a ManagedAgent object. [...]\" Also in the code example: [CODE_BLOCK] According to the changelog, this class was removed in 1.8.0.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "feab25d3-ccd6-410c-b467-32d90030c5b9", "embedding": null, "metadata": {"issue_id": 656, "title": "Using Open-Deep-Research with Inference Endpoints", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "857a43ba-375a-4eb5-8751-0089d45bc928", "node_type": "4", "metadata": {"issue_id": 656, "title": "Using Open-Deep-Research with Inference Endpoints", "state": "open", "labels": [], "type": "issue"}, "hash": "92732d6a84a1652747f432cc9e2f278ee9ab912f4320b3131b3dbb53f401d50a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Using Open-Deep-Research with Inference Endpoints\n\nDescription: I tried to run Open-Deep-Research with an inference endpoint in a way that's possible to deploy for example deepseek-r1:670B but it seems no possible to use it. I tried using Ollama / LiteLLM, but no option to configure it correctly, is it possible to use it, or the model class won't work with it?\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 382, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34ff43c9-1c56-499a-b611-65d5c0a2dd65", "embedding": null, "metadata": {"issue_id": 655, "title": "[BUG] Sambanova, Groq and Cerebras models not found after update to 1.9.1", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7a5a4c6-9b0e-4ca1-a680-bcaa832b7e09", "node_type": "4", "metadata": {"issue_id": 655, "title": "[BUG] Sambanova, Groq and Cerebras models not found after update to 1.9.1", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "ae6a33a3247229ee555079e55512cf24bdab45205e939dfcf68a7ebf5df0916b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Sambanova, Groq and Cerebras models not found after update to 1.9.1\n\nDescription: **Describe the bug** Sambanova, Groq and Cerebras models not found after update to 1.9.1. Running an agent (whether Code or ToolCalling) using models from said companies ends up in an error. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** In verion 1.8.1 this was working normally with a smol tweak mentioned in here **Packages version:** 1.9.1\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 574, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a0b0e802-99bb-45c7-a565-51d6b207e0c5", "embedding": null, "metadata": {"issue_id": 647, "title": "[BUG] Guided Tour does not open in Colab", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "024fb0fb-7a75-4493-9e7b-bed8613d789b", "node_type": "4", "metadata": {"issue_id": 647, "title": "[BUG] Guided Tour does not open in Colab", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "c0491f09ba14c3223bb4b85684ee52386754d7669c56aba31ccc3b46b9567762", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Guided Tour does not open in Colab\n\nDescription: **Describe the bug** When I click \"Open in Colab\" button on the Guided Tour online https://huggingface.co/docs/smolagents/guided_tour regardless of the option chosen (Mixed, PyTorch, tensorflow) I get an error: \"Notebook not found There was an error loading this notebook. Ensure that the file is accessible and try again. [...]\" The link in the error message is https://github.com/huggingface/notebooks/blob/main/smolagents_doc/en/guided_tour.ipynb which results in \"404 - page not found The main branch of notebooks does not contain the path smolagents_doc/en/guided_tour.ipynb.\" Note: Colab is authorized in Github, this is **not** a permissions problem.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4cc52d4f-1d96-41be-a756-95a10010a696", "embedding": null, "metadata": {"issue_id": 646, "title": "[BUG] Open researcher get 400 error from litellm after planning step when using deepseek-r1 (CodeAgent)", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ec7a4bc-6465-4939-ab5a-c19f5a49ae6d", "node_type": "4", "metadata": {"issue_id": 646, "title": "[BUG] Open researcher get 400 error from litellm after planning step when using deepseek-r1 (CodeAgent)", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "400946ec05807d6b7b442e858811fe452b88684189346421ff4a39945848b6ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Open researcher get 400 error from litellm after planning step when using deepseek-r1 (CodeAgent)\n\nDescription: **Describe the bug** I'm using open researcher with deepseek-r1 using litellmmodel. The CodeAgent after the planning steps would create an input message that contains two consecutive message from \"assistant\". See the following screenshot from the open-telemetry. !Image The first assistant message is from the \"initial_facts\" step while the second assistant message was from the \"initial_planning\" step. This message would cause the deepseek-r1 model to return a 400 error code. **Code to reproduce the error** open researcher with deepseek-r1 as model **Error logs (if any)** Error in generating model output: litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': '<400> InternalError.Algo.InvalidParameter: An unknown error occurred due to an unsupported input format.', 'type': 'invalid_request_error'}, 'id': 'chatcmpl-df620c7e-749a-999e-b873-bc32d7a6112a', 'request_id': 'df620c7e-749a-999e-b873-bc32d7a6112a'} **Packages version:** 1.8.1 **Additional context** Add any other context about the problem here.\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "95e08d47-a0af-4bae-9de3-51172c2d5276", "embedding": null, "metadata": {"issue_id": 640, "title": "Multi Agent code execution failed - InterpreterError: It is not permitted to evaluate other functions than the provided tools or functions defined/imported in previous code", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bce42228-6785-4733-a486-f25fa344909b", "node_type": "4", "metadata": {"issue_id": 640, "title": "Multi Agent code execution failed - InterpreterError: It is not permitted to evaluate other functions than the provided tools or functions defined/imported in previous code", "state": "closed", "labels": [], "type": "issue"}, "hash": "d72d5ad6e28277ed9cebffe86dc49527d531148709e83c137ad754993857832d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Multi Agent code execution failed - InterpreterError: It is not permitted to evaluate other functions than the provided tools or functions defined/imported in previous code\n\nDescription: Hey, I'm getting this error all the time: **InterpreterError:It is not permitted to evaluate other functions than the provided tools or functions defined/imported in previous code** In manager.run I'm trying to pass additional_args=dict(source_file=\"file.csv\"). Example of agent: [CODE_BLOCK] [CODE_BLOCK] It works normally when I try to call agent_core.run directly, without manager_agent. Does anyone have a similar problem?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cdd6e36c-f230-48b1-8803-fdbfa3f60427", "embedding": null, "metadata": {"issue_id": 639, "title": "[BUG] CodeAgent no longer takes system prompt (?) - Breaking Change", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc8381bf-00c4-4f69-a849-cff4f7cfa113", "node_type": "4", "metadata": {"issue_id": 639, "title": "[BUG] CodeAgent no longer takes system prompt (?) - Breaking Change", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "d2a197dd6c2fe37d09029e2a91b771e733b7eea66fb72e48d283d44b178e4219", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] CodeAgent no longer takes system prompt (?) - Breaking Change\n\nDescription: **Describe the bug** It seems that system_prompt is no longer available, despite being shown here: https://huggingface.co/docs/transformers/v4.48.2/en/main_classes/agenttransformers.CodeAgent and also in some Github examples. **Code to reproduce the error** NA **Error logs (if any)** NA **Expected behavior** NA **Packages version:** smolagents==1.8.1 **Additional context** This is breaking any older scripts that try to specify a prompt. Note that \"from smolagents.prompts import CODE_SYSTEM_PROMPT\" is also broken/deprecated\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a13a84d6-c70e-4e6d-b26d-73a9ca50ca89", "embedding": null, "metadata": {"issue_id": 637, "title": "[BUG] Open Deep Research search is failing on HF Spaces", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c4f33ceb-fb77-4e78-8f82-4c7ae884392d", "node_type": "4", "metadata": {"issue_id": 637, "title": "[BUG] Open Deep Research search is failing on HF Spaces", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "1ba5bd4a2fd22d4d369f10c6ea855ed76aa803bdcc895adc38691a41b90da63a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Open Deep Research search is failing on HF Spaces\n\nDescription: **Describe the bug** When running a query it seems that the search element for Open Deep Research on HF spaces is not working (see screen shot) **Code to reproduce the error** Run the query below [CODE_BLOCK] **Error logs (if any)** Output suggests web search is failing completely: * \"Since querying the URL directly didn't provide results, I should instead perform a web search using keywords related to the model and ollama platform to gather relevant details. This will help identify any documentation or articles that discuss its usage requirements and compatibility.\" * \"Since the specific queries have not yielded results\" **Expected behavior** Search tooling should return decent results for terms and URLs provided. **Packages version:** N/A -- HF spaces **Additional context** <img width=\"966\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/f2bb5656-c9c8-441a-a527-fff2636c7783\" />\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1062, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "471ce790-c866-482c-9c7b-32e96fdcf5fb", "embedding": null, "metadata": {"issue_id": 635, "title": "[BUG] Installation instructions for open_deep_research still produce an error", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac53c92d-86d3-4b2c-aa61-ed259521300c", "node_type": "4", "metadata": {"issue_id": 635, "title": "[BUG] Installation instructions for open_deep_research still produce an error", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "57efadc400f220e3961d09e5fae780cb5f9200b4d8920f0fbdecb93034eb33fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Installation instructions for open_deep_research still produce an error\n\nDescription: **Describe the bug** The current installation instructions do not result in a working build **Code to reproduce the error** pip install -e smolagents[dev] **Error logs (if any)** ERROR: smolagents[dev] is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file). **Expected behavior** Command should build smolagents from within smolagents\\examples\\open_deep_research **Packages version:** openinference-instrumentation-smolagents==0.1.4 -e git+https://github.com/huggingface/smolagents.git@41a388dac60013b9957768bd36a45cafb8aa5efeegg=smolagents\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "534aa1cd-4747-4435-a4b6-6c912d4b0216", "embedding": null, "metadata": {"issue_id": 629, "title": "If possible, could you please provide the model output files of gaia validation", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eea84e11-dbf7-4dc7-acae-3f57b245372c", "node_type": "4", "metadata": {"issue_id": 629, "title": "If possible, could you please provide the model output files of gaia validation", "state": "closed", "labels": [], "type": "issue"}, "hash": "821775c99b1897f5c3853fbbdd682350683588f1b16bb19bbdf76398ca8a26b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: If possible, could you please provide the model output files of gaia validation\n\nDescription: Thank you for this reproducing work, which is really cool! I am currently trying to study deep research related work and I really hope to analyze how model works under codeagent and toolagent step by step. May I ask if you can provide the model output files (the input for analysis.ipynb) for the 55.15% on codeagent version and the 33% toolagent version mentioned in the article ? This will be very helpful for my study. Thank you very much!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "363f748e-8944-408c-856c-f631b50e16f1", "embedding": null, "metadata": {"issue_id": 626, "title": "[BUG] MCP ToolCollection raises KeyError: 'type'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3399e176-b658-45ef-acbe-1c00f21d6160", "node_type": "4", "metadata": {"issue_id": 626, "title": "[BUG] MCP ToolCollection raises KeyError: 'type'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "427aeaacd4d7e2514c31c8c8af7db392b1f8b650baa8ef818adfb4ff956e2cce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] MCP ToolCollection raises KeyError: 'type'\n\nDescription: **Describe the bug** When loading an MCP tool (e.g. mcp_server_tavily), an exception occurs in _generate_tool_inputs(...). key \"type\" is not found. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** No exception **Packages version:** smolagents==1.8.1 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "beeebc64-524c-4e72-a14f-95599d213490", "embedding": null, "metadata": {"issue_id": 622, "title": "Support OpenAI (or similar) Batch API like functionality", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6817ca62-be0c-4c41-91e9-7bc7951fd221", "node_type": "4", "metadata": {"issue_id": 622, "title": "Support OpenAI (or similar) Batch API like functionality", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "1cf8d3d3e65beacbef39718f0089050cdd423fe4a42970f4954e6e652eb82073", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support OpenAI (or similar) Batch API like functionality\n\nDescription: **Is your feature request related to a problem? Please describe.** With agents, we might not necessarily need responses instantaneously, to reduce costs, by half atleast, supporting batch/async processing functionality might help. **Describe the solution you'd like** Not fully flushed yet, but lets say we want OpenAI chat completions, instead of calling the API directly, we can call the https://platform.openai.com/docs/guides/batch API option. Any parallel non dependent tasks could carry on, while this pipeline waits for the response to come up. Once the response is available, the followup tasks continue. **Is this not possible with the current options.** From looking up at code in some places, I do not see it implemented. **Describe alternatives you've considered** this is a functionality addition, a nice to have, not necessarily a must have probably, hence not a feature yet? **Additional context** https://platform.openai.com/docs/guides/batch support for these options.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54cce97f-6b5c-492a-b69e-28ba85f6a102", "embedding": null, "metadata": {"issue_id": 620, "title": "Please share the GAIA detailed benchmark report deepseek R1 vs gpt 4o vs claude 3.5 sonnet", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab7623e9-a1c5-4533-9c51-6f1e260aee81", "node_type": "4", "metadata": {"issue_id": 620, "title": "Please share the GAIA detailed benchmark report deepseek R1 vs gpt 4o vs claude 3.5 sonnet", "state": "closed", "labels": [], "type": "issue"}, "hash": "aa0330653ed65f19d5369950c3211a795c8fdb29574503b81a61a7b70f11fc9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Please share the GAIA detailed benchmark report deepseek R1 vs gpt 4o vs claude 3.5 sonnet\n\nDescription: I am trying to figure out the pros and cons of DeepSeek R1 compared to the SOTA chat model like gpt 4o. Can somebody share the GAIA detailed benchmark of GAIA. I want to get the feeling of details of DeepSeek R1 advantages\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 349, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31263969-4097-49c1-974b-fb61587cdeac", "embedding": null, "metadata": {"issue_id": 619, "title": "[BUG] nothing happened when run example", "state": "closed", "labels": ["invalid"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41372ece-38d3-42db-b516-caed0e2ea0b0", "node_type": "4", "metadata": {"issue_id": 619, "title": "[BUG] nothing happened when run example", "state": "closed", "labels": ["invalid"], "type": "issue"}, "hash": "bde9f9652ed641e987124f4d6cb7ceafa9c991d26fdc94e9f76e1ed308f21ce4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] nothing happened when run example\n\nDescription: **Describe the bug** I run the example code in cmd line, nothing happend then process exit **Code to reproduce the error** The simplest code snippet that produces your bug. import os from smolagents import CodeAgent, DuckDuckGoSearchTool, OpenAIServerModel if __name__ == '__main__': print('=================') model = OpenAIServerModel( model_id=\"Qwen25-7B-Instruct\", api_base=\"http://101.230.144.224:20336/v1/chat/completions\" ) agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model) agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\") print('========') **Error logs (if any)** nothing **Packages version:** Run pip freeze | grep smolagents and paste it here. Name: smolagents Version: 1.8.1 Summary: \ud83e\udd17 smolagents: a barebones library for agents. Agents write python code to call tools or orchestrate other agents. Home-page: Author: Thomas Wolf Author-email: Aymeric Roucher <aymeric@hf.co> License: Location: c:\\users\\54698\\documents\\gitlab\\agent-demo\\.venv\\lib\\site-packages Requires: rich, duckduckgo-search, python-dotenv, huggingface-hub, pillow, jinja2, requests, pandas, markdownify Required-by: **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: invalid\n\nCategories: category-invalid", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1355, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31c7e56f-ad97-4609-bec7-1d5aa63445fd", "embedding": null, "metadata": {"issue_id": 611, "title": "[BUG] Installation instructions for open_deep_research are fiction", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "21d8c527-bd63-4774-86fa-4eec9838039e", "node_type": "4", "metadata": {"issue_id": 611, "title": "[BUG] Installation instructions for open_deep_research are fiction", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "959c61e6d0f7c935d6090f6e782a9f58d463ea0b9d2200bbd353187fe78cf114", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Installation instructions for open_deep_research are fiction\n\nDescription: [CODE_BLOCK] **Code to reproduce the error** ai/code/smolagents/examples/open_deep_research$ pip install -e .[dev] **Error logs (if any)** ERROR: file:///media/ai/code/smolagents/examples/open_deep_research does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found. **Expected behavior** Installed package **Packages version:** latest git clone\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37e87783-b03b-4846-a8cd-8b62ef1fa7de", "embedding": null, "metadata": {"issue_id": 610, "title": "Is this normal? Im getting this a lot", "state": "closed", "labels": ["question"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c46694dc-b139-45e8-b482-c554835a763d", "node_type": "4", "metadata": {"issue_id": 610, "title": "Is this normal? Im getting this a lot", "state": "closed", "labels": ["question"], "type": "issue"}, "hash": "b47387435c58156bb9f700db1f6c3e81b19303169676c595e97799ea274e6335", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Is this normal? Im getting this a lot\n\nDescription: Hey, is this normal? !Image also, out: None is this ok as well??\n\nState: closed\n\nLabels: question\n\nCategories: category-question\n\nType: Question or help request", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 219, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5af9b416-d55c-46e9-9162-a9cd8d4b02bb", "embedding": null, "metadata": {"issue_id": 607, "title": "Agent raises warning/error if max_tokens exhausted in a step", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a68cfa04-e6d1-4595-9b33-59ef5714a5b0", "node_type": "4", "metadata": {"issue_id": 607, "title": "Agent raises warning/error if max_tokens exhausted in a step", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "ecfe39b7cce9cdf3b6d96c2ed38edfa2a14fbc29b29776e42394b188e3f97fee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Agent raises warning/error if max_tokens exhausted in a step\n\nDescription: **Is your feature request related to a problem? Please describe.** The errors that can occur when max_tokens is used up by the model are not intuitive. Usually they are some kind of parsing error that keeps repeating until max steps is reached. **Describe the solution you'd like** Since the agent can access input/output token counts, or see the stop reason in the output message, the agent could provide a specific warning/error if tokens were exhausted in a step.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "27fb4159-8ced-4b25-87d8-68a8f5e9cc96", "embedding": null, "metadata": {"issue_id": 606, "title": "[BUG] Error when using ToolCallingAgent as manager", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aedcacd1-0088-4bd6-957b-e9f1753a664b", "node_type": "4", "metadata": {"issue_id": 606, "title": "[BUG] Error when using ToolCallingAgent as manager", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "57c9e8e7ff7fd5a7641cecca1c0e994383e9b737a88d6f6db1d422c6995a0d4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Error when using ToolCallingAgent as manager\n\nDescription: **Describe the bug** Error in calling team member **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** 1.8 **Additional context** I am trying to create a manger agent of type ToolCallingAgent, by simply modifying the multi-agents example available on the docs, but instead of a CodeAgent I use the ToolCallingAgent as the manager agent.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cadb06ad-ca2c-4941-a948-a6f107777cd4", "embedding": null, "metadata": {"issue_id": 597, "title": "[BUG] Task not indicated in planning prompt", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a23dfb4-f0ed-4603-8f68-3c2700a73bbe", "node_type": "4", "metadata": {"issue_id": 597, "title": "[BUG] Task not indicated in planning prompt", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "abf3a3a12a805e0507162a23b0fc4384a693d6840d98d9b1a03110707ec70d68", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Task not indicated in planning prompt\n\nDescription: **Describe the bug** It seems to me that the framework misses to use the task variable in the initial planning step, leading some models to ask back what the task is. **Code to reproduce the error** Run an agent with gpt-4o and the standard prompt. **Error logs (if any)** -- **Expected behavior** The agent should instead make up some information. **Packages version:** smolagents==1.8.0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf8834eb-5d7f-4c1b-afed-ccf7d2309761", "embedding": null, "metadata": {"issue_id": 596, "title": "[Docs] Add Authentication Note and Link to Guided Tour in Quick Demo Section", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "001cd6e5-2392-4275-916a-4c868fe7d6cb", "node_type": "4", "metadata": {"issue_id": 596, "title": "[Docs] Add Authentication Note and Link to Guided Tour in Quick Demo Section", "state": "open", "labels": [], "type": "issue"}, "hash": "d51fd00c1f912868b16a0c6b63c55b009f385eaf3609b56a77506dbe62ffff19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Docs] Add Authentication Note and Link to Guided Tour in Quick Demo Section\n\nDescription: The current Quick demo section in the smolagents README does not mention that authentication may be necessary for certain use cases, especially when using HfApiModel. This can lead to confusion when users encounter errors or when trying to access gated models. Suggested Change Add a short note in the Quick demo section explaining that: - Users may need to authenticate to avoid rate limits or access gated models. - Link to Guided Tour for detailed instructions on how to set the token. Why This is Important - Helps new users avoid common errors - Ensures smoother onboarding experience\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57e4ad80-0ec1-47c4-9d4e-13158dfeca10", "embedding": null, "metadata": {"issue_id": 593, "title": "Tools Spaces Gallery Overview", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab7f342a-9fb1-4d70-be6f-9f71b60b5e3e", "node_type": "4", "metadata": {"issue_id": 593, "title": "Tools Spaces Gallery Overview", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "571e0f0174914f53b0ef01c2b497d0b4dd138877edfbd4ffe49c2431255fb8fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Tools Spaces Gallery Overview\n\nDescription: **Is your feature request related to a problem? Please describe.** It would be cool to get an overview of tools and tool-calling tuned models. **Describe the solution you'd like** Something like: https://huggingface.co/spaces/enzostvs/zero-gpu-spaces or https://huggingface.co/spaces/gradio/theme-gallery **Is this not possible with the current options.** Only via API calls using tags tools agents smolagents **Describe alternatives you've considered** NA **Additional context** NA\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 641, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21a3761a-553d-4406-9e0c-17bdaaeafaf7", "embedding": null, "metadata": {"issue_id": 590, "title": "`AzureOpenAIServerModel` doesn't support Microsoft Entra ID authentication", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f75571e-a8f8-49ce-bae9-dd2bc0115000", "node_type": "4", "metadata": {"issue_id": 590, "title": "`AzureOpenAIServerModel` doesn't support Microsoft Entra ID authentication", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "b7a5cc8299e916bc78aa6dc82d429190e49c00e1fe4cde7444dbfc82e9bd2213", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: `AzureOpenAIServerModel` doesn't support Microsoft Entra ID authentication\n\nDescription: The current AzureOpenAIServerModel can only be authenticated via API key. While this is useful, does not account for the use of Microsoft Entra ID to authenticate. We can make a simple change to the AzureOpenAIServerModel to enable such authentication method. Pull request to be created to address this issue. The usage example can be found below: [CODE_BLOCK]\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 564, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c89cf0d-aa9a-45c4-8cb6-58b5b4c434ef", "embedding": null, "metadata": {"issue_id": 589, "title": "[BUG] getting started example not working", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ac96821-25b8-4599-b6db-54109a9815c2", "node_type": "4", "metadata": {"issue_id": 589, "title": "[BUG] getting started example not working", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "a189d6dd4727d2bb18578b1dc941d63da499b1d254ec99c735c195d7b3d9e6d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] getting started example not working\n\nDescription: **Describe the bug** When I try to run the example you have on the homepage, it gets stuck saying that \"Model too busy, unable to get response in less than 60 second(s)\" **Code to reproduce the error** The simplest code snippet that produces your bug. [CODE_BLOCK] **Error logs (if any)** \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 How many seconds would it take for a leopard at full speed to run through Pont des Arts? \u2502 \u2502 \u2502 \u2570\u2500 HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Error in generating model output: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: qV9GhI) Model too busy, unable to get response in less than 60 second(s) [Step 0: Duration 60.60 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Error in generating model output: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions (Request ID: rc6T0r) Model too busy, unable to get response in less than 60 second(s) [Step 1: Duration 60.16 seconds] **Expected behavior** Should work as in your viodeos **Packages version:** Run pip freeze | grep smolagents and paste it here. smolagents==1.8.0 **Additional context** Add any other context about the problem here. python --version Python 3.13.0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2162, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f38a335-e7af-4a48-9d87-45277435a34c", "embedding": null, "metadata": {"issue_id": 583, "title": "[BUG] invalid message format when using `OpenAIServerModel`", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "193364bf-bc0d-465e-b086-a54fa2e52dc0", "node_type": "4", "metadata": {"issue_id": 583, "title": "[BUG] invalid message format when using `OpenAIServerModel`", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "f5dfab851be3e6956e91a8b73546954d91362cff9e522fd35bf3e3914d87e135", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] invalid message format when using `OpenAIServerModel`\n\nDescription: **Describe the bug** The latest code runs into invalid message format when using OpenAIServerModel **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** main branch **Additional context** Pretty sure this is due to OpenAI SDK uses flat message format without images but nested message format with image. The current code always use nested message, which results in this bug. without image [CODE_BLOCK] with image [CODE_BLOCK]\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "50900d3a-a53a-43cf-af47-40859bc2ff9b", "embedding": null, "metadata": {"issue_id": 582, "title": "Agent will get stuck after a few iterations if used in a loop", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b11d6a3-6223-492f-83f0-cb4c810975c4", "node_type": "4", "metadata": {"issue_id": 582, "title": "Agent will get stuck after a few iterations if used in a loop", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "a857d515bc8212304032d15a5de5d83a572e877341307ee174962e834b6271b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Agent will get stuck after a few iterations if used in a loop\n\nDescription: Hello, I am trying to run an agent in a for loop, and after just a few iterations it gets stuck for no reason if I use TransformersModel with CodeAgent running on a GPU. I observe this behavior on Linux cloud and on my private Windows PC for **any version** of smolagents. **Minimal code to reproduce the error** [CODE_BLOCK]\" for i in range(50, 101): agent.run(prompt.format(i=i)) `` The LLama 1B model fits nicely on my 8GB GPU and there is a lot of free VRAM left. After a few failed steps of some iteration the agent will get stuck on response generation stage. On Windows PC I see that the GPU memory controller load increases dramatically with time while no output is generated, so perhaps this is a VRAM related issue. I can run the same LLM without smolagents using transformers directly in a simple loop and it successfully finishes all iterations. I can also run LiteLLMModel model with smolagents` that is connected to Ollama server on my PC, and it is running fine. Could you please take a look at it? **Packages versions:** transformers 4.47.0 smolagents 1.8.0 or main\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c500f006-ae3e-4f91-838e-d6eeb7c5552d", "embedding": null, "metadata": {"issue_id": 580, "title": "[BUG]", "state": "closed", "labels": ["invalid"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd2c5b10-d122-45bc-aad5-ce877a378e34", "node_type": "4", "metadata": {"issue_id": 580, "title": "[BUG]", "state": "closed", "labels": ["invalid"], "type": "issue"}, "hash": "b919b66ab93b3facf539fdc9274319c1de13dfcda8416ae4caa235378b6f1ed5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]\n\nDescription: The library does not work with requests in Russian. The code returns unicodeencoderror. Can you add a utf-8 support, please\n\nState: closed\n\nLabels: invalid\n\nCategories: category-invalid", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b98975e1-8c8d-4b9d-846a-8ae0bf68f47a", "embedding": null, "metadata": {"issue_id": 579, "title": "[BUG]ValueError: Tool validation failed:", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51da5dd7-32b5-4b98-b8fd-99077f8a467c", "node_type": "4", "metadata": {"issue_id": 579, "title": "[BUG]ValueError: Tool validation failed:", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "125ea4cf1fce119f357186d9151c57a9d6cb9e7c161d7111d006e950935975b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]ValueError: Tool validation failed:\n\nDescription: **Describe the bug** when try use_e2b_executor and DuckDuckGoSearchTool(), [CODE_BLOCK] **Code to reproduce the error** [CODE_BLOCK] this is in e2b_executor.py L72 [CODE_BLOCK] where [CODE_BLOCK] why validate_tool_attributes needs __init__ takes no argument, what consideration? DuckDuckGoSearchTool's __init__ has default arguments, probably we should skip arguments who has default value? [CODE_BLOCK] In addition errors will report [CODE_BLOCK] where result is defined in list comprehension. [CODE_BLOCK] I added the following code to fix the latter bug: [CODE_BLOCK] Then what's the consideration of validate __init__ like that? If understanded, then I can submit a PR?\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 816, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d8574e9-6247-4f8d-8c88-ff802c2942b2", "embedding": null, "metadata": {"issue_id": 577, "title": "[docs] Simple Syntax Error in Guided Tour", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f05d33b4-67c3-4399-b508-c21feb61df9d", "node_type": "4", "metadata": {"issue_id": 577, "title": "[docs] Simple Syntax Error in Guided Tour", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "2fd68e66d559fa6f53429c00dee47a202a6867e2a457b2113efada11c4509b77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [docs] Simple Syntax Error in Guided Tour\n\nDescription: **Describe the bug** There is a syntax error when trying to create an instance of the LiteLLMModel class because of a missing comma in the constructor parameters. The code as written does not have a comma after the api_key argument, which results in a runtime error. **Code to reproduce the error** https://github.com/huggingface/smolagents/blob/d74837b10a4e2bc51105cce9b81f21b64dde55ac/docs/source/en/guided_tour.md?plain=1L95-L100 **Error logs (if any)** SyntaxError: invalid syntax. Perhaps you forgot a comma? **Expected behavior** I expect the code to run without errors once the comma is added after the api_key argument. **Packages version:** smolagents==1.8.0 **Additional context** The error is caused by the lack of a comma between the api_key and num_ctx arguments in the constructor. This issue could be fixed by simply adding the missing comma.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1001, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23dc481c-1b41-46b2-985b-a8dd9e05eb92", "embedding": null, "metadata": {"issue_id": 575, "title": "[BUG] MultiStepAgent planning_step first step prompts just contains system prompt", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "03d31969-89ab-4816-b4ff-f396577a9986", "node_type": "4", "metadata": {"issue_id": 575, "title": "[BUG] MultiStepAgent planning_step first step prompts just contains system prompt", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "332621ac0bba9af2bd005e3eb26a6112984ed911a1897badc7c6078aa91ead6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] MultiStepAgent planning_step first step prompts just contains system prompt\n\nDescription: **Describe the bug** A clear and concise description of what the bug is. [CODE_BLOCK] For some models, It will comes to error like A conversation must start with a user message. Try again with a conversation that starts with a user message. **Code to reproduce the error** Just use some models like deepseek-v3 **Expected behavior** When planning the steps, put the question on the user prompt. For some model like o1 or deepseek-r1, just use user_prompt. **Packages version:** Just use the main branch, I run open_deep_research\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ef315389-f9a8-4939-8599-e28b2330600d", "embedding": null, "metadata": {"issue_id": 574, "title": "Allow for introspection and type hints over class attributes when defining tool classes", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "12951fe0-4516-4efa-95d0-019b256d5bdb", "node_type": "4", "metadata": {"issue_id": 574, "title": "Allow for introspection and type hints over class attributes when defining tool classes", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "7a5945b54a6ca987eefb2dffc693d5bb18d49f124b493749d38dc442cb215672", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Allow for introspection and type hints over class attributes when defining tool classes\n\nDescription: **Is your feature request related to a problem? Please describe.** I love smolagents, but defining my own tools as classes feels clunky Currently when using classes over functions, class definitions are not very idiomatic python, e.g. [CODE_BLOCK] all of the 4 required class attributes could instead be introspected from docstrings and type hints (as they are when using tool function decorators). Additionally, the example is confusing, because it looks like there is some introspection of the forward happening method: https://github.com/huggingface/smolagents/blob/d74837b10a4e2bc51105cce9b81f21b64dde55ac/src/smolagents/tools.pyL149-L157 but it's not really clear how that relates to the class attributes, if one takes precedence over the other, particularly the (less expressive) type in output_type. It also looks like it's possible to have contradictory type hints and class attributes, and it's not clear what the overall effect is on prompt generation and result interpretation. **Describe the solution you'd like** More idiomatic python would look like: [CODE_BLOCK] I could try a PR for this but I don't know where this fits on your roadmap **Is this not possible with the current options.** As an interim step it would be good to have more docs here that explain best practice: https://huggingface.co/docs/smolagents/tutorials/tools In particular this isn't clear: > An output_type attribute, which specifies the output type. The types for both inputs and output_type should be Pydantic formats, they can be either of these: ~AUTHORIZED_TYPES(). The pydantic docs are for generating json schema, and it looks like the ~AUTHORIZED_TYPES() is meant to auto-expand. **Describe alternatives you've considered** An alternative would be to encourage use of decorated functions, but to add a context argument, similar to pydantic-ai, which would bypass the current limitations of this (simpler) approach https://ai.pydantic.dev/tools-dependency-injection-example\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "32e97546-8415-40ab-b6f8-478db35ec3d0", "embedding": null, "metadata": {"issue_id": 572, "title": "Exception during agent creation on Windows", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f19fe663-b550-4a0a-a5ae-ab5f3630e120", "node_type": "4", "metadata": {"issue_id": 572, "title": "Exception during agent creation on Windows", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "152c2ff0cb3863a6e9c4b360f038ab7b6f8a5b7451772bdc928f5462144881be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Exception during agent creation on Windows\n\nDescription: Hello, I must confess that I use Anaconda on Windows PC to run smolagents, and version 1.7.0 was running fine, but I cannot run **version 1.8.0 and higher** because the following line crashes the agent creation: https://github.com/huggingface/smolagents/blob/d74837b10a4e2bc51105cce9b81f21b64dde55ac/src/smolagents/agents.pyL651 I can confirm that I can run these versions on Linux environments, but on Windows it throws the following exception: [CODE_BLOCK] My version of python on Windows is 3.12.3 So, somehow, prompts is a resource on Linux and not a resource on Windows. As a private workaround I created __init__.py file in the prompts directory and it worked. Could you please tell me where this should be fixed? If this is a problem with importlib, then I would like to report it, because I need GitHub points really badly...\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 978, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2963a698-5c7e-4def-90a6-370f145f2198", "embedding": null, "metadata": {"issue_id": 570, "title": "[BUG] helium vision_web_browser.py NoneType error after saving image", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bf71d72-3e2d-4d7d-8e73-39ca75615f8e", "node_type": "4", "metadata": {"issue_id": 570, "title": "[BUG] helium vision_web_browser.py NoneType error after saving image", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "22dd8a2f67840c3eb26e5c85e91949a675d85eae5f75b989ae57cee3181fb438", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] helium vision_web_browser.py NoneType error after saving image\n\nDescription: **Describe the bug** When running vision_web_browser.py on Windows 11 with helium. with the following model : [CODE_BLOCK] I get the error below. **Code to reproduce the error** src/smolagents/vision_web_browser.py with the above model on Windows 11 with helium. **Error logs (if any)** [CODE_BLOCK] **Expected behavior** No error **Packages version:** [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 536, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f20e9fd-974c-4b23-af89-10e8b3d014b5", "embedding": null, "metadata": {"issue_id": 566, "title": "[BUG] GradioUI\u2019s uploadfile Function Incorrectly Modifies File Extensions", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "284fcf93-569d-44d5-881d-c6d44b0186ed", "node_type": "4", "metadata": {"issue_id": 566, "title": "[BUG] GradioUI\u2019s uploadfile Function Incorrectly Modifies File Extensions", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "bb6d05895976902108ec461730388f452e863ada564ceca6547758927d76ddd2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] GradioUI\u2019s uploadfile Function Incorrectly Modifies File Extensions\n\nDescription: Due to a logical error in the upload_file function of GradioUI, the suffix of the uploaded filename was incorrectly modified. I have already submitted a pull request 342 addressing this issue a long time ago. Please take a look. !Image\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 409, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dea2c539-0345-4240-a9e7-444436f2b27e", "embedding": null, "metadata": {"issue_id": 565, "title": "Couple of suggestions in deep research", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe16f91b-48a9-432b-a6db-74249823f2f0", "node_type": "4", "metadata": {"issue_id": 565, "title": "Couple of suggestions in deep research", "state": "open", "labels": [], "type": "issue"}, "hash": "def9fb85d7645a80fb24230b83aa1448e17f0160d2f3367e80ee7f4723d89cfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Couple of suggestions in deep research\n\nDescription: Hi Team , Couple of suggestions 1. Can we pls add option of other open source visual model as well . Currently , GPT-4o is hardcoded in visual_qa.py 2. Can we also add open source web search like duck duck go as serpent and all needs API key and it has free tier of 100 searches per month only.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d47acae-4859-44e2-aa6a-d834cbf6b565", "embedding": null, "metadata": {"issue_id": 560, "title": "[BUG] Example in documentation throws ImportError", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66646443-a60f-4cf7-a09a-5503ccd85f4c", "node_type": "4", "metadata": {"issue_id": 560, "title": "[BUG] Example in documentation throws ImportError", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f01184b16c551761745c6937b35c8e5674af78113ee4a6f3d2b38f5a11674e54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Example in documentation throws ImportError\n\nDescription: **Describe the bug** A clear and concise description of what the bug is. With the release of version 1.8.0, the documentation is no longer up to date. **Code to reproduce the error** The simplest code snippet that produces your bug. Execute the below script as shown in the doc [CODE_BLOCK] **Error logs (if any)** Cf picture !Image **Expected behavior** A clear and concise description of what you expected to happen. Expecting the scrip to run and to get an answer **Packages version:** Run pip freeze | grep smolagents and paste it here. **Additional context** Add any other context about the problem here. It's coming from the latest release (1.8.0), the ManagedAgent class might have been deprecated/removed too quickly. !Image !Image\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 891, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da28682f-8c23-49d7-ac39-ae8562a9d6b6", "embedding": null, "metadata": {"issue_id": 559, "title": "Getting Token Usage as part of the Final Answer", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2767fb1e-a580-427d-99b8-60c3be2a931e", "node_type": "4", "metadata": {"issue_id": 559, "title": "Getting Token Usage as part of the Final Answer", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "22eee6b00cd6b867459439412cc65323e464ad927ec7d4797023d4f60469b544", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Getting Token Usage as part of the Final Answer\n\nDescription: **Is your feature request related to a problem? Please describe.** No **Describe the solution you'd like** For cost/usage tracking it would be nice to be able to get the used input and output token count or even actual API usage expense. **Is this not possible with the current options.** It appears it may be possible to hook into the Monitor to get the token usage data or wrap it into the agent's run. **Describe alternatives you've considered** I looked into modifying the MultStepAgent.run function to bring in the token counts from the monitor. [CODE_BLOCK] **Additional context** Being able to get usage/cost data from API calls would help from a business perspective to track cost of execution and aid in iterating on agents to lower costs.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 925, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d391139c-2712-4c9f-b353-531401026358", "embedding": null, "metadata": {"issue_id": 555, "title": "Code execution failed at line 'from sklearn.linear_model import LinearRegression' due to: AttributeError:module 'scipy.sparse._coo' has no attribute 'upcast'", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfe35d3c-6c7e-422b-9dc4-3694bee9d51f", "node_type": "4", "metadata": {"issue_id": 555, "title": "Code execution failed at line 'from sklearn.linear_model import LinearRegression' due to: AttributeError:module 'scipy.sparse._coo' has no attribute 'upcast'", "state": "closed", "labels": [], "type": "issue"}, "hash": "8a1f9017186b44d531ce5855e84d2f2e9cb61eda55386e5f9dc7830b5686f260", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Code execution failed at line 'from sklearn.linear_model import LinearRegression' due to: AttributeError:module 'scipy.sparse._coo' has no attribute 'upcast'\n\nDescription: I run following codes in local PC as well as colab. Code execution error as indicated by the title occured when smolagents try to run the python codes suggested by deepseek comprised \"from sklearn.linear_model import LinearRegression\". Similar error happens in google colab as well. Please advice. [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe7500e6-ea1c-42df-9748-470b0be93eb0", "embedding": null, "metadata": {"issue_id": 554, "title": "[BUG] AzureOpenAIServerModel Sends Unsupported 'stop' Parameter for o1-mini", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7f100a8-be80-4f7d-8349-713f890a1c86", "node_type": "4", "metadata": {"issue_id": 554, "title": "[BUG] AzureOpenAIServerModel Sends Unsupported 'stop' Parameter for o1-mini", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "b36f19ed5c28aa65105cf6bff29b2f1216c7533dde02d45a39349699bad2c004", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] AzureOpenAIServerModel Sends Unsupported 'stop' Parameter for o1-mini\n\nDescription: **Describe the bug** When using the AzureOpenAIServerModel with the o1-mini deployment, the request being sent includes a \"stop\" parameter, which is not supported by the model. This results in a 400 error with the message: \u201cUnsupported parameter: 'stop' is not supported with this model.\u201d **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** The request to Azure OpenAI should succeed without including a \"stop\" parameter when using the o1-mini model. The API call should complete successfully and return a valid response without triggering a 400 error. **Packages version:** smolagents==1.8.0 **Additional context** The error appears to be caused by the internal method _prepare_completion_kwargs in the class AzureOpenAIServerModel class, which adds a \"stop\" parameter to the API request. A possible workaround is to override this method to remove the \"stop\" parameter if present. For example: [CODE_BLOCK]\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1135, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4986f890-8942-4730-a2ff-2fcadd70d86d", "embedding": null, "metadata": {"issue_id": 551, "title": "LiteLLM ollama bugs Update", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e5423e3-cf42-45bd-8a0a-0df956d3556f", "node_type": "4", "metadata": {"issue_id": 551, "title": "LiteLLM ollama bugs Update", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "93b2f9a32f928c63bdce007974b8c186df6f953120c3e9ebf68c5305c09a5dda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73a15992-c064-438d-a6cd-52736437c36f", "node_type": "1", "metadata": {}, "hash": "fc157d160812e218565ff0adaec46c0ad3357b63a4a98a503d3a644fa511975e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: LiteLLM ollama bugs Update\n\nDescription: Hi @merveenoyan as requested in 406 here is the current status with ollama along with code to reproduce. TL;DR: If people have trouble using ollama, pls try ollama/modelname instead of ollama_chat/modelname (yes, LiteLLM recommends otherwise) and prefer CodeAgent over ToolCallingAgent (which you should do anyway if possible) - there is different behavior for ollama_chat/model and ollama/model - there is different behaviour of models rated either as \"tool-compatible\" by ollama or \"not tool-compatible\", and in 1.7.0 only the later worked \ud83d\ude48 - in 1.8.0 ollama/model now works for all models I tested and for both ToolCallingAgent and CodeAgent; vision does not work since we hardcorded flatten_images_as_text=True in 406 . It would work in theory, but since currently only Llava is supported for ollama in LiteLLM not a big issue imo - in 1.8.0 ollama_chat works for the CodeAgent with all models, but the ToolCallingAgent only works for models marked as \"tool-compatible\" and seems rather unstable compared to ollama/model Details: **v1.8.0** ollama_chat/llama3.3 (tool-compatible): - ToolCallingAgent: \u2705 (thought in all my runs it throws an error in the first 2-3 steps before adapting something in it's answer: Error in generating tool call with model: 'NoneType' object is not iterable. In Step3 or 4 and afterwards it gets it right ollama_chat/phi4:latest (also deepseek-r1:70b both *NOT* classified as tools-compatible by ollama): - ToolCallingAgent: :x: always throws Error in generating tool call with model: 'NoneType' object is not iterable. After max_steps it gives a final answer like \"The current weather in Paris cannot be provided due to technical difficulties accessing real-time data.\" - CodeAgent: \u2705 ollama/llama3.3 AND phi4:latest & deepseek-r1:70b both *NOT* classified as tools-compatible by ollama): - ToolCallingAgent: \u2705 (though deepseek-r1:70b really has trouble with this, often needs multiple steps to get the tool call right, phi4 and llama3.3 no problem) - CodeAgent: \u2705 ollama/llava & ollama_chat/llava - Vision+CodeAgent: :x: Error in generating model output: Cannot use images with flatten_messages_as_text=True (to be expected due to the hack in 406) **v1.7.0** ollama_chat/all_models: - ToolCallingAgent: :x: Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '400 Bad Request' for url 'http://localhost:11434/api/chat' - CodeAgent: :x: Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '400 Bad Request' for url 'http://localhost:11434/api/chat' - Vision: :x: Error in generating model output: litellm.APIConnectionError: Ollama_chatException - Client error '400 Bad Request' for url 'http://localhost:11434/api/chat' ollama/phi4 (or deepseek-r1:70b both *NOT* classified as tools-compatible by ollama): - ToolCallingAgent: \u2705 \ud83d\ude43 - CodeAgent: \u2705 ollama/llama3.3 which is compatible for tool calling according to ollama's list of tool-compatible models - ToolCallingAgent: :x: Error in generating tool call with model: litellm.APIConnectionError: 'arguments' Traceback (most recent call last): File \"/Users/roland/venvs/sa_1_7/lib/python3.12/site-packages/litellm/main.py\", line 2808, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/roland/venvs/sa_1_7/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73a15992-c064-438d-a6cd-52736437c36f", "embedding": null, "metadata": {"issue_id": 551, "title": "LiteLLM ollama bugs Update", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e5423e3-cf42-45bd-8a0a-0df956d3556f", "node_type": "4", "metadata": {"issue_id": 551, "title": "LiteLLM ollama bugs Update", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "93b2f9a32f928c63bdce007974b8c186df6f953120c3e9ebf68c5305c09a5dda", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4986f890-8942-4730-a2ff-2fcadd70d86d", "node_type": "1", "metadata": {"issue_id": 551, "title": "LiteLLM ollama bugs Update", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "c14e8864caf68813c2bc346d7e8ca3a1eb9b22002b50e797348abf5ded56e615", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "py\", line 370, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/roland/venvs/sa_1_7/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformati on.py\", line 264, in transform_response \"arguments\": json.dumps(function_call[\"arguments\"]), ~~~~~~~~~~~~~^^^^^^^^^^^^^ KeyError: 'arguments' - CodeAgent: \u2705 ollama/llava - Vision+CodeAgent: \u2705 (!) There is other bugs in the example code which caused it to not work for me earlier, but after fixing those, in 1.7.0. it actually works. Though LiteLLM currently only supports Llava with ollama, which is a very weak model and doesn't really get stuff done, but there are PRs pending to support better models **Code to reproduce the error** Python3.12.9 smolagents v1.7.0 / v1.8.0 fresh .venv's Test for ToolCallingAgent and CodeAgent: examples/agent_from_any_llm.py with setting chosen_inference = \"ollama\" and model_id=\"ollama_chat/modelname\" or model_id=\"ollama/modelname\" respectively Vision: [CODE_BLOCK] Example code from the blogpost (There are some other bugs in this example code, will fix those separately, mainly a missing driver = helium.get_driver() in a couple of places and a missing hint to start the browser first in the helium prompt for weaker models) @aymeric-roucher , @sysradium fyi\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 3474, "end_char_idx": 4868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e366d16-8318-4c77-b202-c2514d1274ab", "embedding": null, "metadata": {"issue_id": 546, "title": "[BUG] Standard function `pow` cannot be executed by an agent", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e61b09f-2b5b-4318-9c43-3d52b2878383", "node_type": "4", "metadata": {"issue_id": 546, "title": "[BUG] Standard function `pow` cannot be executed by an agent", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "e64474955aa17a0659b94f54581cae2192101c985c6337106acd25b4ea695504", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Standard function `pow` cannot be executed by an agent\n\nDescription: Hello, my steps are failing when the agent tries to invoke the standard pow function with the third argument **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** A clear and concise description of what you expected to happen. **Packages version:** version 0.1.7 and main Perhaps there are other standard functions that are affected by this error, a unit test would be nice\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f1eae12-c44c-4811-b140-6526de6d3908", "embedding": null, "metadata": {"issue_id": 532, "title": "[BUG]  custom_role_conversions does not work with litellm models", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b893ad7-6f45-48a4-af3e-8df3ca477e18", "node_type": "4", "metadata": {"issue_id": 532, "title": "[BUG]  custom_role_conversions does not work with litellm models", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "9c1744b16bc12c93e1407d8c2d0630141d6185f89fa7115af29131eb65a03766", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]  custom_role_conversions does not work with litellm models\n\nDescription: **Describe the bug** custom_role_conversions does not work with litellm models **Code to reproduce the error** Investigated models.py - LiteLLMModel class does not have the custom_role_conversions argument when initialising.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cced98d9-a5c4-4f00-aef3-ad856f78a7c8", "embedding": null, "metadata": {"issue_id": 531, "title": "Full system prompt saved to memory each time", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25c4390a-6708-4780-ac99-316ecacd87cf", "node_type": "4", "metadata": {"issue_id": 531, "title": "Full system prompt saved to memory each time", "state": "closed", "labels": [], "type": "issue"}, "hash": "9a44908e9ce19b156c9c0556f06f1df86dcf89c84fe3465d9e8914a4ea0fc869", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Full system prompt saved to memory each time\n\nDescription: I noticed that every message I add to the conversation increases the token input by about 2,000 tokens. I checked the memory and saw that the full system prompt is being saved with each message. Is that necessary? It seems like a waste of tokens.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 327, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf74bada-6d37-4045-a4ea-6ee07d3b698c", "embedding": null, "metadata": {"issue_id": 524, "title": "[BUG] SmolAgents CodeAgent Truncates Input Before Tool Execution", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ceb87884-a32d-43dc-a5d7-3b909499b59d", "node_type": "4", "metadata": {"issue_id": 524, "title": "[BUG] SmolAgents CodeAgent Truncates Input Before Tool Execution", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "2b75a07e8179076bf44dc6866fe6651f1657a8c377b3d92954ca50dd9e0bcbab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] SmolAgents CodeAgent Truncates Input Before Tool Execution\n\nDescription: Summary SmolAgents' CodeAgent incorrectly processes input before invoking tools, leading to truncation of long text even when a tool is explicitly designed to handle it. This defeats the purpose of tools and prevents reliable execution in workflows requiring large text processing. Steps to Reproduce Define a Tool (e.g., SummarizationTool) that accepts long text and chunks it internally. Register the Tool with CodeAgent. Send a large text input via agent.run(prompt). Observe the Issue: The input never reaches the tool in full. Instead, CodeAgent processes it first, truncating it to its own model\u2019s context length. The remaining truncated input is then passed to the tool, defeating the purpose of handling long text. Expected Behavior CodeAgent should immediately pass the input to SummarizationTool, without truncating it or attempting to reason about it first. Observed Behavior CodeAgent processes the input first, leading to truncation. Even if the tool is the only valid choice for execution, the agent still tries to analyze the text before passing it. The tool only receives partially processed, truncated input instead of the full text. Proposed Fixes Honor System Prompt Rules If the system prompt explicitly states: \"For any summarization task, immediately use the summarization_tool without processing the input first.\" The agent must respect this directive. Provide a Direct Tool Execution Mode If the user specifies a tool explicitly in the request: { \"tool\": \"summarization_tool\", \"args\": { \"text\": \"Very long text...\" } } The agent should not attempt intermediate processing. Modify CodeAgent to Respect Token Limits at the Tool Level If an input exceeds the agent\u2019s context limit, the agent should pass it to the tool immediately instead of trying to process it first. Workaround (Not Ideal) Users currently have to bypass SmolAgents completely and call the tool manually: summary = SummarizationTool().forward(long_text) This makes SmolAgents useless for tool orchestration in cases requiring large text handling. Impact Critical usability issue for any tool requiring large input (summarization, document parsing, data extraction, etc.). Prevents SmolAgents from handling real-world workloads where LLM context limits are a known constraint. Reproducible Code from smolagents import CodeAgent, LiteLLMModel from summarization_tool import SummarizationTool Custom tool agent = CodeAgent( tools=[SummarizationTool()], model=LiteLLMModel(model_id=\"openai/gpt-4o\"), system_prompt=\"Always use summarization_tool for large text. Do not process text yourself.\" ) long_text = \"...\" Large document exceeding LLM context limit response = agent.run(f\"Summarize this:\\n{long_text}\") print(response) \ud83d\udd34 Receives truncated text, tool never sees full input Priority: HIGH This blocks any real-world usage of tools that are supposed to handle large input. If SmolAgents cannot be trusted to delegate correctly, it cannot be used in production. Request \ud83d\ude80 Please fix this by ensuring CodeAgent does not process input that is meant for a tool before calling the tool. I've include a GIST of the code here: https://gist.github.com/grahama1970/12e4051997f8988bfdd56796436a6cae Much appreciation in advance\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "111f1bfa-6f7a-4f2c-ad04-15460fda17fd", "embedding": null, "metadata": {"issue_id": 522, "title": "[BUG] open_deep_research issue related to TypeError: MultiStepAgent.__init__() got an unexpected keyword argument 'name'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8661f70-fd91-4d49-8290-4ff90b8f61a6", "node_type": "4", "metadata": {"issue_id": 522, "title": "[BUG] open_deep_research issue related to TypeError: MultiStepAgent.__init__() got an unexpected keyword argument 'name'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "a73a840fee2f4ccf902f75c8750a2054cee16e04e5c298da3e74b133ad9b5a8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] open_deep_research issue related to TypeError: MultiStepAgent.__init__() got an unexpected keyword argument 'name'\n\nDescription: I was attempting to run the example for open_deep_research, but when I try running it, I bumped into the error: TypeError: MultiStepAgent.__init__() got an unexpected keyword argument 'name' You can refer to the example code provided above, but for reference it looks like the below: [CODE_BLOCK] When I look into MultiStepAgent, which looks like this: [CODE_BLOCK] name, description, provide_run_summary and managed_agent_prompt are not existing in MultiStepAgent. I am not sure if I am doing the right thing here, but does it mean we are passing the arguments that do not exist in MultiStepAgent the first place? The agents are correctly defined in the jupyter notebook - but that notebook seems to be having some other issue too, see 501\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0470a02c-2dad-4e7b-8c42-21c59c4c1688", "embedding": null, "metadata": {"issue_id": 521, "title": "authenticated sessions with smolagents (how to be logged in during browser use)", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d51b9960-f060-45c2-81cf-19e5a2ae1aa0", "node_type": "4", "metadata": {"issue_id": 521, "title": "authenticated sessions with smolagents (how to be logged in during browser use)", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "7a8e31f9740ada0a0deac6a26a78d8e358f73f9d03e8287fb3a2c503f24d3eeb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: authenticated sessions with smolagents (how to be logged in during browser use)\n\nDescription: **Is your feature request related to a problem? Please describe.** I would like smolagents to be able to use websites with my login credentials. **Describe the solution you'd like** Either a way to give Helium credentials, or a way to use my actual browser, like: https://github.com/browser-use/browser-use/blob/main/examples/browser/real_browser.py **Is this not possible with the current options.** I'm fairly certain this is not possible with the current implementation. (If it is, can you make a demo code?) **Describe alternatives you've considered** I can use https://github.com/browser-use/browser-use/ instead **Additional context** https://github.com/browser-use/browser-use/ does a really good job of providing multiple options for this.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 954, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2bc19a8d-bfbc-4eaf-829a-3afc6681143e", "embedding": null, "metadata": {"issue_id": 509, "title": "Utilizing Open WebUI API", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30feb971-8a9b-4ea6-99a6-107c05d275a2", "node_type": "4", "metadata": {"issue_id": 509, "title": "Utilizing Open WebUI API", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f41571ebc2f1e4ce6d536d5a970882c65548c726ae0ca61fe35c393d21ba2f3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51ac3db6-29fb-44d2-97a2-09cea7570855", "node_type": "1", "metadata": {}, "hash": "b5e39cb4dff1b63b232a48bcc397e97a1f0d5e20a42e2821e0906ef163590f20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Utilizing Open WebUI API\n\nDescription: **Describe the bug** I'm using OpenWebUI (because it's awesome) to hold some data to be utilized in the model's response in a RAG-like way. (For example, I can attach a PDF about dinosaurs to my New Model, based on Llama 3.3, and then it reads and utilizes that information in it's responses). Open WebUI adds some capability beyond what VLLM pulls out of the LLM. It's a sort of pre-agent step. But I just learned that OpenWebUI had an API endpoint that seems to be OpenAI compatible. So, In OpenWebui, I click on the workspace. Under the \"Models\" tab, I have a model based on LLama3.3 that I've given some knowledge to add context and stuff. Now I want to use that local model in my agentic responses! **Code to reproduce the error** from smolagents import CodeAgent, OpenAIServerModel, GradioUI model = OpenAIServerModel( model_id=\"id-of-my-model-on-open-webui\",\\ api_base=\"http://192.168.XX.YY:PPPP/api/v1/\",\\ api_key=\"abcdefghijklmnopqrstuvwxyz1234567890\" ) agent = CodeAgent(tools=[DuckDuckGoSearchTool(), ClockTool(), CalcTool()], additional_authorized_imports=[\"requests\"], model=model) gr = GradioUI(agent) gr.launch(ssl_verify=False, server_name=\"0.0.0.0\") **Error logs (if any)** Here is what the GradioUI outputs: Step 1 Thought: I need to create a Python script that lists all prime numbers from 2 to 300. To achieve this, I will use a loop to iterate through the numbers in the given range and check if each number is prime.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51ac3db6-29fb-44d2-97a2-09cea7570855", "embedding": null, "metadata": {"issue_id": 509, "title": "Utilizing Open WebUI API", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30feb971-8a9b-4ea6-99a6-107c05d275a2", "node_type": "4", "metadata": {"issue_id": 509, "title": "Utilizing Open WebUI API", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f41571ebc2f1e4ce6d536d5a970882c65548c726ae0ca61fe35c393d21ba2f3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bc19a8d-bfbc-4eaf-829a-3afc6681143e", "node_type": "1", "metadata": {"issue_id": 509, "title": "Utilizing Open WebUI API", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "6354987ce8939329a6d2bd8c0cf0bbc53d7f9ddd4533132f6ee8af3e08ddfd65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "025aab3b-0c3c-48f3-82a5-94c7b5650f33", "node_type": "1", "metadata": {}, "hash": "0711fa2fbd82b3b92842f79b3d31663b93527ae3173b0e24482accedce34b58f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Code to reproduce the error** from smolagents import CodeAgent, OpenAIServerModel, GradioUI model = OpenAIServerModel( model_id=\"id-of-my-model-on-open-webui\",\\ api_base=\"http://192.168.XX.YY:PPPP/api/v1/\",\\ api_key=\"abcdefghijklmnopqrstuvwxyz1234567890\" ) agent = CodeAgent(tools=[DuckDuckGoSearchTool(), ClockTool(), CalcTool()], additional_authorized_imports=[\"requests\"], model=model) gr = GradioUI(agent) gr.launch(ssl_verify=False, server_name=\"0.0.0.0\") **Error logs (if any)** Here is what the GradioUI outputs: Step 1 Thought: I need to create a Python script that lists all prime numbers from 2 to 300. To achieve this, I will use a loop to iterate through the numbers in the given range and check if each number is prime. Code: def is_prime(n): if n <= 1: return False if n == 2: return True if n % 2 == 0: return False max_divisor = int(n**0.5) + 1 for d in range(3, max_divisor, 2): if n % d == 0: return False return True prime_numbers = [n for n in range(2, 301) if is_prime(n)] print(prime_numbers) \ud83d\udee0\ufe0f Used tool python_interpreter def is_prime(n): if n <= 1: return False if n == 2: return True if n % 2 == 0: return False max_divisor = int(n**0.5) + 1 for d in range(3, max_divisor, 2): if n % d == 0: return False return True prime_numbers = [n for n in range(2, 301) if is_prime(n)] print(prime_numbers) \ud83d\udcdd Execution Logs [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293] Last output from code snippet: None Step 1 | Input-tokens:2,431 | Output-tokens:172 | Duration: 18.34 Step 2 \ud83d\udca5 Error Error in generating model output: Error code: 500 - {'detail': \"expected string or bytes-like object, got 'list'\"} Step 2 | Input-tokens:2,431 | Output-tokens:172 | Duration: 7.69 Step 3 \ud83d\udca5 Error Error in generating model output: Error code: 500 - {'detail': \"expected string or bytes-like object, got 'list'\"} Step 3 | Input-tokens:2,431 | Output-tokens:172 | Duration: 7.15 Step 4 \ud83d\udca5 Error Error in generating model output: Error code: 500 - {'detail': \"expected string or bytes-like object, got 'list'\"} Step 4 | Input-tokens:2,431 | Output-tokens:172 | Duration: 2.24 Step 5 \ud83d\udca5 Error Error in generating model output: Error code: 500 - {'detail': \"expected string or bytes-like object, got 'list'\"} Step 5 | Input-tokens:2,431 | Output-tokens:172 | Duration: 6.96 Step 6 \ud83d\udca5 Error Error in generating model output: Error code: 500 - {'detail': \"expected string or bytes-like object, got 'list'\"} Step 6 | Input-tokens:2,431 | Output-tokens:172 | Duration: 1.55 Step 7 \ud83d\udca5 Error Reached max steps.", "mimetype": "text/plain", "start_char_idx": 750, "end_char_idx": 3484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "025aab3b-0c3c-48f3-82a5-94c7b5650f33", "embedding": null, "metadata": {"issue_id": 509, "title": "Utilizing Open WebUI API", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30feb971-8a9b-4ea6-99a6-107c05d275a2", "node_type": "4", "metadata": {"issue_id": 509, "title": "Utilizing Open WebUI API", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f41571ebc2f1e4ce6d536d5a970882c65548c726ae0ca61fe35c393d21ba2f3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51ac3db6-29fb-44d2-97a2-09cea7570855", "node_type": "1", "metadata": {"issue_id": 509, "title": "Utilizing Open WebUI API", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "fd9c8b1353a0be26e510e2374424ea76261b857bca79a7188a1160a46792a8e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Step 7 | Input-tokens:2,431 | Output-tokens:172 | Duration: 1.55 Final answer: Error in generating final LLM output: Error code: 500 - {'detail': \"expected string or bytes-like object, got 'list'\"} **Expected behavior** I'm unclear as to why, after step 1, it starts outputting errors. It seems like maybe it's unable to use the outputs **Packages version:** >> pip freeze | grep smolagents smolagents==1.7.0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 3485, "end_char_idx": 3974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4d5f146-57d5-484f-8857-eee645d86b95", "embedding": null, "metadata": {"issue_id": 508, "title": "[BUG]Having `\"*\"` in `authorized_imports` does not actually allow you to use any import", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0fa5d5c-0cce-432f-8c8c-10011eae57ea", "node_type": "4", "metadata": {"issue_id": 508, "title": "[BUG]Having `\"*\"` in `authorized_imports` does not actually allow you to use any import", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "4301a637ab6aa9daf2da2c13edf72a0fda21db18808649009967f24658fb2d7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG]Having `\"*\"` in `authorized_imports` does not actually allow you to use any import\n\nDescription: **Describe the bug** Having \"*\" in authorized_imports does not matter in get_safe_module **Code to reproduce the error** The simplest code snippet that produces your bug. [CODE_BLOCK] **Error logs (if any)** !Image **Expected behavior** I should be able to run the example when using \"*\" **Packages version:** Run pip freeze | grep smolagents and paste it here. 1.8.0 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae9911fe-6bac-4a56-95e8-95da54153f95", "embedding": null, "metadata": {"issue_id": 503, "title": "[BUG] CI test results are obscured with terminal logging messages", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c733223d-4d82-4ae7-8f76-bbac3813a56e", "node_type": "4", "metadata": {"issue_id": 503, "title": "[BUG] CI test results are obscured with terminal logging messages", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "0a63b1301e462d4457f1ecabc3acaf4bd80e96b3110dba4d1314d9d8a59a78ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] CI test results are obscured with terminal logging messages\n\nDescription: **Describe the bug** CI test results are difficult to read because they are filled withe terminal logging messages. For example: [CODE_BLOCK] <MagicMock name='mock().content' id='139668781543248'> [CODE_BLOCK]` instead of: [CODE_BLOCK] **Code to reproduce the error** See Agent tests: https://github.com/huggingface/smolagents/actions/runs/13153127424/job/36704204259 **Expected behavior** No terminal logging messages in CI tests.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 599, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d91d96b-ab83-4060-bf2b-24b2b844472c", "embedding": null, "metadata": {"issue_id": 501, "title": "How to run open_deep_research\uff1f", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1dcf9aa7-dc7d-4453-aa3a-e9b6371f1c59", "node_type": "4", "metadata": {"issue_id": 501, "title": "How to run open_deep_research\uff1f", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "f1f1feaa5a0bbc7d1b3dfd4772cf40f52b892c646a0d92d33600a5c6b2418647", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to run open_deep_research\uff1f\n\nDescription: How to run open_deep_research\uff1f\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b0c8601-bac7-4a7b-98b4-1939afead726", "embedding": null, "metadata": {"issue_id": 500, "title": "Different agent frameworks integration", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61ccb7fc-2c77-40cc-bba9-d338ce48f891", "node_type": "4", "metadata": {"issue_id": 500, "title": "Different agent frameworks integration", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "d6214726ff7b3e961f720aee59c91c25619c7f28968ba0c6dd2d8b8286eabd6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Different agent frameworks integration\n\nDescription: **Is your feature request related to a problem? Please describe.** > All agents in smolagents are based on singular MultiStepAgent class, which is an abstraction of ReAct framework. Since **ReAct** is our go-to framework for agents, is there any possibility to make the codebase more flexible by introduction adapters of different frameworks like **SayCan, Reflexion, CLIN**, etc? **Describe the solution you'd like** It would be great if smoleagents can provide the ability to switch the framework instead of relying solely upon ReAct. I understand it would take huge effort to make it framework agnostic. Just want to put it here for discussion and consideration. **Additional context** Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization ScienceWorld: Is your Agent Smarter than a 5th Grader?\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1262, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28e6c567-cc1c-4f24-977a-f6295522e68e", "embedding": null, "metadata": {"issue_id": 493, "title": "[BUG] Unable to execute function defined in the same Python interpreter.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c88c7793-c670-4eb0-8acc-937938463a1c", "node_type": "4", "metadata": {"issue_id": 493, "title": "[BUG] Unable to execute function defined in the same Python interpreter.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "0e47df36863d9308c5370ce4ac7bc9f328e0d695bda000887e4efb41e64b0e2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Unable to execute function defined in the same Python interpreter.\n\nDescription: **Describe the bug** Unable to execute function defined in the same Python interpreter. See example below: **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** The function is local to the CodeAgent environment, I expect it to have access to the local functions. **Packages version:** Run pip freeze | grep smolagents and paste it here. smolagents==1.7.0 **Additional context** Add any other context about the problem here.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57fc1444-3ca0-4118-8b09-aa105eae966e", "embedding": null, "metadata": {"issue_id": 489, "title": "GradioUI.launch() hardcodes Shared = True and downloads a malicious reverse-proxy flagged by Corp ITRisk", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "045cbc85-271d-44e7-a11b-22846329658a", "node_type": "4", "metadata": {"issue_id": 489, "title": "GradioUI.launch() hardcodes Shared = True and downloads a malicious reverse-proxy flagged by Corp ITRisk", "state": "closed", "labels": [], "type": "issue"}, "hash": "810f8d9cb8d7bb702340686468026ce785d72e2cc29e2c2ce009bd5545bf8aea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: GradioUI.launch() hardcodes Shared = True and downloads a malicious reverse-proxy flagged by Corp ITRisk\n\nDescription: Hi In GradioUI.launch() demo.launch(debug=True, **share=True**, **kwargs) share=True will trigger a download of a malicious file that is gets alarm bells ringing in Corp IT departments as it is on 30+ threat databases that all these departments will be using Many people are reporting this: https://github.com/gradio-app/gradio/issues/3230 The resolution suggested is to ensure share=False, but obviously GradioUI hardcoding this makes this hard without resorting to a monkeypatch. Might I suggest we add this flag to the constructor of GradioUI ?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 688, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ab37036-96cc-4e6c-b56c-c010119517ab", "embedding": null, "metadata": {"issue_id": 486, "title": "[BUG] unexpected keyword argument 'provider'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "702e9957-ead9-41c9-bd50-5cc2ba88131e", "node_type": "4", "metadata": {"issue_id": 486, "title": "[BUG] unexpected keyword argument 'provider'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "41e836144ae4a08e1ce84713381c498912543556f02a32c87e83aaa5aecb8e1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] unexpected keyword argument 'provider'\n\nDescription: make test fails: [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af5046e5-de1d-4876-90ff-8da9f8dc0c84", "embedding": null, "metadata": {"issue_id": 485, "title": "LLM-ready documentation", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed17c9ba-b6c6-4fa7-a7c8-22a4dc44d1f0", "node_type": "4", "metadata": {"issue_id": 485, "title": "LLM-ready documentation", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "cf4fe1f1f808d85ee23c49dcad407e76a578d531cea007e3a58d53dada9bc787", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: LLM-ready documentation\n\nDescription: The idea is to have condensed documentation ready for LLM to generate agent code. I currently do CTRL-C, CTRL-V of the smolagents documentation pages and give a prompt with what I want to get. I also often use Cline/VSC and Aider, where documentation files can be added to the context. It would be nice to have a documentation generator for LLM ready versions: - minimum documentation (max ?) - full documentation (max 64KiB) - extended documentation with examples etc. (max 128K) So it would be great to have ready updated documentation, or to have a script that generates such documentation from e.g. smolagents github. It would certainly speed up the code development process. Thank you in advance for your consideration!\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f64fdfe7-dd4e-4fab-b58f-e935963b0a91", "embedding": null, "metadata": {"issue_id": 483, "title": "Strongly enforce types in tools", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc7ce2a5-ee2a-468b-969a-d4630b788b62", "node_type": "4", "metadata": {"issue_id": 483, "title": "Strongly enforce types in tools", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "9036406f5c4b64b70e7a10e35a1b6865d82337abb8035ad6b6fa7ba7f5396a1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Strongly enforce types in tools\n\nDescription: **Is your feature request related to a problem? Please describe.** Some times agents will try calling a tool incorrectly despite the type hints, especially if the backbone weak and/or the tools are many with complex signatures. This could lead to runtime errors (in which case agent has a chance to fix its error but still the error message is not guaranteed to reveal the root case) or return wrong results with no indication that an error occurred which would make debugging a nightmare. **Describe the solution you'd like** The tool can optionally enforce its argument types similar to a strongly typed programming language. So we would exit early with an error message revealing the root cause and giving the agent the best possible chance to recover. **Is this not possible with the current options.** It is, one could manually add assertions in the body of their decorated tool function (or the forward method of their tool), but its probably useful to automate. **Describe alternatives you've considered** I made a draft implementation for the decorator: https://github.com/huggingface/smolagents/pull/482. Happy to know whether this is a feature worth supporting and/or my approach sounds promising.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af10ea39-cabc-4aba-a667-5dbea5fcbb25", "embedding": null, "metadata": {"issue_id": 477, "title": "[BUG] OTEL tracing not grouping traces when using GradioUI", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2c78208-803b-4782-bef1-4de589613012", "node_type": "4", "metadata": {"issue_id": 477, "title": "[BUG] OTEL tracing not grouping traces when using GradioUI", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "999b1969a9794273aa91e62c83fc5fb561c2f8ed84b4560eb47fb43dbddabf7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] OTEL tracing not grouping traces when using GradioUI\n\nDescription: **Describe the bug** I\u00b4m trying the example from here and adding the GradioUI at the end, and I see that after adding the GradioUI then the traces are not being grouped together on the OTEL tracing tool (Arize). Without GradioUI it works as expected. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** !Image **Expected behavior** All steps should be grouped under the same CodeAgent.run trace, similar as it is already working when not using GradioUI **Packages version:** smolagents 1.7.0 **Additional context** No additional context required\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6e3abbe8-3266-4a16-800d-38b5df408763", "embedding": null, "metadata": {"issue_id": 476, "title": "[BUG] Invalid type warnings/errors on OTEL tracing example", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f629957-dc1a-4525-93dc-d0f317afbd5d", "node_type": "4", "metadata": {"issue_id": 476, "title": "[BUG] Invalid type warnings/errors on OTEL tracing example", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "777e98e7656057f0131270a761ce311ebc4d967b6586c3b3ff6f3be7fbffbd8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Invalid type warnings/errors on OTEL tracing example\n\nDescription: **Describe the bug** I\u00b4m trying the example described here and warnings of \"Invalid type\" are appearing constantly on the terminal. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** No warnings or errors **Packages version:** smolagents 1.7.0 **Additional context** No additional context\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e903432e-7dda-4516-bb7f-afb3fbfe2121", "embedding": null, "metadata": {"issue_id": 473, "title": "[BUG] SpeechToTextTool not working", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e3aca7f-f7f3-4912-9a39-5e0d88469f94", "node_type": "4", "metadata": {"issue_id": 473, "title": "[BUG] SpeechToTextTool not working", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "fa22b18869388051a878c0b4ea2bc9bd24fd94e0c4586db8a4f8a846eb0adc2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] SpeechToTextTool not working\n\nDescription: **Describe the bug** When I try to use the SpeechToTextTool, it fails **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** I expect it to work as other tools work **Packages version:** smolagents==1.7.0 **Additional context** No required additional context\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2b18bcfe-0d0b-4391-81c8-d3bcde03e7a9", "embedding": null, "metadata": {"issue_id": 472, "title": "Separate small package for the local python interpreter code?", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1fd5f5b8-6707-4989-9836-456bd63261ec", "node_type": "4", "metadata": {"issue_id": 472, "title": "Separate small package for the local python interpreter code?", "state": "open", "labels": [], "type": "issue"}, "hash": "81d26ebb611c778c19fc8be4d5d4973e7e97796b64fc7599807bf0231cf13d33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Separate small package for the local python interpreter code?\n\nDescription: Hi, This is not a bug - just a pointer to a similar dynamic function calling approach I just open-sourced in the functionsmith package. It's targeted at writing all the code it needs from scratch rather than calling external tools, but that's because I'm coming from a slightly different angle. You are welcome to copy any of that code - the license is Apache 2.0. BTW, I like your local python interpreter code. Do you have any plans to make it available as a separate package? Best, Simon\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7ed71b6-cb44-4799-ac54-cf73c8e95f16", "embedding": null, "metadata": {"issue_id": 469, "title": "Batch Generation with Agents for RL", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb9cca86-34ae-4c92-9670-98472127f7d4", "node_type": "4", "metadata": {"issue_id": 469, "title": "Batch Generation with Agents for RL", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "16e120a655ec044abdc2b463873c462e5500b6f846568e5d67a473e8764120ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Batch Generation with Agents for RL\n\nDescription: follow up on our earlier discussion. This year seems to be all about AI labs flexing their Agents trained with reinforcement learning. There\u2019s been a lot of request in trl and open r1, especially since smolagents Hugging Face\u2019s Agents library. We'll need to make smolagents work well with RL and simplify the process of training these Agents. To get started with training agents using GRPO (the RL method for Deepseek R1), we\u2019ll first need to generate Agent responses in batches to maximize GPU utilization. Then, we can integrate that into TRLs GRPO. We could maybe modify Modelhttps://github.com/huggingface/smolagents/blob/93c433c81f781f64454f8d3450ce7d52dc5d35a3/src/smolagents/models.pyL240 or TransformersModel https://github.com/huggingface/smolagents/blob/93c433c81f781f64454f8d3450ce7d52dc5d35a3/src/smolagents/models.pyL415 to handle batch generation, along with MultiStepAgent https://github.com/huggingface/smolagents/blob/93c433c81f781f64454f8d3450ce7d52dc5d35a3/src/smolagents/agents.pyL125 for processing those agent calls sequentially. Or we could create separate classes for each. And when the vllm backend is ready, we can do the same thing with vllm for even better efficiency.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de07e68c-bef4-46f6-afc7-04c7613ef866", "embedding": null, "metadata": {"issue_id": 468, "title": "[BUG] Unsupported parameter: 'max_tokens' with o3-mini", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5e88a0f-4609-415f-bbe8-0d30286220df", "node_type": "4", "metadata": {"issue_id": 468, "title": "[BUG] Unsupported parameter: 'max_tokens' with o3-mini", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "dc3140c05cd20f72d0192feeaa7ba71648618f37b99db659d10331bffd6aaf92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Unsupported parameter: 'max_tokens' with o3-mini\n\nDescription: OpenAi's o3-mini doesn't accept max_tokens but max_completion_tokens instead. When running: [CODE_BLOCK] I get: [CODE_BLOCK] This is with smolagents version = \"1.7.0\". The temporary solution I found is to delete such default param from kwargs: [CODE_BLOCK] That way it works. I believe this will be a problem with all reasoning models like o1 and o1-mini.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f93433dc-4aaf-441c-a36e-3255cb0007f8", "embedding": null, "metadata": {"issue_id": 467, "title": "[BUG] Error in generating model output: embedding(): argument 'indices' (position 2) must be Tensor, not list", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0bc5d897-d02a-45d7-a836-093d128062a7", "node_type": "4", "metadata": {"issue_id": 467, "title": "[BUG] Error in generating model output: embedding(): argument 'indices' (position 2) must be Tensor, not list", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "61b53830dcbcee9d8f40667132016fbbb7110676f4fceba1e6889755bb62f7f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Error in generating model output: embedding(): argument 'indices' (position 2) must be Tensor, not list\n\nDescription: Using locally run model on a GPU, results in the following error: Error in generating model output: embedding(): argument 'indices' (position 2) must be Tensor, not list Have tried with the following models: model_id = \"meta-llama/Llama-3.2-3B-Instruct\" model_id_2 = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\\ model_id_4 = \"Qwen/CodeQwen1.5-7B\" model_id_5 = \"Qwen/Qwen2.5-Coder-3B-Instruct\" Issue does not happen when model is run locally on a cpu. Sample code: [CODE_BLOCK] **Packages version:** pip show smolagents Name: smolagents Version: 1.2.2\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd5de574-c3e9-4a3a-853b-f22ac775c4eb", "embedding": null, "metadata": {"issue_id": 466, "title": "[BUG] Incorrect README Link to Helium Cryptocurrency Repository", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15c589df-aa44-4e50-96ae-8835f3480cb0", "node_type": "4", "metadata": {"issue_id": 466, "title": "[BUG] Incorrect README Link to Helium Cryptocurrency Repository", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "e8a3daf6287c7b0ce4a0e6b18cf46b8ac606817df9990085798e9a0be5a9cf27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Incorrect README Link to Helium Cryptocurrency Repository\n\nDescription: The README.md file for the Helium project incorrectly links to the Helium cryptocurrency GitHub repository instead of the Helium browser automation library. This can confuse users looking for information on browser automation. Code to reproduce the error N/A (This is not a code-related issue, but rather a documentation error.) Error logs (if any) N/A Expected behavior The README.md should contain a link to the correct Helium browser automation library GitHub repository, allowing users to find relevant resources without confusion. Packages version: N/A (This issue is related to documentation and not dependent on package versions.) Additional context This issue may lead to confusion for new users who are trying to understand how to use Helium for browser automation. Correcting the link in the README.md will help direct users to the appropriate resources and improve overall user experience. Feel free to modify any details as necessary!\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8ca7d0bf-2c54-4836-a8c1-34923f9f600a", "embedding": null, "metadata": {"issue_id": 465, "title": "ability to make custom prompt files without overwriting the lib one", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47572387-09e4-48e1-ac2b-efc98d1d5f67", "node_type": "4", "metadata": {"issue_id": 465, "title": "ability to make custom prompt files without overwriting the lib one", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "47b944625ffb6ec54eb4438b387bda3c8ca3456c2f93678610bed9d71e162eba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: ability to make custom prompt files without overwriting the lib one\n\nDescription: **Is your feature request related to a problem? Please describe.** i wanted to be able to make custom prompts.py files without overwriting the original one. the goal was to modify them as per model basis because all models do not respond the same tho the prompts. i know there is system prompt option but i thinked to have more granularity **Describe the solution you'd like** add a \"if custom_prompt.py... else use prompts.py **Is this not possible with the current options.** i tried monkeypatching without success but i am just an amateur **Describe alternatives you've considered** modify the file each times **Additional context** also i have another question, is there any reason why you use: {{tool_descriptions}} and {tool_descriptions} or we can modify the code to have the everything in double brackets or in single brackets\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1031, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9228f656-0ce0-4a43-98ac-4babbe73eff2", "embedding": null, "metadata": {"issue_id": 464, "title": "[BUG] tools from codeAgent are not passed to model in tools_to_call_from. normal?", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c950854f-6228-48f8-b8e3-a44019a0364e", "node_type": "4", "metadata": {"issue_id": 464, "title": "[BUG] tools from codeAgent are not passed to model in tools_to_call_from. normal?", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "e32d42e79c4aaf647bd93513f27f25d4ea03eb66354fc56a668834af9fdc9f25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] tools from codeAgent are not passed to model in tools_to_call_from. normal?\n\nDescription: **Describe the bug** when i put some tools in a codeAgent, i do not find them in tools_to_call_from if i make a print in models.py is it normal?. **Code to reproduce the error** in a script web_agent2 = CodeAgent( tools=[DuckDuckGoSearchTool()], model=model_deepseek_r1 ) in models.py: print(tools_to_call_from);sys.exit() result: None **Error logs (if any)** **Expected behavior** provide tools to put into the request or add to the prompt **Packages version:** from v.1.5 to v.1.8.dev i think **Additional context** the problem is that small llms can not call those tools, so they call tool exemple from the system prompt and we get nothing\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "133a6cc6-9a70-4e86-8a40-2fc59dc5862a", "embedding": null, "metadata": {"issue_id": 449, "title": "Feature Request: Add `LlamaCppModel` Support to smolagents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23ae7718-b71c-40d8-8a00-cd43f6cb82ba", "node_type": "4", "metadata": {"issue_id": 449, "title": "Feature Request: Add `LlamaCppModel` Support to smolagents", "state": "open", "labels": ["enhancement"], "type": "issue"}, "hash": "9ed1d53fc17daa4c8476080b05847dd8a1dfe1d1f947ddd3c63d79d38b1ffa99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feature Request: Add `LlamaCppModel` Support to smolagents\n\nDescription: **Motivation Behind This Feature** As the landscape of language models continues to evolve, integrating diverse model architectures becomes crucial for enhancing the versatility and applicability of libraries like **smolagents**. The llama.cpp framework offers an efficient and optimized way to run large language models with reduced resource consumption, making it an attractive option for developers and researchers. **Current Challenges:** - **Limited Model Support:** While **smolagents** currently supports models like those from Hugging Face's Transformers library, there's a growing demand for integrating models managed by llama.cpp. - **Performance Optimization:** llama.cpp provides optimized performance for running large language models on resource-constrained environments, which is beneficial for users who require high efficiency without compromising on model capabilities. **Proposed Solution:** Introduce a new LlamaCppModel class that seamlessly integrates llama.cpp models into the **smolagents** ecosystem, ensuring proper parameter handling and conditional tool usage. **Detailed Description** The LlamaCppModel class is designed to interact with llama.cpp models, providing robust parameter management and the ability to utilize tools only when explicitly provided. This integration ensures that users can leverage the efficiency of llama.cpp while maintaining the flexibility and functionality that **smolagents** offers. **Key Features:** - **Flexible Model Loading:** Supports loading models from a local path or directly from a Hugging Face repository. - **Parameter Management:** Allows customization of GPU layers, context size, and maximum token generation. - **Conditional Tool Integration:** Integrates tools seamlessly when they are passed, ensuring optimized performance.\n\nState: open\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "601056d7-94cb-4450-a930-538cd2232804", "embedding": null, "metadata": {"issue_id": 442, "title": "\"uvx\" instead of \"uv\" in  MCP sample code?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5d74cc9-ed51-46e8-ada4-07279a3595aa", "node_type": "4", "metadata": {"issue_id": 442, "title": "\"uvx\" instead of \"uv\" in  MCP sample code?", "state": "closed", "labels": [], "type": "issue"}, "hash": "3eab459bb97eb107528731e25c3340612aa4afb948aaaa96b5b0239c630a0b35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: \"uvx\" instead of \"uv\" in  MCP sample code?\n\nDescription: https://github.com/huggingface/smolagents/blob/181a500c5d45d41a85e62461f4ee42b5442aef23/docs/source/en/tutorials/tools.md?plain=1L239 Was following the docs and trying out the sample code, I'm getting error message as below. Running with usx seems to work fine. Didn't look through history to find out where it went wrong, or if this is the expected behaviour. Just FYI. !Image\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6c2ca91e-73f0-42a9-950a-150141a6593c", "embedding": null, "metadata": {"issue_id": 441, "title": "[BUG] LiteLLMModel, ModuleNotFoundError: No module named 'cgi'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04db8004-e935-4775-bd9e-398d18da5a80", "node_type": "4", "metadata": {"issue_id": 441, "title": "[BUG] LiteLLMModel, ModuleNotFoundError: No module named 'cgi'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "684316f4193bd390c2a904ea4cab79a93b4e777c66053fcf6de1acc6a2decc7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] LiteLLMModel, ModuleNotFoundError: No module named 'cgi'\n\nDescription: **Describe the bug** There is already a litellm issue about the issue. The LiteLLMModel depends on litellm which depends on cgi. The cgi has recently been removed from python, pep-0594, causing litellm to break. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** That the ollama gets invoked, and prints out something like the following. [CODE_BLOCK] **Packages version:** [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "483f6ebe-4060-4b53-bf82-f1f0b4343bd7", "embedding": null, "metadata": {"issue_id": 440, "title": "[BUG] When using the code on the telemetry example", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "013f3ad4-14bc-4fd5-908c-f0dcfc25aae1", "node_type": "4", "metadata": {"issue_id": 440, "title": "[BUG] When using the code on the telemetry example", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "1147ecc1513b7f03a15311e538b4d2c213e2845c81989009c32011bb7b4e2ef1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] When using the code on the telemetry example\n\nDescription: **Describe the bug** When reproducing the code with telemetry (docs/source/en/tutorials/inspect_runs.md). There is an error from the telemetry with the message format. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** Invalid type dict in attribute 'llm.input_messages.0.message.content' value sequence. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or None **Expected behavior** The message text property is logged **Packages version:** openinference-instrumentation-smolagents==0.1.2 smolagents==1.6.0\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a1ef0e36-01fe-4c4e-b2c6-b2a19f9eccef", "embedding": null, "metadata": {"issue_id": 434, "title": "[BUG] Error when using e2b executor that should return image", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19c6e413-b123-4d4f-8f7d-d0767ce0c020", "node_type": "4", "metadata": {"issue_id": 434, "title": "[BUG] Error when using e2b executor that should return image", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "cbe2b4dca10e858c84abe43269187ab902fcf6d8620957f2beb5d8b08c50672e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Error when using e2b executor that should return image\n\nDescription: **Describe the bug** When using smolagents(1.6.0) and a CodeAgent that should be returning a PIL image and running on E2B executor, image returns fail. Other returns works well. The error shows that the return is missing a parameter (got 2 expected 3). **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Should yield valid return **Packages version:** smolagents==1.6.0 **Additional context** From what I can gather, this issue arises from the E2BExecutor class, on line 138: [CODE_BLOCK] which probably should be [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51f71a58-453f-403b-9dbb-d5a933ab8dd8", "embedding": null, "metadata": {"issue_id": 430, "title": "[BUG] DeepSeek Reasoner rejects system messages through LiteLLM with invalid message format error", "state": "open", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "518b4109-d8ed-4336-afbf-5b454bcd19bb", "node_type": "4", "metadata": {"issue_id": 430, "title": "[BUG] DeepSeek Reasoner rejects system messages through LiteLLM with invalid message format error", "state": "open", "labels": ["bug"], "type": "issue"}, "hash": "1410f8e40487387d3a6ad70967fe2a1fd4adf5dc7b38ae6dcfc352c950fab137", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] DeepSeek Reasoner rejects system messages through LiteLLM with invalid message format error\n\nDescription: **Describe the bug** When using LiteLLM to interact with DeepSeek Reasoner's API, the API rejects requests that include system messages with a 400 error. The error message indicates that \"The last message of deepseek-reasoner must be a user message, or an assistant message with prefix mode on\", suggesting that DeepSeek Reasoner has specific requirements about message ordering and format that aren't being handled correctly by the current LiteLLM integration. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Should search for weather and respond based on search results. **Packages version:** [CODE_BLOCK] **Additional context** Note that a simple LiteLLM-only test script like this works fine: [CODE_BLOCK]\n\nState: open\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 963, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe9c2b2b-0c2f-4c94-afdf-decb547cf5ae", "embedding": null, "metadata": {"issue_id": 429, "title": "[BUG] Groq API incompatible with smolagents system messages when using OpenAI endpoint via LiteLLMModel", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "980da410-5399-4a6e-8884-816bf15cdcaa", "node_type": "4", "metadata": {"issue_id": 429, "title": "[BUG] Groq API incompatible with smolagents system messages when using OpenAI endpoint via LiteLLMModel", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "02221dc7cc96d9ceb6b7786750e121c2efa156b59b1d96caedeae12bb72612be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Groq API incompatible with smolagents system messages when using OpenAI endpoint via LiteLLMModel\n\nDescription: **Describe the bug** A clear and concise description of what the bug is. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** Should run as any other litellm model. **Packages version:** Run pip freeze | grep smolagents and paste it here: [CODE_BLOCK] **Additional context** I ran the same tests on groq using only litellm (i.e. not as an agent) and there is no issue handling system prompts.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67d2ab4b-c74d-4c0b-9964-242cd66ad54c", "embedding": null, "metadata": {"issue_id": 427, "title": "Cannot publish to hub if the tool has a file object (bytes/BytesIO) as input?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e957df9-3e3c-425a-aa2b-f283c9237d19", "node_type": "4", "metadata": {"issue_id": 427, "title": "Cannot publish to hub if the tool has a file object (bytes/BytesIO) as input?", "state": "closed", "labels": [], "type": "issue"}, "hash": "8cafee93a097b03efbe1657a5bc3acb751ead2a8fd2a3124299c3cbaa9ef54e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Cannot publish to hub if the tool has a file object (bytes/BytesIO) as input?\n\nDescription: I am trying to publish a tool to HG but ran into an issue - it seems like I cannot publish a tool if it takes a file object as input. I am passing an image to the tool. The error comes from `utils.py` line 363-364 [CODE_BLOCK] Log: TypeError: module, class, method, function, traceback, frame, or code object was expected, got _SpecialForm Am I missing something obvious?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f64ffe5a-2778-4c13-af17-583a48c9a93c", "embedding": null, "metadata": {"issue_id": 425, "title": "agent memory as graphs or traces", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e923492d-80f7-42da-9bf4-057a6397f7d0", "node_type": "4", "metadata": {"issue_id": 425, "title": "agent memory as graphs or traces", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "da3b5f5f4dcf1921c4cccce5a1e79e51b6eaf3c4c6eedab6a4132aae42d03a60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: agent memory as graphs or traces\n\nDescription: **Is your feature request related to a problem? Please describe.** Agents are cool but then can be a bit black-boxy. Logs and memory solve this a bit but they show a per-run overview, which requires more focus/attention to actually process as a human. **Describe the solution you'd like** - Convert them to traces which could be visualised in something like langfuse. - Convert them to a graph. !Image - Convert them to process logs https://www.processmining.org/event-data.html. Might be used to indicate failure points in more complex agent flow, which I used for chatbot analysis a long time ago, although this is more researchy. **Is this not possible with the current options.** This is not an out-of-the-box integration. **Describe alternatives you've considered** Looking at the logs. **Additional context** N.A.\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 981, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16d11b19-4d7d-4124-98a9-3c24de1a6be2", "embedding": null, "metadata": {"issue_id": 424, "title": "Importing smolagents takes a large amount of time", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8d095ce-b538-4cba-b997-ae0d1b911c91", "node_type": "4", "metadata": {"issue_id": 424, "title": "Importing smolagents takes a large amount of time", "state": "closed", "labels": ["enhancement"], "type": "issue"}, "hash": "17ec304df1dcdb6b5f19990acbc5a59e5d2a55e180cdccca1bbf62ce6d234220", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Importing smolagents takes a large amount of time\n\nDescription: **Is your feature request related to a problem? Please describe.** The was referenced 100 and was addressed by not making certain heavy dependencies required like torch. After this was addressed, the import time was cut in half down to a much more manageable 3.8s 147 . However, the import time has now gone back up significantly seen after running [CODE_BLOCK] which results in ~ Import time: 7.92 seconds. **Describe the solution you'd like** Ideally, I\u2019d love to see smolagents import faster, especially since it was optimized in a previous issue (100). Maybe there\u2019s an opportunity to revisit that optimization or find another way to delay some of the heavier imports until they\u2019re actually needed. That said, I totally get that some libraries just take time to load, and maybe this is unavoidable. If that\u2019s the case, I\u2019d appreciate any insight into why it\u2019s happening and if there are any best practices for mitigating it (like lazy loading, import restructuring, etc.). **Is this not possible with the current options.** I\u2019m not sure! It might be that smolagents is doing exactly what it needs to do, and this is just the reality of working with it. But if there\u2019s an easy way to improve things, I\u2019d be curious to hear about it. **Describe alternatives you've considered** I\u2019ve thought about breaking up the import so that I only bring in the specific pieces I need instead of the full package, but since I\u2019m using CodeAgent, I\u2019m not sure if that would help much. Another option could be lazy-loading some dependencies, but I don\u2019t know enough about the internals of smolagents to say if that would work here. **Additional context**\n\nState: closed\n\nLabels: enhancement\n\nCategories: category-enhancement\n\nType: Feature request or enhancement", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f777a8a8-0d0f-46d5-b708-317e030d0256", "embedding": null, "metadata": {"issue_id": 421, "title": "[BUG] Planning interval feature causes TypeError when concatenating message content", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a8392bf-8404-4835-ae1f-7bc3c0eeb1b3", "node_type": "4", "metadata": {"issue_id": 421, "title": "[BUG] Planning interval feature causes TypeError when concatenating message content", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "c932c078f54c830e67308a63203da62f6ee1dd9c2756c77238adf237efbb7ae0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Planning interval feature causes TypeError when concatenating message content\n\nDescription: **Describe the bug** The planning_interval feature in smolagents fails with a TypeError when using certain models (e.g., Gemini). The error occurs because the model returns a list instead of a string for the planning step content, causing a type error in message concatenation. Code to reproduce the error [CODE_BLOCK] **Error logs** [CODE_BLOCK] **Expected behavior:** The planning step should handle both string and list responses from models appropriately, allowing the agent to periodically reflect on its progress and plan next steps without throwing type errors. **Packages version:** v1.5.0 **Additional context:** - Fails with Claude sonnet 3.5 as well.\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 847, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd0e0c29-1801-4982-81a2-c8d2894f9775", "embedding": null, "metadata": {"issue_id": 420, "title": "Multi agent example not working", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96e29989-63bd-4323-b59a-45723b719487", "node_type": "4", "metadata": {"issue_id": 420, "title": "Multi agent example not working", "state": "open", "labels": [], "type": "issue"}, "hash": "071352aa1230de8adbedf4002b2241c087137e5f2974f44921ee1d64670d0e82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Multi agent example not working\n\nDescription: Hi, I tried out the following code from the example here. [CODE_BLOCK] And get the same response for 5 iterations: [CODE_BLOCK] The agent is called in the wrong way and therefore this results in that error: [CODE_BLOCK] What could be the reason? Is the model too weak to understand how to call the managed agent?\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 378, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "baf27815-6f45-43e3-910e-123269e5bfc9", "embedding": null, "metadata": {"issue_id": 417, "title": "Following up on Untangling logging", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2832bc45-8572-4312-bfa2-d2a60b3a8d4f", "node_type": "4", "metadata": {"issue_id": 417, "title": "Following up on Untangling logging", "state": "closed", "labels": [], "type": "issue"}, "hash": "2c4fdd266c5c1c28f08408959f4c0fb8c293b8fbde2613ce4abc73eb246f48fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Following up on Untangling logging\n\nDescription: Following up on points raised in PR 316: - The replay method could be agent-assigned instead of memory assigned. This would allow to combine the current \"logging steps to console\" logic from agents.py from the \"logging steps to console\" logic in the replay method. - Putting logger (as in \"show in console what's happening) logic in monitoring.py\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0e924809-e08d-4bc8-8e80-d6b65f322e57", "embedding": null, "metadata": {"issue_id": 415, "title": "[BUG] ToolingAgent fails to call tool if telemetry is enabled.", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3c8f477-0d07-49fa-8ece-1eda41ddb0ec", "node_type": "4", "metadata": {"issue_id": 415, "title": "[BUG] ToolingAgent fails to call tool if telemetry is enabled.", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "89a89da9d16fcb34be81cba0326d90a71db7e71981dbf63800b74bb2c54ef96f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] ToolingAgent fails to call tool if telemetry is enabled.\n\nDescription: **Describe the bug** I observed probably minor bug while running running ToolCallingAgent() with SmolagentsInstrumentor. If tool passed to agent does not have docstring (checked in smolagents._function_type_hints_utils.get_json_schema) code try to raise error while fails because of func is DuckDuckGoSearchTool object and it does not have __name__ attribute. [CODE_BLOCK] **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** If SmolagentsInstrumentor().instrument is commented out results looks like expected [CODE_BLOCK] **Packages version:** [CODE_BLOCK] **Additional context** This setUp method: [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb6f7ee3-cf92-47e7-aa57-0f4b17859c41", "embedding": null, "metadata": {"issue_id": 414, "title": "[BUG] Running with `TransformersModel` does not work", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "87afa38a-8521-485d-a511-d7d98dac43a8", "node_type": "4", "metadata": {"issue_id": 414, "title": "[BUG] Running with `TransformersModel` does not work", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "1a6d860efe2258c092d481bcd5c05e16a861199ba48f6feb474fa0b32e6c2d2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] Running with `TransformersModel` does not work\n\nDescription: **Describe the bug** When replacing HfApiModel with TransformersModel in examples/benchmark.ipynb, the eval results for meta-llama/Llama-3.1-8B-Instruct (and various other published models) are far worse than published (scores of less than 5). **Code to reproduce the error** https://github.com/danielkorat/smolagents/blob/transformers/examples/benchmark-transformers.ipynb **Error logs (if any)** Seems like a big part of problem is the parsing of the LLM output (specifically the assistant role): !Image Also, the regex parsing error arises in nearly all examples. **Expected behavior** Trying to reproduce the results for meta-llama/Llama-3.1-8B-Instruct, as published in the original notebook: !Image **Packages version:** [CODE_BLOCK] **Additional context** Add any other context about the problem here. [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dff02263-62e4-47fa-bbbf-9945f4f84e52", "embedding": null, "metadata": {"issue_id": 413, "title": "We want you to check out the smolagents roadmap!", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53bca5e5-27f6-4ad3-85e5-ad72342fdc5a", "node_type": "4", "metadata": {"issue_id": 413, "title": "We want you to check out the smolagents roadmap!", "state": "open", "labels": [], "type": "issue"}, "hash": "805d8e5a27a25805ae038f670e32bceb1a69f2018d3246836472ea1e912dc63c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: We want you to check out the smolagents roadmap!\n\nDescription: **Checkout the smolagents roadmap to get a direct insight on what features the team is developing next**: all unassigned issues are issues that you can take on!\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 243, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "def543a2-bec9-4a40-8496-0125f2493d9b", "embedding": null, "metadata": {"issue_id": 404, "title": "[BUG] \"Error in generating tool call with model: 'NoneType' object is not subscriptable\" when using FinalAnswerTool ", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e647575b-3d17-4625-862d-93dfd3184076", "node_type": "4", "metadata": {"issue_id": 404, "title": "[BUG] \"Error in generating tool call with model: 'NoneType' object is not subscriptable\" when using FinalAnswerTool ", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "63622f8130622d1d282209c5dd8e23f007e2f754adf5532e3586ccaab871b401", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] \"Error in generating tool call with model: 'NoneType' object is not subscriptable\" when using FinalAnswerTool \n\nDescription: **Describe the bug** I am trying to reproduce the agentic RAG cookbook example (https://github.com/huggingface/cookbook/blob/main/notebooks/en/agent_rag.ipynb). But I noticed that the FinalAnswerTool was almost always throwing me an error. I tried with : * gpt4o (AzureOpenAI) * mistral-large-2407 (OpenAI-compatible API) I also replaced ToolCallingAgent by CodeAgent, the error log is different but it still does not work most of the time. I would say 2/3 of the time, and even when it works the agent needs multiple steps to get an answer. I decided to remove every RAG-related component in my pipeline and I just kept the FinalAnswerTool in my agent to make sure the RAG logic as nothing to do with this. The error remains the same. **Code to reproduce the error** [CODE_BLOCK] **Error logs (if any)** [CODE_BLOCK] **Expected behavior** I expect the question to be answered easily as this is a simple question using a FinalAnswerTool provided by default by the Smolagents package. **Packages version:** smolagents 1.5.1 **Additional context** My previous issue (https://github.com/huggingface/smolagents/issues/381) was closed and I can't re-open it. However, my problem was not that much related to the OpenAIServerModel class as I had already found an alternative by customizing it. My issue is with ToolCallingAgent (and also CodeAgent that I tried thanks to your suggestion), I am still getting error pretty much 50%, even if CodeAgent works more often than ToolCallingAgent. I can provide with more information if you need\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1749, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cc3928ce-09dc-487a-b9cf-59d11058f346", "embedding": null, "metadata": {"issue_id": 402, "title": "CLI interface", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a94c646b-1ec4-4b4d-824b-3888ce49aaed", "node_type": "4", "metadata": {"issue_id": 402, "title": "CLI interface", "state": "closed", "labels": [], "type": "issue"}, "hash": "699c865c03409c6d6510412de66e64e162d8e2f0b40d48af8092118f2e8ba6d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: CLI interface\n\nDescription: Add CLI commands to quickly run agents, possibly in a gradio interface.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 121, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0ab3837c-a8f7-43f2-88f8-1b60decb7ef6", "embedding": null, "metadata": {"issue_id": 401, "title": "Improve Readme", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9257dab4-09c2-44e9-96ce-d3e07b68ddc4", "node_type": "4", "metadata": {"issue_id": 401, "title": "Improve Readme", "state": "closed", "labels": [], "type": "issue"}, "hash": "f2de3d2048b7788bce24ba80f57bf87dfa8594533c438d4cd6739ccf84204bc0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Improve Readme\n\nDescription: From discussions with @thomwolf , the current Readme could be improved: - Reduce first part (for instance the mention of ToolCallingAgent could be st further down) - Provide more code examples - Some examples could be in collapsible sections - Mention VLM and MCP support - Next steps: add a list of course on agents, for instance\u201cAwesome agent courses\u201d\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "68e97cb3-9e56-44ed-b31e-b32bdda6506b", "embedding": null, "metadata": {"issue_id": 400, "title": "Make a performant WebBrowserAgent class", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b8dac23-62f8-4fe9-a51f-b5178b3e5f6a", "node_type": "4", "metadata": {"issue_id": 400, "title": "Make a performant WebBrowserAgent class", "state": "open", "labels": [], "type": "issue"}, "hash": "5e77c27cb1904a2b38f521f3a3ff49cb4e67f2ab254236d5aeeb0d4d952401c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Make a performant WebBrowserAgent class\n\nDescription: Web browsing is a highly specific task, requiring very different tools and state than other agentic tasks. There are two big avenues for developing web browsers: - text-based - vision-based At the moment, from internal tests, text works better than a raw vision from a base VLM that has no labelling done to help him click on screenshots. But browser-use has lots of success with such a scaffolding for vision models, so it could be something to try out.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06d22126-5bb4-4837-96c1-206517e11264", "embedding": null, "metadata": {"issue_id": 399, "title": "Put a DeepSeek-R1 based agent on top of GAIA leaderboard", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5b5da26-2bc5-459e-a0b3-a4314049134d", "node_type": "4", "metadata": {"issue_id": 399, "title": "Put a DeepSeek-R1 based agent on top of GAIA leaderboard", "state": "closed", "labels": [], "type": "issue"}, "hash": "4c65606259537405a20fe4176f3c13acf8df224fb645b94775d2af64cb63d5d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Put a DeepSeek-R1 based agent on top of GAIA leaderboard\n\nDescription: The GAIA leaderboard for agents is currently topped by agents powered by OpenAI or Anthropic models. Let's put open models to their rightful place, all the way on top! I've open this branch to work on it: https://github.com/huggingface/smolagents/tree/gaia-submission-r1\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "14bd19da-66ee-4638-aefa-86674e98e4df", "embedding": null, "metadata": {"issue_id": 398, "title": "Share full agents to the Hub", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0600506b-8d6d-40f0-8928-65b4445e17cc", "node_type": "4", "metadata": {"issue_id": 398, "title": "Share full agents to the Hub", "state": "closed", "labels": [], "type": "issue"}, "hash": "9f5e3f47895e2a2f5b253f09b9c7bc5105cc53acb1a31ca04e00f489193178d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Share full agents to the Hub\n\nDescription: We need a method agent.push_to_hub!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a45c9290-9438-485a-845b-ed1fe6e35f30", "embedding": null, "metadata": {"issue_id": 397, "title": "Share agent prompts to Hub", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7844c890-03e6-4ed6-b04a-148061431acd", "node_type": "4", "metadata": {"issue_id": 397, "title": "Share agent prompts to Hub", "state": "closed", "labels": [], "type": "issue"}, "hash": "745bb03c2c5217dea3f5bbf732182691dfa491b5e09c43cee626e22720c552ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Share agent prompts to Hub\n\nDescription: Can use prompt-templates library by @MoritzLaurer : https://github.com/MoritzLaurer/prompt_templates\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 163, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73a1677d-c027-4eb9-a9ee-8ed8bebf881f", "embedding": null, "metadata": {"issue_id": 395, "title": "Add an R1-based submission to the GAIA benchmark", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7035dea-6439-4415-8324-6e9a35850d6a", "node_type": "4", "metadata": {"issue_id": 395, "title": "Add an R1-based submission to the GAIA benchmark", "state": "open", "labels": [], "type": "issue"}, "hash": "4eca60f03a402e63f9d42b6aaa378cdee308611318b0d6c3486bfe5895ff8fe1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add an R1-based submission to the GAIA benchmark\n\nDescription: Top solutions in the GAIA leaderboard are all OpenAI or Anthropic-based. Let's supplant them with open source! Branch started here.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3fa088de-c7b5-46ff-9eb8-041dcd9b78f7", "embedding": null, "metadata": {"issue_id": 394, "title": "Agent benchmarking", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd75b581-9e4b-425d-a284-9cfbefb75394", "node_type": "4", "metadata": {"issue_id": 394, "title": "Agent benchmarking", "state": "closed", "labels": [], "type": "issue"}, "hash": "f3be4256f093c46b9681fb4e377b7e93deacff4828922226cfa9fd67b5f0f52a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Agent benchmarking\n\nDescription: Finish/update the benchmark notebook with push to Hub of answers/eval_results.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7068fb6-175b-47f6-ab07-197766ee6de6", "embedding": null, "metadata": {"issue_id": 393, "title": "Consolidate Hub Agent/Tool sharing options", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "910b1dbb-4ad0-4a52-8d3c-5a3ced4e5f66", "node_type": "4", "metadata": {"issue_id": 393, "title": "Consolidate Hub Agent/Tool sharing options", "state": "closed", "labels": [], "type": "issue"}, "hash": "735c1eafb222eabbf6ffaace3a61bc72fe80762074274b9b34a980ab3ef625cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Consolidate Hub Agent/Tool sharing options\n\nDescription: - Fix requirements: https://github.com/huggingface/smolagents/pull/281discussion_r1924390630 - inconsistency between .py and .json: https://github.com/huggingface/smolagents/issues/154\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4830aec-9e93-4d9a-9978-635c4560c309", "embedding": null, "metadata": {"issue_id": 392, "title": "Improve E2B Code Executor", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5250ccf2-a4b3-4956-8e29-7233e5f609f3", "node_type": "4", "metadata": {"issue_id": 392, "title": "Improve E2B Code Executor", "state": "closed", "labels": [], "type": "issue"}, "hash": "3371a9c63be73f5458ad7a0c48737f34d9ad1f90a802e8c2283dc625bde7d17f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Improve E2B Code Executor\n\nDescription: The current E2B Code executor could be improved on several aspects: - Enable running multi-agent workflows - Reducing sandbox cold start by making a custom smolagents E2B sandbox - Find a way to link E2B API Key with HF Token, so as to let users authenticate to E2B directly with their HF token.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 357, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7975a122-ab26-4ad7-9f2e-9a5aa2fa19fd", "embedding": null, "metadata": {"issue_id": 391, "title": "Docker Code Executor", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "21977fe8-4d52-4813-9423-2c6e009cf807", "node_type": "4", "metadata": {"issue_id": 391, "title": "Docker Code Executor", "state": "closed", "labels": [], "type": "issue"}, "hash": "30abed41f110f2ad24ebf9d68b69cec8c75d6be0d94c667447f05f71e608fcb1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Docker Code Executor\n\nDescription: We want to support executing code in a Docker container. TODO: - [ ] Add more doc on this Docker executor. - [ ] Publish a tiny blog post announcing the availability of this executor\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2497acc2-265a-4830-9adc-29911b17e696", "embedding": null, "metadata": {"issue_id": 381, "title": "\"Error in generating tool call with model: 'NoneType' object is not subscriptable\" when using FinalAnswerTool", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c717491b-5216-4c31-b4ed-4bb5156a67d6", "node_type": "4", "metadata": {"issue_id": 381, "title": "\"Error in generating tool call with model: 'NoneType' object is not subscriptable\" when using FinalAnswerTool", "state": "closed", "labels": [], "type": "issue"}, "hash": "6b6e078f731c7d6dcd20c46c9f124847efc8d6e1f3dd507f2eacc604390a1410", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: \"Error in generating tool call with model: 'NoneType' object is not subscriptable\" when using FinalAnswerTool\n\nDescription: I am trying to reproduce the agentic RAG cookbook example (https://github.com/huggingface/cookbook/blob/main/notebooks/en/agent_rag.ipynb). But I noticed that the FinalAnswerTool was almost always throwing me an error. The error is : [CODE_BLOCK] So I decided to remove every RAG-related component in my pipeline and I just kept the FinalAnswerTool in my agent to make sure the RAG logic as nothing to do with this, see the code below : [CODE_BLOCK] I use smolagents 1.5.1 and I tried running a mistral-large available in an OpenAI compatible API I have access to, I also tried with gpt4o in Azure OpenAI. Note : I create a CustomOpenAIServerModel but if you check it it's just because my OpenAI-compatible API needs the organization parameter, I don't change anything else. As previously said, the FinalAnswerTool tool almost fail everytime, sometimes it works at step 2 or 3, sometimes I reach the maximum number of errors. What can I do ? My initial objective is to add a retriever tool to my agent but as the FinalAnswerTool Tool fails almost all the time I can\u2019t trust this approach.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "25d30118-1a2c-4b4a-a7b7-d209298664f0", "embedding": null, "metadata": {"issue_id": 380, "title": "Add separate tree for Model docs", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73f91a80-6ab4-4d8d-988a-1746884b8f2c", "node_type": "4", "metadata": {"issue_id": 380, "title": "Add separate tree for Model docs", "state": "closed", "labels": [], "type": "issue"}, "hash": "f42f360e0707ca35b7ae01bfe4ebcbdc51e99ed50717ac0a463b21a47f98874c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add separate tree for Model docs\n\nDescription: @aymeric-roucher I think it's better to add this Model docs to separate tree in the reference column. Do let me know what you think I can do this if you want.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c14a0c44-db1d-45c3-a1e7-231eb27c946e", "embedding": null, "metadata": {"issue_id": 375, "title": "Need help avoiding agent calling tools with json", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40052c1c-9b6d-4c1c-af9c-3d0cfa3e1db1", "node_type": "4", "metadata": {"issue_id": 375, "title": "Need help avoiding agent calling tools with json", "state": "open", "labels": [], "type": "issue"}, "hash": "19489dd0e794cdb4dad5bb939e972c40ed732f489155d03c5bcca66c03ddb1b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Need help avoiding agent calling tools with json\n\nDescription: I've noticed that - particularly weaker models - often call tools using json (in addition to writing code). e.g.: [CODE_BLOCK]py print(\"=== SEARCHING FOR FIRST THREE SECTIONS CONTENT ===\") sections = bm25_retriever(query=\"Need for the Proposed Development Economic Benefits Recreational Benefits Wind Farm introduction\", num_snippets=5) print(sections) [CODE_BLOCK] Are there any tips on avoiding this?\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 485, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13ed791a-62cb-42fe-b080-4cb4739cdf98", "embedding": null, "metadata": {"issue_id": 365, "title": "Allow more options for in GradioUI(agent).launch()", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "528c8396-fb75-4a60-8803-3d486820b5c6", "node_type": "4", "metadata": {"issue_id": 365, "title": "Allow more options for in GradioUI(agent).launch()", "state": "closed", "labels": [], "type": "issue"}, "hash": "421f09a8b492453c6bd6557ced1a9c2b2c66f37add94c247be640f3ba0aa11aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Allow more options for in GradioUI(agent).launch()\n\nDescription: Hi everyone, I just tried the Gradio interface in smolagents. I am missing the option to set args in the launch() method. For my use case, I need to set launch(server_name=\u201c0.0.0.0\u201d, server_port=7860) to make the server accessible in the local network. Gradio support several args in the launch() method, as it can be seen here. Can we extend the GradioUI().launch() with some or all of these args? By adding **kwargs?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5129c4a-46ce-45ad-8f2a-512f39618b1b", "embedding": null, "metadata": {"issue_id": 364, "title": "[Feature Request] State Serialization & Enhanced User Interaction Support", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ada50d04-8172-4bd7-943c-3bb0fd8018b3", "node_type": "4", "metadata": {"issue_id": 364, "title": "[Feature Request] State Serialization & Enhanced User Interaction Support", "state": "open", "labels": [], "type": "issue"}, "hash": "67d5676c0593e89a056466c3f1a8485be8886818fe39724e51b0b3b58d4197c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Feature Request] State Serialization & Enhanced User Interaction Support\n\nDescription: I've been working on two features that could enhance smolagents' capabilities: 1. **State Serialization** - Direct serialization to/from bytes for flexible storage (databases, caches, etc.) - Preserves complex objects (numpy arrays, pandas DataFrames) - Example use case: [CODE_BLOCK] 2. **Rich User Interaction** - Built-in ask_user function similar to final_answer - Preserves state between interactions - Example use case: [CODE_BLOCK] These features would enable: - Stateful agents that can be paused/resumed - Integration with various storage backends - More interactive agent workflows - Better user experience for applications requiring user input Would love to hear your thoughts on these features and potential implementation approaches.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 854, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bae6df5-a702-4059-99e8-602fdc7257fc", "embedding": null, "metadata": {"issue_id": 361, "title": "When using free models on OpenRouter I run into litellm.RateLimitError: RateLimitError: OpenrouterException", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8be42efc-dc32-47fa-98f4-e00c02d42001", "node_type": "4", "metadata": {"issue_id": 361, "title": "When using free models on OpenRouter I run into litellm.RateLimitError: RateLimitError: OpenrouterException", "state": "open", "labels": [], "type": "issue"}, "hash": "c6dcdf8ca08e153ed48f70724433adc2f9885c7ce5a2fe1bbd5f67e8bd80c541", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: When using free models on OpenRouter I run into litellm.RateLimitError: RateLimitError: OpenrouterException\n\nDescription: According to the documentation there is a limit of 20 requests per minute, which I might be hitting here. > There are a few rate limits that apply to certain types of requests, regardless of account status: > > Free limit: If you are using a free model variant (with an ID ending in :free), then you will be limited to 20 requests per minute and 200 requests per day. > > DDoS protection: Cloudflare's DDoS protection will block requests that dramatically exceed reasonable usage. https://openrouter.ai/docs/limits I'm executing this example: [CODE_BLOCK] Here is my console output: [CODE_BLOCK] What is a bit odd, is that the Output tokens still seem to change, but I'm still getting a RateLimitError starting from Step 3. In general, this could probably be solved with an exponential back-off and retry or a rate limit feature for the model endpoint.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4f689f3-7352-4a6e-81d8-0376e4cba151", "embedding": null, "metadata": {"issue_id": 357, "title": "Do I need to manually install transformers module?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b3d9755-7c0a-4ddf-a94f-93c18edf2950", "node_type": "4", "metadata": {"issue_id": 357, "title": "Do I need to manually install transformers module?", "state": "closed", "labels": [], "type": "issue"}, "hash": "b4f850d2b5fdc4aa8737dbc486bd7f83145d885ec8460ac897af4c915c3fba37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Do I need to manually install transformers module?\n\nDescription: I just installed smolagents with uv, when i run the sample test from the guide, it says transformers module not found. Do I have to manually install it? Seems it's not included in the package. Pls advise, thanks. [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c22c8d7-3911-4c37-af20-920edafd5043", "embedding": null, "metadata": {"issue_id": 356, "title": "[SUGGESTION] Default for MultiStepAgent is CODE_SYSTEM_PROMPT and not something more generic", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "424d7a27-71d4-44d7-b061-fc78ffafd979", "node_type": "4", "metadata": {"issue_id": 356, "title": "[SUGGESTION] Default for MultiStepAgent is CODE_SYSTEM_PROMPT and not something more generic", "state": "closed", "labels": [], "type": "issue"}, "hash": "a099a8464b5a13a5a7e3621d60506ed69d81bd72e01a2a270cfb797c27329565", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [SUGGESTION] Default for MultiStepAgent is CODE_SYSTEM_PROMPT and not something more generic\n\nDescription: The default prompt for the MultiStepAgent is actually the CODE_SYSTEM_PROMPT. While it makes sense for this to be the default for the CodeAgent, I would argue that a more generic prompt, one that does not explicitly ask the agent to use code to solve the problem, should be used as default for the MultiStepAgent. Thoughts?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9742dbe-189a-451b-a8b3-2b51a1310db7", "embedding": null, "metadata": {"issue_id": 355, "title": "Retrieving planning step in streaming mode", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7ab2dcca-3e4b-496e-80ba-920e716120e0", "node_type": "4", "metadata": {"issue_id": 355, "title": "Retrieving planning step in streaming mode", "state": "open", "labels": [], "type": "issue"}, "hash": "ceea9b90471ac9b7ea100b26eeddc34e72de9480d9a849a46f3c959ca1abb7fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Retrieving planning step in streaming mode\n\nDescription: Hi, I am using the agent.run method in streaming mode and returning the steps to the client as a stream. However, the planning steps take a very long time, and the client receives the first ActionStep information from the agent.run method quite late. Are there any ways to get the planning step as stream while using the agent.run method.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "914a1346-f615-4f90-aafe-1eb4689c80eb", "embedding": null, "metadata": {"issue_id": 354, "title": "v1.5.0 not working with ollama", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f02f0db7-2326-4c4f-9c8d-bd523e4c3630", "node_type": "4", "metadata": {"issue_id": 354, "title": "v1.5.0 not working with ollama", "state": "closed", "labels": [], "type": "issue"}, "hash": "bcf023f85adfee885970f6a23e83e20eef581a27f8e5cb758dea660963bfb266", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: v1.5.0 not working with ollama\n\nDescription: v1.4.1 is working with the following code, but v1.5.0 don't work anymore(ollama version is 0.5.7): [CODE_BLOCK] Ollama return HTTP 400 Bad Request: [CODE_BLOCK] if using tcpdump, I see not particular change unless the payload size 9370 with v1.5.0 vs 9293 with v1.4.1 . With v1.5.0: [CODE_BLOCK] with v1.4.1: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a538cd76-4780-45ff-b25a-4c1a80b2e68a", "embedding": null, "metadata": {"issue_id": 353, "title": "Not able to get inference based on image kind data", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9231891e-bd0b-408a-a3b2-6b50f9435c31", "node_type": "4", "metadata": {"issue_id": 353, "title": "Not able to get inference based on image kind data", "state": "closed", "labels": [], "type": "issue"}, "hash": "5330120b7c5fb98c24787fdeef8630b9b5efd4ca99c66b229654476b7e0cf992", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Not able to get inference based on image kind data\n\nDescription: from io import BytesIO from math import sin from time import sleep from smolagents import CodeAgent, LiteLLMModel, OpenAIServerModel, TransformersModel, tool,ToolCallingAgent noqa: F401 from smolagents.agents import ActionStep from smolagents import GradioUI Let's use Qwen-2VL-72B via an inference provider like Fireworks AI model = OpenAIServerModel( model_id=\"Qwen/Qwen2.5-72B-Instruct\", api_base=\"https://api.hyperbolic.xyz/v1\", api_key=\"<KEY>\" ) document_1 = \"/Users/rajivmehtapy/Desktop/Social_Media/Ghk3KWjWUAAuPAk.jpeg\" agent = CodeAgent(tools=[], model=model,) agent.run(\"Describe the image\",single_step=True,images=[document_1]) ---------ERROR------------------------ Error in generating model output: 'str' object has no attribute 'save' Traceback (most recent call last): File \"/Users/rajivmehtapy/Desktop/LLMOps/KB-Test/gr-interface/.venv/lib/python3.12/site-packages/smolagents/agents.py\", line 953, in step llm_output = self.model( ^^^^^^^^^^^ File \"/Users/rajivmehtapy/Desktop/LLMOps/KB-Test/gr-interface/.venv/lib/python3.12/site-packages/smolagents/models.py\", line 738, in __call__ completion_kwargs = self._prepare_completion_kwargs( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/rajivmehtapy/Desktop/LLMOps/KB-Test/gr-interface/.venv/lib/python3.12/site-packages/smolagents/models.py\", line 267, in _prepare_completion_kwargs messages = get_clean_message_list( ^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/rajivmehtapy/Desktop/LLMOps/KB-Test/gr-interface/.venv/lib/python3.12/site-packages/smolagents/models.py\", line 219, in get_clean_message_list \"image_url\": {\"url\": make_image_url(encode_image_base64(element[\"image\"]))}, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/Users/rajivmehtapy/Desktop/LLMOps/KB-Test/gr-interface/.venv/lib/python3.12/site-packages/smolagents/utils.py\", line 393, in encode_image_base64 image.save(buffered, format=\"PNG\") ^^^^^^^^^^ AttributeError: 'str' object has no attribute 'save' The above exception was the direct cause of the following exception: Traceback (most recent call last): File \"/Users/rajivmehtapy/Desktop/LLMOps/KB-Test/gr-interface/image_proc_exp.py\", line 25, in <module> agent.run(\"Describe the image\",single_step=True,images=[document_1]) File \"/Users/rajivmehtapy/Desktop/LLMOps/KB-Test/gr-interface/.venv/lib/python3.12/site-packages/smolagents/agents.py\", line 558, in run result = self.step(step_log) ^^^^^^^^^^^^^^^^^^^ File \"/Users/rajivmehtapy/Desktop/LLMOps/KB-Test/gr-interface/.venv/lib/python3.12/site-packages/smolagents/agents.py\", line 960, in step raise AgentGenerationError(f\"Error in generating model output:\\n{e}\", self.logger) from e smolagents.utils.AgentGenerationError: Error in generating model output: 'str' object has no attribute 'save'\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0f607853-9902-4c8e-a583-7a95091fe94f", "embedding": null, "metadata": {"issue_id": 352, "title": "tried running the tutorial openai code and returns an error", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62d7bcfe-997a-4ae8-aa31-2b71d6675c9c", "node_type": "4", "metadata": {"issue_id": 352, "title": "tried running the tutorial openai code and returns an error", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "hash": "c64061614762b73d79e6e3a0b7a421c601c16cf9e3117ccb68a91a4352e6b9c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: tried running the tutorial openai code and returns an error\n\nDescription: /usr/local/lib/python3.11/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2: * 'fields' has been removed warnings.warn(message, UserWarning) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 Could you give me the 118th number in the Fibonacci sequence? \u2502 \u2502 \u2502 \u2570\u2500 LiteLLMModel - openai/o1-mini \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 0 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'. Error in generating model output: litellm.APIError: APIError: OpenAIException - Error code: 500 - {'error': {'message': 'The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.', 'type': 'model_error', 'param': None, 'code': None}} [Step 0: Duration 9.50 seconds]\n\nState: closed\n\nLabels: duplicate\n\nCategories: category-duplicate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b83527fc-4854-4160-9972-1b2076173597", "embedding": null, "metadata": {"issue_id": 351, "title": "pip install smolagents not working", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "831de48c-eb98-4812-af0c-35150d5fb809", "node_type": "4", "metadata": {"issue_id": 351, "title": "pip install smolagents not working", "state": "closed", "labels": [], "type": "issue"}, "hash": "318bc1afcac023294e7ab8eba5a3b446b981541fbe65d7936746e330bc6bfe9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: pip install smolagents not working\n\nDescription: ERROR: Could not find a version that satisfies the requirement smolagents (from versions: none) ERROR: No matching distribution found for smolagents does anyone have this issue?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ec181f3-983f-44d6-8d2a-67270981b24b", "embedding": null, "metadata": {"issue_id": 350, "title": "[Feature Request] Remote MCP tools", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8247043f-2bdc-4fac-94d8-e37057a5fde3", "node_type": "4", "metadata": {"issue_id": 350, "title": "[Feature Request] Remote MCP tools", "state": "closed", "labels": [], "type": "issue"}, "hash": "0fe61fd3897c90e48143621f1679242e4019d1adc0175a98bd857e247ccedb96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Feature Request] Remote MCP tools\n\nDescription: The MCP standard is protocol-agnostic and can be built upon any transport protocol. Currently, only local MCP tools are supported by providing an StdioServerParameters.In particular, SSE support comes ootb. I think it would be great if we could have a higher level implementation of MCP tools to allow for more flexibility. Currently the way I see it is we could make .from_mcp() accept an MCP client from the official Python SDK (or maybe just make it an abstract class if people want to use their custom implementation). Then the ToolCollection would handle tool discovery and calling through the client and provide the tools to the agent the standard way. The only problem would maybe be the async nature of MCP tool calls, but that could be handled the same way MCPAdapt does it, or through proper support of async calls (https://github.com/huggingface/smolagents/issues/145).\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a3228640-43d2-4517-adbd-a85192266c0e", "embedding": null, "metadata": {"issue_id": 346, "title": "Explicit agent routing", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ce84a99-b85c-4954-912b-713413033466", "node_type": "4", "metadata": {"issue_id": 346, "title": "Explicit agent routing", "state": "closed", "labels": [], "type": "issue"}, "hash": "b1403ba0fad50e883e370408da7b42c8b5ef27c94353cac06fbd2bed72bea364", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Explicit agent routing\n\nDescription: This is perhaps more of a question than anything else. Clearly smolagents can do multi-agent orchestration. However, it's not obvious to me if smolagents supports explicit agent routing. I.e., instead of some \"manager\" or autonomous system deciding what agent or tool is called next, I want to be able to define a precise workflow akin to Langgraph (for example). Is this currently possible in smolagents, and if not, is there an appetite to implement this feature? I would be happy to dig into it. Edit: now that I think of it, I think that once 316 is merged, this will be more straightforward to implement too. Once an agent is \"done\" it could simply pass its task list to another agent? All that we'd need is a pattern for that design (perhaps similar to LangGraph). Thoughts?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9747e9e9-7e9e-4dc7-a3ed-3c842d59250a", "embedding": null, "metadata": {"issue_id": 343, "title": "Handling Additional Arguments for Tools in a Multi-Agent System", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0b77987-92dc-43e9-bfe9-f917ce85c5ca", "node_type": "4", "metadata": {"issue_id": 343, "title": "Handling Additional Arguments for Tools in a Multi-Agent System", "state": "open", "labels": [], "type": "issue"}, "hash": "09a64c481ce3d641d6800beba2dae0ee7fa93dce0934a1ff07a6c459601706ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Handling Additional Arguments for Tools in a Multi-Agent System\n\nDescription: In a multi-agent system, how can we handle cases where a tool requires additional arguments to function correctly? If using a single CodeAgent, you can do agent.run(input, additional_args={...}. However this seems not to work with an agent that uses multiple manged_agents. Example Scenario: The metadata_retriever_agent uses the retrieve_metadata tool, which requires a dedicated retriever argument to perform its operations. Below is a code snippet illustrating the setup: [CODE_BLOCK] Problem: The retrieve_metadata tool within the metadata_retriever_agent requires a retriever argument. However, it\u2019s unclear how to pass this argument effectively in the current multi-agent system, especially when the tool is encapsulated within a managed agent. Question: What is the best practice for passing required arguments (like retriever) to tools in such a system? Are there recommended patterns or mechanisms to streamline this process?\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "68240108-6c24-44af-9e8f-b7922272b82b", "embedding": null, "metadata": {"issue_id": 340, "title": "Use linting to detect code errors", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "877f8c3e-86db-4f23-8f61-c424364cc7aa", "node_type": "4", "metadata": {"issue_id": 340, "title": "Use linting to detect code errors", "state": "open", "labels": [], "type": "issue"}, "hash": "4fa64458d9df8e938bb5722511c32051f7c8211d4f1b8910dc5962a972e69299", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Use linting to detect code errors\n\nDescription: It might be a good idea to use linting in ci to catch errors early. Common tools used are pylint and mypy. You could ignore all of pylint's Python style recommendations and just look for errors. e.g.: [CODE_BLOCK] And you get a few potential errors (I filtered some out): [CODE_BLOCK] The assignment-from-no-return stem from the step function not being declared as abstractmethod (you might argue that as a style). In the following code pylint says observation_name might be used before assignment because observation_type could possibly neither be AgentImage nor AgentAudio: [CODE_BLOCK] And in parse_json_tool_call the [CODE_BLOCK] mypy also shows a lot of warnings, some may just be incorrect type hints (which is hard to always get right if not automatically checked). Not sure if any of those indicate other coding errors. I found adding mypy (or any linting) early can make it really much easier than adding it later on. (Over the time I also personally warmed to many of the style recommendations)\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1072, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "12b1a614-f6fd-4f10-81b8-e80face97bdb", "embedding": null, "metadata": {"issue_id": 339, "title": "No module named \"_gdbm\"", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d1eb232-b9cd-45ad-8630-2e8135580dd2", "node_type": "4", "metadata": {"issue_id": 339, "title": "No module named \"_gdbm\"", "state": "closed", "labels": [], "type": "issue"}, "hash": "93524b9329d104c866a7a09a99f40771e8d742a299bf8e495bedd5c7e5927874", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: No module named \"_gdbm\"\n\nDescription: Code: qwen_model = HfApiModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\", token = hf_token) data_analyst_agent_qwen = CodeAgent( tools=[], model=qwen_model, additional_authorized_imports=[\"numpy\", \"pandas\", \"matplotlib.pyplot\", \"seaborn\"], max_steps=10, ) data_analyst_agent_qwen.run(data_analyst_prompt, additional_args = dict(source_file_path = input_file_path)) Result: Error when smolagent is executing the following code during one of the steps. This code is generated by the llm during step 1. The following code runs without any error when I run it in a separate notebook. Logs are not showing any additional information. Error message: No module named \"_gdbm\". The error seems to be generated by the code for the plots. Agent works without any issues if I \"EXCLUDE\" the requirement about generating charts. import matplotlib.pyplot as plt import seaborn as sns plt.figure(figsize=(15, 10)) plt.subplot(2, 2, 1) sns.boxplot(x='Product_3', y='Stroke_Length', data=df) plt.title('Distribution of Stroke_Length by Product_3') plt.xticks(rotation=45) plt.subplot(2, 2, 2) sns.boxplot(x='Cylinder_Function', y='Closed_Length_mm', data=df) plt.title('Distribution of Closed_Length_mm by Cylinder_Function') plt.xticks(rotation=90) plt.subplot(2, 2, 3) sns.boxplot(x='Type_2', y='Attributes_Rod_ID', data=df) plt.title('Distribution of Attributes_Rod_ID by Type_2') plt.subplot(2, 2, 4) sns.boxplot(x='Type_2', y='Attributes_Internal_Bore', data=df) plt.title('Distribution of Attributes_Internal_Bore by Type_2') plt.tight_layout() plt.show() question1 = \"What is the average Stroke_Length for each Product_3 category?\" question2 = \"What is the maximum Closed_Length_mm for each Cylinder_Function category?\" question3 = \"What is the average Attributes_Rod_ID for each Type_2 category?\" answer1 = df.groupby('Product_3')['Stroke_Length'].mean() answer2 = df.groupby('Cylinder_Function')['Closed_Length_mm'].max() answer3 = df.groupby('Type_2')['Attributes_Rod_ID'].mean() print(\"Question 1:\", question1) print(\"Answer 1:\", answer1) print(\"\\nQuestion 2:\", question2) print(\"Answer 2:\", answer2) print(\"\\nQuestion 3:\", question3) print(\"Answer 3:\", answer3)\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d921090e-cfb4-4920-8873-ef34fe507684", "embedding": null, "metadata": {"issue_id": 338, "title": "[BUG] LLM generated correct code but execution returned unexpected error message", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "22d32470-8dec-4cba-8710-cf81fbb4c4c0", "node_type": "4", "metadata": {"issue_id": 338, "title": "[BUG] LLM generated correct code but execution returned unexpected error message", "state": "open", "labels": [], "type": "issue"}, "hash": "6dff03508a12c3ce14532ce1f20ce4467fe1f81d13d8b99acb1f7a4b7a6de554", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [BUG] LLM generated correct code but execution returned unexpected error message\n\nDescription: I'm doing text to sql and this is the tool I defined: <img width=\"672\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/b25c2980-7099-4ad6-a94d-b4ab8dab2bab\" /> I initialized an agent with qwen2.5 and ran a simple query agent = CodeAgent( tools=[db_query_tool], model=HfApiModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\"), ) agent.run(\"how many albums are in the Album table?\") The agent generated the correct code, but the execution of the code returned an error message: <img width=\"964\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d9c4da44-16ea-4406-9dcf-97534f361576\" /> I mean if we just take the code out and run it, it is supposed to execute without error and print out 347. Because album_count is [(347,)], album_count[0] is (347,), and running album_count[0] should not give any error. So why is the code not able to be executed by the agent?\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 985, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c53206e-f987-4af3-bf85-966f68dfadce", "embedding": null, "metadata": {"issue_id": 334, "title": "Async tool support", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60551e2d-0254-47ae-8a38-b380c671cf5a", "node_type": "4", "metadata": {"issue_id": 334, "title": "Async tool support", "state": "open", "labels": [], "type": "issue"}, "hash": "3178263ee57ac595e3ae807a5b552352d97a1681b816eeef27f1d387e5f5c0d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Async tool support\n\nDescription: Are async functions currently unsupported, or is there a workaround for tools that rely on asynchronous behaviour? For example, I have a tool that retrieves data asynchronously from various web resources, and the agent needs to await these responses: [CODE_BLOCK] Attempting to use this tool results in the following error: [CODE_BLOCK] Is there a recommended approach to handle asynchronous tools in such scenarios? Any guidance would be greatly appreciated.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1cef2dd0-a473-43dc-b417-d5ac568f5519", "embedding": null, "metadata": {"issue_id": 333, "title": "Invalid type error on every message", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c624b3d-923f-444b-994d-5cca82cf4b0f", "node_type": "4", "metadata": {"issue_id": 333, "title": "Invalid type error on every message", "state": "closed", "labels": [], "type": "issue"}, "hash": "aa7c1a25278e0418f6738bd18c2853b28c1fe5a0ea6deb2213db6cea2ad02e6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Invalid type error on every message\n\nDescription: With simplest example using CodeAgent I'm getting the following error on every agent message: [CODE_BLOCK] [CODE_BLOCK] **Python** 3.12.8 **smolagents** v1.4.1\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73a650f6-ef8b-4294-ba9e-3e519e049d9c", "embedding": null, "metadata": {"issue_id": 331, "title": "Telemetry does not work on main", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "629f9131-735f-4dcc-943e-f3943720558e", "node_type": "4", "metadata": {"issue_id": 331, "title": "Telemetry does not work on main", "state": "closed", "labels": [], "type": "issue"}, "hash": "ca36c8bb7c6fc2e0dcb3fa794b157f5bd31f125ff33ca57d0bd2372e9b598c80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Telemetry does not work on main\n\nDescription: 2 things need to be done: - first, imports were changed between 1.5.0 and before, so telemetry calls needs to be updated: https://github.com/Arize-ai/openinference/pull/1229 - second, using the dev tag (when installing from main) makes openinference's requirement fail with DependencyConflict: requested: \"smolagents >= 1.2.2\" but found: \"smolagents 1.5.0.dev0\" - either this comparison operation needs to be fixed in openinference, or the dev versions of smolagents need to have another naming convention Linked to Arize-ai/openinference/pull/1184\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d176b3a0-1760-4e04-971e-dfcf50d19205", "embedding": null, "metadata": {"issue_id": 326, "title": "Ask agent to do a daily job, it stuck in a loop", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a821efa-bf97-4669-be68-d82519ce2de4", "node_type": "4", "metadata": {"issue_id": 326, "title": "Ask agent to do a daily job, it stuck in a loop", "state": "open", "labels": [], "type": "issue"}, "hash": "5d83198e83b821db37d82acd22949cd707aeda6d40693c378f44e1254c3cd78a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Ask agent to do a daily job, it stuck in a loop\n\nDescription: When requesting the agent to perform a daily task, such as ping me every day at 9:00, the agent ends up in an infinite loop. This issue arises because the generated code contains a while True {} statement, preventing it from returning at all. The reproduction code is following: [CODE_BLOCK] And the output looks like: <img width=\"1422\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/42f88247-02d2-4d1c-bfd2-73eb679db3b5\" />\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4a38388-6782-41bf-8338-55b115bb15fa", "embedding": null, "metadata": {"issue_id": 323, "title": "question: Why use self.input_messages as instance variable in agent methods?", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b95e198-d034-4552-b1a7-07e65d474be7", "node_type": "4", "metadata": {"issue_id": 323, "title": "question: Why use self.input_messages as instance variable in agent methods?", "state": "open", "labels": [], "type": "issue"}, "hash": "63d960efc3a24c1f9ff1a26ec3fb1ea4d08753fe0e4dd8a9df0732831e5665db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: question: Why use self.input_messages as instance variable in agent methods?\n\nDescription: I have noticed a similar issue in several places within the agent module. Before calling the model, the constructed input_messages is assigned to self.input_messages. However, it seems that self.input_messages is only used when calling self.model, and not utilized elsewhere. Wouldn't it be simpler to create a local variable input_messages instead? Is there a specific design consideration behind this choice? https://github.com/huggingface/smolagents/blob/fe2f4e735caae669949dae31905756ad70fcf63e/src/smolagents/agents.pyL345 [CODE_BLOCK]\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 651, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "657643f9-b341-4318-8a9d-acf75505c5bc", "embedding": null, "metadata": {"issue_id": 322, "title": "How to capture CodeAgent's full thinking including the code, not just the final response into a variable", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c26df50c-012a-4e47-b417-6771300a7d4f", "node_type": "4", "metadata": {"issue_id": 322, "title": "How to capture CodeAgent's full thinking including the code, not just the final response into a variable", "state": "open", "labels": [], "type": "issue"}, "hash": "b7462b295108fb6933b9e281ce1714997208686b2d9e1ccb0c9163988c6f5f42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to capture CodeAgent's full thinking including the code, not just the final response into a variable\n\nDescription: When we run a CodeAgent in a notebook, it print the question/task, the LLM model used, code (Executing this code, Execution logs) and the Final answer. The return value from agent.run contrains only the final response. I'm working on some demos for which I wanted to run a number of tasks, capture all the output (not just the final answer) and write them to an md or html file, so that I can show everything including the code generated by the agent without running the agents live in the demo. I tried logging, stdout, from contextlib import redirect_stdout, etc but couldn't capture the full output to a variable. Thanks,\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2fa4ab96-4aa0-4406-8c85-f41609f03e0b", "embedding": null, "metadata": {"issue_id": 312, "title": "how to exec a bin and use the output as agent arg ?", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dce4757d-60e8-4254-bba1-a3af987211c3", "node_type": "4", "metadata": {"issue_id": 312, "title": "how to exec a bin and use the output as agent arg ?", "state": "open", "labels": [], "type": "issue"}, "hash": "97ae82911c3c7bf1d00ee9c63f621713d7a36005c646230793473c4b851dc72e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: how to exec a bin and use the output as agent arg ?\n\nDescription: hi a simple exec tool as exec(path,[args]) should be in examples. then an agent call as \"use exec(/bin/ls,/bin)\" put the result in sql db \"(as bin-name) for later use and tell me how much of them are scripts while using sbx -z on each non-scripts\" as a short example\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5922a96f-e62f-4b7e-a8f7-528583db6845", "embedding": null, "metadata": {"issue_id": 310, "title": "Feat: usage of Gradio `Tool.from_gradio` based on URL", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b388fa2-4bc3-4e12-a2d9-c32df65eb496", "node_type": "4", "metadata": {"issue_id": 310, "title": "Feat: usage of Gradio `Tool.from_gradio` based on URL", "state": "open", "labels": [], "type": "issue"}, "hash": "1701e1d698152b388e7ad46875ff55eca30e0417a4323b0c21b53a7c84e46a14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feat: usage of Gradio `Tool.from_gradio` based on URL\n\nDescription: I think it could be useful to allow initialising a Gradio tool using a direct URL. This would allow people to use Gradio tools that are hosted outside of the Hub. We could potentially also explore if this plays together well with Tool.from_spaces where we internally use gr.load(\"gradio/question-answering\", src=\"spaces\").launch() to launch a local version of the Space of choice.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf5966cf-5156-46c0-96c2-e3af187edd77", "embedding": null, "metadata": {"issue_id": 307, "title": "In smolagents1.4.1", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a9a96cb-5ee8-45d6-a5a1-d37b95391a17", "node_type": "4", "metadata": {"issue_id": 307, "title": "In smolagents1.4.1", "state": "closed", "labels": [], "type": "issue"}, "hash": "bec25954ff8e551b2c8904ebb01547c8172952767e02a46924ca306158bdcfe7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: In smolagents1.4.1\n\nDescription: ImportError: cannot import name 'define_import_structure' from 'transformers.utils.import_utils'\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c4b6956-3e1b-4eeb-be76-c523ecb9136d", "embedding": null, "metadata": {"issue_id": 304, "title": "Output format? `{\"answer\": \"insert your final answer here\"}` or `\"insert your final answer here\"` ?", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7486ba2b-df7a-425a-a09f-8bc6529b525e", "node_type": "4", "metadata": {"issue_id": 304, "title": "Output format? `{\"answer\": \"insert your final answer here\"}` or `\"insert your final answer here\"` ?", "state": "open", "labels": [], "type": "issue"}, "hash": "6dec98945aab78d4a57e5afaf7d4e4bfc711219951d109d35712fd58266ed412", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Output format? `{\"answer\": \"insert your final answer here\"}` or `\"insert your final answer here\"` ?\n\nDescription: Hello and congrats for your job. The output format seems to vary. These are examples from the constant TOOL_CALLING_SYSTEM_PROMPT : [CODE_BLOCK] ... [CODE_BLOCK] In the first example, the result is {\"answer\": \"insert your final answer here\"}. In the last, it is \"1302.678\". Just to share food for thought, I escaped the problem with: [CODE_BLOCK] And then, get_result_str can be used with: [CODE_BLOCK] In the case that you would like me to to try to code something, let me know. Live long and prosper.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 636, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c21e670-e68e-4878-aea7-b57b718b153f", "embedding": null, "metadata": {"issue_id": 302, "title": "System Message is doesnt seem to be sent to LLM when using LiteLLMModel", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aab7324-b17e-4964-9790-a0d65624db67", "node_type": "4", "metadata": {"issue_id": 302, "title": "System Message is doesnt seem to be sent to LLM when using LiteLLMModel", "state": "closed", "labels": [], "type": "issue"}, "hash": "cfefd1af172bdfbf95a7e4edbc9e542436f097d32035e8dae2b9ef2b7790c585", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: System Message is doesnt seem to be sent to LLM when using LiteLLMModel\n\nDescription: Version: 1.4.1 I'm using the code form the documentation for \"leopard-Pont des Arts\" question. When using the LiteLLMModel with ollama and Qwen2.5-Instruct the system message with the instructions about code calling dont seem to be acually used in the LLM. I made several experiments and the answers that I can inspect in the Phoenex are always exactly the same as if I just ask the task question directly in ollama console without any instructions. If I manually copy paste the system message I get the expected response with python code. I can not investigate this any further. [CODE_BLOCK] > I'm also getting this errors. I tried with LiteLLMMode connected to ollama and tabbyAPI with different quantisations of Qwen2.5-Instruct and DeepseekR1. Judging from observing the \"Trace Details\" in Pheonix the in system message (\"You are an expert assistant who can solve any task using code blobs...\") doesn't seem to be sent to the LLM. At least the answer makes me believe that. > > Example: > > [CODE_BLOCK] > > When i copy paste the system message with the task in ollama directly i get a proper response > > [CODE_BLOCK]py > bridge_length_result = web_search(query=\"length of Pont des Arts\") > print(\"Bridge length result:\", bridge_length_result) > > leopard_speed_result = web_search(query=\"maximum speed of a leopard\") > print(\"Leopard speed result:\", leopard_speed_result) > [CODE_BLOCK] _Originally posted by @dadaphl in 201_\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1539, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5b8a264-1ec8-434d-9c28-bdab668267b0", "embedding": null, "metadata": {"issue_id": 298, "title": "How to pass images as input to CodeAgent?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fb59c471-df2c-4ec8-b1f1-6ae9f3e5b466", "node_type": "4", "metadata": {"issue_id": 298, "title": "How to pass images as input to CodeAgent?", "state": "closed", "labels": [], "type": "issue"}, "hash": "b6a493c782a7aafff351820855ae0c1f59a15546a77fe00260c769fd50c4c83f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to pass images as input to CodeAgent?\n\nDescription: Hello, I want to pass an input image along with the prompt to CodeAgent.run. I see that there is an additional_args argument but when I pass the image as {\"image\": \"path/to/image.png\"}, the agent ends up loading the image via pytesseract to read the contents of the image instead of passing it to OpenAI/Anthropic directly. Is there any way that I can ensure that the image is passed along with the prompt so that the model can infer information from it instead of using external libraries to load the image when using the LiteLLM integration? My code for reference: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 657, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6eaaeb60-e037-433b-b9f7-69ecef2d5992", "embedding": null, "metadata": {"issue_id": 294, "title": "Tool Calling not working - parameters not set correct", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb990123-6dd7-4fca-a3cc-261598f7087b", "node_type": "4", "metadata": {"issue_id": 294, "title": "Tool Calling not working - parameters not set correct", "state": "closed", "labels": [], "type": "issue"}, "hash": "2712d18d225ec57d0866ffe454f4b1ab77bce33f945402349f3d6af2d2bf0f4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Tool Calling not working - parameters not set correct\n\nDescription: Hi there, thanks for providing Smolagents. I wanted to have a first contact with Smolagents. Looks very promising and easy to use, so I gave it a try. I was picking an given example to get first insights. **Example: tool_calling_agent_from_any_llm.py** - Picking the given example from the examples directory - Set it up to run with gpt4o on azure - Run and checked the output --> cool, works **Stepped into the tool call with a breakpoint to check the function parameters** Parameters seems to be wrong 1. Set a breakpoint in get_weather to see the parameters 2. Run the sample in debugger 3. Checking parameters: location = '{\"location\":\"Paris\"}', celsius = False There seems to be no parsing of the parameters upfront the call of the function. I would expect location=\"Paris\" **Modified the example to get a second parameter** print(agent.run(\"What's the weather like in Paris? Please tell me in Celsius.\")) Same test. Checked the values: location = '{\"location\":\"Paris\",\"celsius\":true}' celsius = False** **Observation** The LLM was able to identify the \"location = Paris\" and need for \"celsius = True\". But the tool calling does not work as it should. There is an issue with parsing the parameters and mapped to the function call. Am I doing wrong? Would be great if somebody could check that. Thanks for your support\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1412, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f3dcceed-1adb-4b4b-9848-cd2c02b10ee4", "embedding": null, "metadata": {"issue_id": 291, "title": "Import of matplotlib.pyplot is not allowed.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a33c7700-ce99-4e3a-87be-35f85e96781b", "node_type": "4", "metadata": {"issue_id": 291, "title": "Import of matplotlib.pyplot is not allowed.", "state": "closed", "labels": [], "type": "issue"}, "hash": "f0513ea84436bd8ca17745656615fb2d8ccf034e30334fec50a1258dc3f6905b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Import of matplotlib.pyplot is not allowed.\n\nDescription: It would be nice if you could add this import so the plot outputted can be saved as an image for example. This could turn the code Agent into a data visualisation Agent.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ece2c1a7-7f4a-416d-ab73-e93a4489d3c1", "embedding": null, "metadata": {"issue_id": 289, "title": "Installation is broken: ModuleNotFoundError: No module named 'huggingface_hub'", "state": "closed", "labels": ["bug"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07aa0e2d-18a7-41ee-8b0a-bfb3124db8a1", "node_type": "4", "metadata": {"issue_id": 289, "title": "Installation is broken: ModuleNotFoundError: No module named 'huggingface_hub'", "state": "closed", "labels": ["bug"], "type": "issue"}, "hash": "4efd8b0e6cc26c3274583a47b1ddaea065085de61cdf278d1610f6bfc71ec78e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Installation is broken: ModuleNotFoundError: No module named 'huggingface_hub'\n\nDescription: Installation is broken: [CODE_BLOCK] raises [CODE_BLOCK]\n\nState: closed\n\nLabels: bug\n\nCategories: category-bug\n\nType: Bug report or error", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 237, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0bac670-2673-465c-8e26-11692c620a6b", "embedding": null, "metadata": {"issue_id": 278, "title": "the most basic steps throws a circular import error", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "366a3350-b6c0-4b5f-8b62-f5e23d44ced9", "node_type": "4", "metadata": {"issue_id": 278, "title": "the most basic steps throws a circular import error", "state": "closed", "labels": [], "type": "issue"}, "hash": "725526ce915ac65a59d22e424d742446c1d66cbc42cd566d10ffb768650128a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: the most basic steps throws a circular import error\n\nDescription: ImportError: cannot import name 'CodeAgent' from partially initialized module 'smolagents' (most likely due to a circular import) [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "79e3106b-84b4-46e7-a3a6-56ed7f774680", "embedding": null, "metadata": {"issue_id": 276, "title": "Add Azure OpenAI support", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f11685d5-3a3a-4c54-ae9f-2aba41ca642a", "node_type": "4", "metadata": {"issue_id": 276, "title": "Add Azure OpenAI support", "state": "closed", "labels": [], "type": "issue"}, "hash": "25ada188603d9fea1727c647379d1c9889809fcde44b53cfec36f0932cfb85d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add Azure OpenAI support\n\nDescription: I'd very much love to have native Azure OpenAI support for smolagents. I understand that writing it is easy, but you may not want the burden of **maintaining** it indefinitely and you're trying to keep the surface area smol until the featureset becomes more complete and things are fleshed out. I've seen PR 161 . I'd like to propose a simpler approach. Since smolagents already has an OpenAIServerModel implementation, this means we can subclass it and get all its current and future functionality for free, all we need to do is replace the openai client with the Azure flavor. So the burden for maintaining this class in the future is dramatically decreased. Would the implementation beow be acceptable for you guys? If yes, I can send a PR. If not, I understand the reasons :). [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 854, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd86195d-6483-443c-b3e3-95515152435b", "embedding": null, "metadata": {"issue_id": 269, "title": "Incorrect Argument Handling in ToolCallingAgent", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "247263aa-7799-4ec2-936a-00f0d6e7593f", "node_type": "4", "metadata": {"issue_id": 269, "title": "Incorrect Argument Handling in ToolCallingAgent", "state": "closed", "labels": [], "type": "issue"}, "hash": "8f3d82217dd4a26da190e46eb77c61adf789b871feda57459cfeada1c139e8d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Incorrect Argument Handling in ToolCallingAgent\n\nDescription: **Issue Description** When running the example code from this smolagents example, the ToolCallingAgent doesn\u2019t handle tool arguments correctly. The arguments seem to be passed as a dictionary-like object, causing the tool to produce unexpected output. **Code to Reproduce** Here\u2019s a slightly modified version of the example: [CODE_BLOCK] **Observed Output** Here\u2019s the Observations I got: Calling tool: 'get_weather' with arguments: {\"location\":\"Paris\",\"celsius\":true} Observations: The weather in {\"location\":\"Paris\",\"celsius\":true} UNGODLY with torrential rains and temperatures below 14\u00b0F **Expected Output** Observations: The weather in Paris is UNGODLY with torrential rains and temperatures below -10\u00b0C **Additional Notes** Using CodeAgent instead of ToolCallingAgent works as expected: \u2500 Executing this code: weather_paris = get_weather(location=\"Paris\", celsius=True) final_answer(weather_paris) The weather in Paris is UNGODLY with torrential rains and temperatures below -10\u00b0C I\u2019ve tried tweaking the tool\u2019s arguments, as well as creating tools with Tool directly, but the issue persists. **Environment** smolagents version: 1.4.1 Python version: 3.12.7\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2cb02add-aa65-41f6-83c1-1a55b80fffdf", "embedding": null, "metadata": {"issue_id": 264, "title": "Expected tool format of LiteLLM for ollama is not followed by Smollagent example Tools", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "019f1ab5-15b0-4b15-893d-459ebc402b92", "node_type": "4", "metadata": {"issue_id": 264, "title": "Expected tool format of LiteLLM for ollama is not followed by Smollagent example Tools", "state": "open", "labels": [], "type": "issue"}, "hash": "fb6d4b3ecc30b1d756cf4f04eff640324bbaac34a5d4a750d69cd51d881f402b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Expected tool format of LiteLLM for ollama is not followed by Smollagent example Tools\n\nDescription: I tried to follow this tutorial with ollama models: https://huggingface.co/docs/smolagents/tutorials/inspect_runs This the error tracked by phoenix: [CODE_BLOCK] This is what is happening in transformation.py arround line 264: [CODE_BLOCK] Lite LLM tries to use keys 'name' and 'argument' unfortunately 'function_call ' contains response: [CODE_BLOCK] Which does not match the keys. I had a look at smollagents prompts.py, which has examples like: [CODE_BLOCK] So please fix the example tools, to match the LiteLLM 'litellm/llms/ollama/completion/transformation.py' implementation.\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "331a90bc-105d-455c-be3b-1bd0ef5deb6a", "embedding": null, "metadata": {"issue_id": 262, "title": "Dockerfile not working", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28465c89-f9e1-42b5-8a18-286a9b8682aa", "node_type": "4", "metadata": {"issue_id": 262, "title": "Dockerfile not working", "state": "closed", "labels": ["duplicate"], "type": "issue"}, "hash": "1e8df4be7fd10a0c273de7d88afac629f0b70ed26db794d3ccc5a362da968edc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Dockerfile not working\n\nDescription: server.py not exist, so the build can't work. could you provide the correct way ? i 'm starting a PR about docker on smolagent and be happy to help on this side.\n\nState: closed\n\nLabels: duplicate\n\nCategories: category-duplicate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 271, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7096591-62b4-466a-acb0-07c5ea3cefbb", "embedding": null, "metadata": {"issue_id": 258, "title": "AttributeError: 'CodeAgent' object has no attribute 'logger'", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "883ed1db-09d7-45eb-9065-937bd210ef66", "node_type": "4", "metadata": {"issue_id": 258, "title": "AttributeError: 'CodeAgent' object has no attribute 'logger'", "state": "closed", "labels": [], "type": "issue"}, "hash": "b90d743172d11266c029229dc533a2f412d3912fc71b5316e58ca5b10b85b60d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: AttributeError: 'CodeAgent' object has no attribute 'logger'\n\nDescription: Hello, I got this error while creating a CodeAgent: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 161, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5953e9a0-ae44-4f49-abba-dfb7c476ee59", "embedding": null, "metadata": {"issue_id": 257, "title": "Missing Arguments for Single-Agent Tools with Multiple Parameters", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fac6a1fd-5395-4ab2-a263-4bc91d0c906e", "node_type": "4", "metadata": {"issue_id": 257, "title": "Missing Arguments for Single-Agent Tools with Multiple Parameters", "state": "closed", "labels": [], "type": "issue"}, "hash": "f63179aa5807975794a1f96c5e48d137a1f4b71ffb500e7cd5ebe311c1d4cbf0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Missing Arguments for Single-Agent Tools with Multiple Parameters\n\nDescription: When creating a agent tool with or without using decorators, any tool that requires more than one argument fails to execute properly. The additional arguments are considered \"missing\" and are not passed to the __call__ method. This issue arises when the arguments are passed as a dictionary with multiple key-value pairs. ; ^ ; [CODE_BLOCK] >\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 Calling tool: 'get_weather' with arguments: {\"location\":\"Paris\",\"celsius\":\"5\"} \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Error in tool call execution: get_weather() missing 1 required positional argument: 'celsius' You should only use this tool with a correct input. As a reminder, this tool's description is the following: >get_weather: Get weather in the next days at given location. Secretly this tool does not care about the location, it hates the weather everywhere. Takes inputs: {'location': {'type': 'string', 'description': 'the location'}, 'celsius': {'type': 'string', 'description': 'the temperature'}} Returns an output of type: string [Step 5: Duration 0.43 seconds| Input tokens: 9,742 | Output tokens: 114] Reached max steps. !Image\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0851d9f2-e2d0-4603-a635-178a1439938e", "embedding": null, "metadata": {"issue_id": 251, "title": "Initialize Agent with prior conversations", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a53ee716-469a-486c-a6bf-59979f0eeb88", "node_type": "4", "metadata": {"issue_id": 251, "title": "Initialize Agent with prior conversations", "state": "closed", "labels": [], "type": "issue"}, "hash": "8ca2a9c92320b66e6b5039171813fbdc0cb98829725ca1ebf15ac2cc7ddfa5ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Initialize Agent with prior conversations\n\nDescription: Is there anyway to pass a list of prior messages like this when I create the agent? I want the agent to pick up where the user left from yesterday or something along the line. I saw there is an option to write to memory from logs. [CODE_BLOCK] Something like this would do. ``` agent = CodeAgent( tools=[sql_engine], model=HfApiModel(\"meta-llama/Meta-Llama-3.1-8B-Instruct\"), memory_to_initialize=messages does this exit now? )\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 505, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d97a6a6-ed82-4b91-b9bf-fb6d6d73b881", "embedding": null, "metadata": {"issue_id": 250, "title": "tool.push_to_hub does not work in .ipynb files", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39fd822c-fdd1-4f0b-9025-ce5c7eac7463", "node_type": "4", "metadata": {"issue_id": 250, "title": "tool.push_to_hub does not work in .ipynb files", "state": "closed", "labels": [], "type": "issue"}, "hash": "7c60a29fb390a5fc17fdd703b83657fb25406de302991d7e4c8449725d80a5f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: tool.push_to_hub does not work in .ipynb files\n\nDescription: When I use the code snippets from the tools docs page in a .ipynb notebook, I get the following error: [CODE_BLOCK] Error: [CODE_BLOCK] The issue is probably, that when defining classes in Jupyter notebooks (the __main__ module), Python doesn't maintain the source code in a way that can be accessed through inspect.getsource(). The same code does work correctly when I run it in a .py file. To be able to push a tool to the hub from a .ipynb script, users first need to create a .py file that contains the module, then import the class from the module and then they can push it to to the hub: [CODE_BLOCK] Not sure if there is an easy solution to make this work in .ipynb more natively without going through a .py file. An intermediate solution might be to add a short bullet point about this in the tools docs page after For the push to Hub to work, your tool will need to respect some rules: ... --- Env: smolagents==1.3.0 cursor IDE\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ff3f6413-dd52-47e4-9689-cec3d5806444", "embedding": null, "metadata": {"issue_id": 249, "title": "Missing translation guide", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "994f2d2e-d81c-4e38-a78f-6ab7de2b2e5d", "node_type": "4", "metadata": {"issue_id": 249, "title": "Missing translation guide", "state": "closed", "labels": [], "type": "issue"}, "hash": "ebd649c1aecca4e6bdcaf80669f3d6194ae9d80c77ee80c20b244f0ac2573eb1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Missing translation guide\n\nDescription: This https://github.com/huggingface/smolagents/blob/main/docs/TRANSLATING.md is missing. Any suggestions on how to fix this?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 186, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e3e0806-3443-4448-965a-68c378d44f46", "embedding": null, "metadata": {"issue_id": 248, "title": "A GitHub Action for automated package release", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82f71bc4-b803-451a-9a1c-a7c0ba73dd07", "node_type": "4", "metadata": {"issue_id": 248, "title": "A GitHub Action for automated package release", "state": "open", "labels": [], "type": "issue"}, "hash": "efcbdf33b38b01a8c3a0c7af514dcd9d4d7603b16263d5c6b18087f18b744a0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: A GitHub Action for automated package release\n\nDescription: Thanks for this amazing tool! I looked around the yaml files of the workflow directory and didn't see any automated way of releasing the package with PyPI and also uploading source codes to GitHub. Do you want me to work on this so that every time, we can just triger new release with git tag vx.x.x. Please correct me if I missed anything. @aymeric-roucher\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a65d0777-b1aa-45df-8309-d1f0093ad858", "embedding": null, "metadata": {"issue_id": 244, "title": "use_e2b_executor=True does not seem to work", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "baa985c3-bc95-41dc-9745-30cfd025ea29", "node_type": "4", "metadata": {"issue_id": 244, "title": "use_e2b_executor=True does not seem to work", "state": "closed", "labels": [], "type": "issue"}, "hash": "087e279827684a9ae86368571c490357ae33947388087c33f76c17a0e58ea7be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6b1ae45-3074-4538-8002-1f793c19b1f7", "node_type": "1", "metadata": {}, "hash": "0a16436a4880e379ce5362c229f844cfcba3c217f0fe0b5c001b9e2244803887", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: use_e2b_executor=True does not seem to work\n\nDescription: I'm trying to run the e2b example from the docs: [CODE_BLOCK] This throws the following error: [CODE_BLOCK] And adding the use_e2b_executor=True flag in a different snipped leads to a loop that seems to successfully write code, but every code run throws not enough values to unpack (expected 3, got 2), which seems unrelated to the code produced by the agent: [CODE_BLOCK] Output: [CODE_BLOCK] The two tested snippets do not throw these errors when setting use_e2b_executor=False` --- Environment: smolagents==1.3.0 run in an ipynb with cursor in a poetry env with smolagents==1.3.0 installed % pip freeze aiofiles==23.2.1 aiohappyeyeballs==2.4.4 aiohttp==3.11.11 aiosignal==1.3.2 annotated-types==0.7.0 anyio==4.8.0 appnope==0.1.4 asttokens==3.0.0 attrs==24.3.0 beautifulsoup4==4.12.3 certifi==2024.12.14 charset-normalizer==3.4.1 click==8.1.8 comm==0.2.2 debugpy==1.8.12 decorator==5.1.1 distro==1.9.0 duckduckgo_search==7.2.1 e2b==1.0.5 e2b-code-interpreter==1.0.3 executing==2.1.0 fastapi==0.115.6 ffmpy==0.5.0 filelock==3.16.1 frozenlist==1.5.0 fsspec==2024.12.0 gradio==5.12.0 gradio_client==1.5.4 h11==0.14.0 httpcore==1.0.7 httpx==0.27.2 huggingface-hub==0.27.1 idna==3.10 importlib_metadata==8.5.0 ipykernel==6.29.5 ipython==8.31.0 jedi==0.19.2 Jinja2==3.1.5 jiter==0.8.2 jsonschema==4.23.0 jsonschema-specifications==2024.10.1 jupyter_client==8.6.3 jupyter_core==5.7.2 litellm==1.58.2 lxml==5.3.0 markdown-it-py==3.0.0 markdownify==0.14.1 MarkupSafe==2.1.5 matplotlib-inline==0.1.7 mdurl==0.1.2 multidict==6.1.0 nest-asyncio==1.6.0 numpy==2.2.1 openai==1.59.7 orjson==3.10.14 packaging==24.2 pandas==2.2.3 parso==0.8.4 pexpect==4.9.0 pillow==11.1.0 platformdirs==4.3.6 primp==0.10.1 prompt_toolkit==3.0.48 propcache==0.2.1 protobuf==5.29.3 psutil==6.1.1 ptyprocess==0.7.0 pure_eval==0.2.3 pydantic==2.10.5 pydantic_core==2.27.2 pydub==0.25.1 Pygments==2.19.1 python-dateutil==2.9.0.post0 python-dotenv==1.0.1 python-multipart==0.0.20 pytz==2024.2 PyYAML==6.0.2 pyzmq==26.2.0 referencing==0.36.1 regex==2024.11.6 requests==2.32.3 rich==13.9.4 rpds-py==0.22.3 ruff==0.9.2 safehttpx==0.1.6 safetensors==0.5.2 semantic-version==2.10.0 shellingham==1.5.4 six==1.17.0 smolagents==1.3.0 sniffio==1.3.1 soupsieve==2.6 stack-data==0.6.3 starlette==0.41.3 tiktoken==0.8.0 tokenizers==0.21.0 tomlkit==0.13.2 tornado==6.4.2 tqdm==4.67.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6b1ae45-3074-4538-8002-1f793c19b1f7", "embedding": null, "metadata": {"issue_id": 244, "title": "use_e2b_executor=True does not seem to work", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "baa985c3-bc95-41dc-9745-30cfd025ea29", "node_type": "4", "metadata": {"issue_id": 244, "title": "use_e2b_executor=True does not seem to work", "state": "closed", "labels": [], "type": "issue"}, "hash": "087e279827684a9ae86368571c490357ae33947388087c33f76c17a0e58ea7be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a65d0777-b1aa-45df-8309-d1f0093ad858", "node_type": "1", "metadata": {"issue_id": 244, "title": "use_e2b_executor=True does not seem to work", "state": "closed", "labels": [], "type": "issue"}, "hash": "e762220257cf7bff2200976e33b646c2f546689d0e207fed7e820624ffd3de89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0.20 pytz==2024.2 PyYAML==6.0.2 pyzmq==26.2.0 referencing==0.36.1 regex==2024.11.6 requests==2.32.3 rich==13.9.4 rpds-py==0.22.3 ruff==0.9.2 safehttpx==0.1.6 safetensors==0.5.2 semantic-version==2.10.0 shellingham==1.5.4 six==1.17.0 smolagents==1.3.0 sniffio==1.3.1 soupsieve==2.6 stack-data==0.6.3 starlette==0.41.3 tiktoken==0.8.0 tokenizers==0.21.0 tomlkit==0.13.2 tornado==6.4.2 tqdm==4.67.1 traitlets==5.14.3 transformers==4.48.0 typer==0.15.1 typing_extensions==4.12.2 tzdata==2024.2 urllib3==2.3.0 uvicorn==0.34.0 wcwidth==0.2.13 websockets==14.1 yarl==1.18.3 zipp==3.21.0\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 2004, "end_char_idx": 2598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65ab4a47-5c3b-4a8e-9dcc-fc17f1a120f7", "embedding": null, "metadata": {"issue_id": 243, "title": "LiteLLM seems to fail on o1 and o1-mini", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f9dab8e-3594-41c1-b1a7-4b9079361527", "node_type": "4", "metadata": {"issue_id": 243, "title": "LiteLLM seems to fail on o1 and o1-mini", "state": "closed", "labels": [], "type": "issue"}, "hash": "3e8db1d9f238952f56190bdda738452e8f403dacf8179ac5b23f23b129500553", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: LiteLLM seems to fail on o1 and o1-mini\n\nDescription: When using LiteLLM to call o1 and o1-mini, the API seems to fail because of: [CODE_BLOCK] After researching some articles, it seems to be related to how tool calls are being formatted to the API: https://community.openai.com/t/error-the-model-produced-invalid-content/747511 Example code to reproduce: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e06389f6-378f-4519-822a-33201675767f", "embedding": null, "metadata": {"issue_id": 241, "title": "Add installation page to docs", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38854334-5aa2-4e46-b289-e3090f1b4f83", "node_type": "4", "metadata": {"issue_id": 241, "title": "Add installation page to docs", "state": "open", "labels": [], "type": "issue"}, "hash": "ed8818fc3d5ff3f4d071c3410531aaa6acc8f8aff3b5f7695409539653c4793e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add installation page to docs\n\nDescription: Add installation page to docs, explaining the different extras available: audio, litellm,... Discussed in: - https://github.com/huggingface/smolagents/pull/229/filesr1919902148\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54dbe19f-64b1-487f-ab18-ed6395c54a2a", "embedding": null, "metadata": {"issue_id": 239, "title": "name 'litellm' is not defined", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e821b8b2-8185-461b-a048-b181a71df34d", "node_type": "4", "metadata": {"issue_id": 239, "title": "name 'litellm' is not defined", "state": "closed", "labels": [], "type": "issue"}, "hash": "a1e5168890235cd5ddd2267f396221a3a55b05b7e9a480f7350b486a7a77a949", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: name 'litellm' is not defined\n\nDescription: model = LiteLLMModel(model_id=\"gpt-4o\") --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-33-cba224b3a328> in <cell line: 0>() 2 import litellm 3 litellm.add_function_to_prompt = True ----> 4 model = LiteLLMModel(model_id=\"gpt-4o\") /usr/local/lib/python3.11/dist-packages/smolagents/models.py in __init__(self, model_id, api_base, api_key, **kwargs) 473 self.model_id = model_id 474 IMPORTANT - Set this to TRUE to add the function to the prompt for Non OpenAI LLMs --> 475 litellm.add_function_to_prompt = True 476 self.api_base = api_base 477 self.api_key = api_key NameError: name 'litellm' is not defined\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 759, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2b8be32-aa46-495d-9263-d185a63b7313", "embedding": null, "metadata": {"issue_id": 231, "title": "Show exception stack trace", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d4ccff4-ff41-4ca4-acaa-fd98df2e3e58", "node_type": "4", "metadata": {"issue_id": 231, "title": "Show exception stack trace", "state": "open", "labels": [], "type": "issue"}, "hash": "7f5493bd80755e7129e364d3e2dd7ea0694e44bf1b8860bc9e4a99266a1cc4a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Show exception stack trace\n\nDescription: When an exception is thrown, usually only the exception message is shown, without any stack trace. An exceptions might then be logged like: > Error in tool call execution: 'NoneType' object has no attribute 'inputs' Or: > Error in tool call execution: 'str' object has no attribute 'is_initialized' Without a stacktrace that is hard to track down. It seems one workaround is to add a callback to step_callbacks that would log the stacktrace of the error. Although that wouldn't show in the telemetry. Related it also seems to be preferred to use explicit exception chaining when translating exceptions. e.g. by adding from e in the following example: [CODE_BLOCK]\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c50cd5d9-c660-463a-8ff1-f08b1778a2c9", "embedding": null, "metadata": {"issue_id": 230, "title": "Issue implementing an own Model Class", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94174509-3005-4ec4-931e-8ca22d5eb565", "node_type": "4", "metadata": {"issue_id": 230, "title": "Issue implementing an own Model Class", "state": "closed", "labels": [], "type": "issue"}, "hash": "26775a5fb12a61a40d52b442da6dcc6cea8824c54644f4cbb65a5f3b8b80097c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Issue implementing an own Model Class\n\nDescription: I am implementing my own Model class as none of the existing works for my inference backend. I managed to get it running but I was puzzled by how the framework invokes the model. Also, did this change from 1.1.0 to 1.2.0? Here is my issue. In CodeAgent around line 916, the model is invoked [CODE_BLOCK] .content is called on the result. However, the signature of __call__ is: [CODE_BLOCK] Also checking the implementation of HfApiModel and LiteLLMModel it seems they all return a str. When I return a str I get the error: 'str' object has no attribute 'content' When I remove the. .content from the CodeAgent my Code works fine. I am sure I am missing something. Some hint would be greatly appreciated.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b03c9f0a-24b2-441d-b971-9bde4728b47b", "embedding": null, "metadata": {"issue_id": 224, "title": "Unable to run Gemma-2-2b-it model using local TransformersModel", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b35e2dc4-88d8-480f-9380-5bd123574000", "node_type": "4", "metadata": {"issue_id": 224, "title": "Unable to run Gemma-2-2b-it model using local TransformersModel", "state": "open", "labels": [], "type": "issue"}, "hash": "5ece3b2ac9beef094842b5522335739dcb1027b408a4631302a1ca3bdeaedcda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Unable to run Gemma-2-2b-it model using local TransformersModel\n\nDescription: I'm trying to use Gemma 2 2b-it model using TransformersModel via a local pipeline, but getting the following error: [CODE_BLOCK] !Image\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82951c5c-bcbb-480f-9199-ba43f18da179", "embedding": null, "metadata": {"issue_id": 223, "title": "[Bug] CodeAgent's system prompt does not contain authorized_imports", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8be22b14-1fba-4751-979a-d7188a8c2061", "node_type": "4", "metadata": {"issue_id": 223, "title": "[Bug] CodeAgent's system prompt does not contain authorized_imports", "state": "closed", "labels": [], "type": "issue"}, "hash": "097db705bae7cacc915f1f57a4785ceaa7dc053705ece58b7293e15c7957c261", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Bug] CodeAgent's system prompt does not contain authorized_imports\n\nDescription: I think there is a small bug with the CodeAgent's system prompt. In agent.run initialize_system_prompt() is called again, which means that the system prompt does not contain correct the following list of modules: {{authorized_imports}} that python can import. As that is replaced in the child classes init method. Possible solutions: - self.initialize_system_prompt() can be removed from the step method, as it is already called in __init__ - self.initialize_system_prompt() can be overridden by CodeAgent to include the following: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 648, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ccb8eb42-053c-4046-8acc-25268bc3538e", "embedding": null, "metadata": {"issue_id": 212, "title": "Documentation for Inspection/OpenTelemetry missing packages", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10e53dc5-a06c-4def-8e19-adebe7584af8", "node_type": "4", "metadata": {"issue_id": 212, "title": "Documentation for Inspection/OpenTelemetry missing packages", "state": "closed", "labels": [], "type": "issue"}, "hash": "4b38b8086e8d607a4259724c817df1e890dbc465018f10d559a5449f86b945cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Documentation for Inspection/OpenTelemetry missing packages\n\nDescription: Documentation/Instructions Issue: When trying to follow the instructions for integrating inspection/opentelemetry docs/source/en/tutorials/inspect_runs.md importing SmolagentsInstrumentor from openinference.instrumentation.smolagents did not work out after following the pip install-instructions within the documentation. Resolve: After some research the issue could be resolved by installing via pip install openinference-instrumentation-smolagents from https://github.com/Arize-ai/openinference.git. Installed pip packages after following documentation: openinference-instrumentation 0.1.20 opentelemetry-sdk 1.29.0 opentelemetry-exporter-otlp 1.29.0 arize-phoenix 7.7.2 smolagents 1.3.0 (missing) openinference-instrumentation-smolagents -- Please note I'm not 100% confident about my conclusion here.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "93904e31-3157-4609-b84e-effc44c54da6", "embedding": null, "metadata": {"issue_id": 211, "title": "Jupyter Notebooks for Documentation", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28de73ee-1914-4017-836e-0017255a4a43", "node_type": "4", "metadata": {"issue_id": 211, "title": "Jupyter Notebooks for Documentation", "state": "closed", "labels": [], "type": "issue"}, "hash": "5c4c88fd19a2edf4721eb1b68b0f00b5aaa3e1969119854363571511862b2d9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Jupyter Notebooks for Documentation\n\nDescription: The documentation does not yet contain notebooks for the various tutorials and guides available. I have previously contributed to the diffusers library by adding the notebooks to huggingface/notebook repo and linking them with the relevant documentation pages. Could you please let me know from which pages to begin from and the level of technical details to be added? Reference: https://github.com/huggingface/diffusers/pull/9905 https://github.com/huggingface/diffusers/pull/10032 https://github.com/huggingface/notebooks/pull/535 cc: @aymeric-roucher\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 625, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "14227da5-eaf2-4ba6-9fc3-419241a4105c", "embedding": null, "metadata": {"issue_id": 210, "title": "Create example of using constrained/structured generation with smolagents", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "875c394c-ee8e-4303-9b9b-9c092e45b2e7", "node_type": "4", "metadata": {"issue_id": 210, "title": "Create example of using constrained/structured generation with smolagents", "state": "closed", "labels": [], "type": "issue"}, "hash": "73ecb6095a2c25807a742709b0d09965c44242234e3e21950f90ee94488b95d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Create example of using constrained/structured generation with smolagents\n\nDescription: Specifying grammar's is currently possible, but it's not explained to users how this can be utilized by agents. Create a tutorial and (possibly) direct functionality within the library to use this tactic to unlock much better tool usage capabilities.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 360, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4ce7410-01b9-44ae-8dda-8864d1b4c267", "embedding": null, "metadata": {"issue_id": 201, "title": "Repeatedly getting \"Error in code parsing: Your code snippet is invalid\"", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5427b3bb-8a87-4033-b687-97e4949910e5", "node_type": "4", "metadata": {"issue_id": 201, "title": "Repeatedly getting \"Error in code parsing: Your code snippet is invalid\"", "state": "open", "labels": [], "type": "issue"}, "hash": "c6d6a206eeb6ffd27f8c24acebc6f39c9bbc3ac7f5ffb91929c214174f523ce1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Repeatedly getting \"Error in code parsing: Your code snippet is invalid\"\n\nDescription: I repeatedly get this error when using the Codeagent: CODE_BLOCK?\\n(.*?)\\n[CODE_BLOCK] I have tried to emphasize proper code block formatting in the agent.run() argument, to no avail. The error occurs while using GPT-4o and Sonnet 3.5. How can I fix this or at least improve realiability? Thanks for any help in this regard!\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 431, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "66a244c6-8d70-4e16-a282-1f58afc86ac3", "embedding": null, "metadata": {"issue_id": 197, "title": "Clarification on Smolagents", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02cb1cff-dd71-4be8-afb3-51e9474fe5af", "node_type": "4", "metadata": {"issue_id": 197, "title": "Clarification on Smolagents", "state": "closed", "labels": [], "type": "issue"}, "hash": "03501eed1c6dc05963f7af4c729e2836deccc7975cddea7fe33dceaf989d1f9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Clarification on Smolagents\n\nDescription: Can someone give a brief description on how CodeAgent is working? In Local where the code is executed? Also can i save/see the AI generated code ?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c39ab6f-4396-4ebe-91e1-13fe2c0dc87d", "embedding": null, "metadata": {"issue_id": 195, "title": "Feature Request: Automatically install litellm if not found.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1196654-fb24-4b71-8d9c-d77ec5e37bad", "node_type": "4", "metadata": {"issue_id": 195, "title": "Feature Request: Automatically install litellm if not found.", "state": "closed", "labels": [], "type": "issue"}, "hash": "cc580d2d302abc957c3ffb541565d1331efeb02f6a08ca3915ada34ff17c6e37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feature Request: Automatically install litellm if not found.\n\nDescription: When installing smolagent and importing LiteLLMModel it gives an error [CODE_BLOCK]. Maybe we can modify the LiteLLMModel so if litellm is not found it install automatically like: [CODE_BLOCK] Output: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b021e161-18ba-40e0-a919-e8dac649e08d", "embedding": null, "metadata": {"issue_id": 192, "title": "local interpeter assumes comparator operators return bool", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0661a5f1-49cc-4764-ae3a-6392e09e600d", "node_type": "4", "metadata": {"issue_id": 192, "title": "local interpeter assumes comparator operators return bool", "state": "closed", "labels": [], "type": "issue"}, "hash": "8444fd8e4861814ff5f8068dc038aa8ce637a769a0914f1c7e3c00140e973022", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: local interpeter assumes comparator operators return bool\n\nDescription: The current implementation of evaluate_condition only works properly if the result of a comparator operation is a boolean. Some modules may return other types for comparisons, especially if they are building their own symbolic representation of an expression. For example, the pulp library for mixed linear integer programming returns an LpConstraint object from a comparison. This causes an exception here, because there is no bitwise and operator for bool & LpConstraint.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b28c91f-02a3-41c5-94c4-69822e67e506", "embedding": null, "metadata": {"issue_id": 190, "title": "local interpreter overrides in-place operators", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "798b12cc-42c0-4d8f-8344-b94d5b0e8844", "node_type": "4", "metadata": {"issue_id": 190, "title": "local interpreter overrides in-place operators", "state": "closed", "labels": [], "type": "issue"}, "hash": "e5d60caa1d745ee11ed2759951dc77a8028ffaf93569ed70488c43d04f821c62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: local interpreter overrides in-place operators\n\nDescription: The smolagents local python interpreter implements in_place operators like y += x as y = y + x, which bypasses any __iadd__ operator that is implemented on the y object. This can lead to poor memory usage (at best) if y is large, and incorrect behavior if the object defines __iadd__ in unusual ways. For example, the pulp module for mixed linear integer programming defines an \"LpProblem\" object, which overrides __iadd__ as a convenience to simplify the syntax of adding constraints and setting objectives: [CODE_BLOCK] The python local interpreter interprets the first line as problem = problem + max_val, but LpProblem doesn't implement an __add__ method, so this results in an exception. (One might argue this is a \"bad\" use of operator overrides, but the LLMs I've tried apply this as the canonical way to add constraints and objectives in pulp, and it works consistently in most other python interpreters.)\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da9e71ea-5530-42ce-960f-7134ac9ebe7a", "embedding": null, "metadata": {"issue_id": 188, "title": "Feature Request: Rust Implementation", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b0caf06-8eee-4605-8699-3670694231f7", "node_type": "4", "metadata": {"issue_id": 188, "title": "Feature Request: Rust Implementation", "state": "closed", "labels": [], "type": "issue"}, "hash": "c8b5acd85d3fb5b6c2878e532e8cd88d4f2f85e92350b3b9c12a850e1c9aed4c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feature Request: Rust Implementation\n\nDescription: I would love to be able to use Smolagents within Rust for on-device scenarios.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 151, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ead7265a-7f28-4ea4-ab13-403695136895", "embedding": null, "metadata": {"issue_id": 184, "title": "ERROR:root:Error in analysis: 'dict' object has no attribute 'strip' An error occurred: 'dict' object has no attribute 'strip'", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c9236da-86a6-4a37-8103-931c26f40b7f", "node_type": "4", "metadata": {"issue_id": 184, "title": "ERROR:root:Error in analysis: 'dict' object has no attribute 'strip' An error occurred: 'dict' object has no attribute 'strip'", "state": "closed", "labels": [], "type": "issue"}, "hash": "c9bb15e297eea30f24189341832b30169ee1374e4e0721deae74f24b2ec5146c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: ERROR:root:Error in analysis: 'dict' object has no attribute 'strip' An error occurred: 'dict' object has no attribute 'strip'\n\nDescription: I have developed an ai agent using smolagent for geoscience purpose, and I received the following error; [CODE_BLOCK] [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "382a1441-a266-496f-bcc1-669690275208", "embedding": null, "metadata": {"issue_id": 183, "title": "Use kwargs wherever possible", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2271342-e0e7-493e-b3e9-c3b817570cc2", "node_type": "4", "metadata": {"issue_id": 183, "title": "Use kwargs wherever possible", "state": "closed", "labels": [], "type": "issue"}, "hash": "7a4e82c7b9b14fb22fec0a94e8b6f91034cd6626a011862d5a05e28994201389", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Use kwargs wherever possible\n\nDescription: Right now, most of the underlying LLMs internals, i.e. the sampler settings, are not exposed and require code changes to be made in smolagents. I've patched this library with my own changes (i.e. device = \"auto\", using flash attention, etc), and most of those patches wouldn't be necessary if we exposed kwargs for the underlying LLM.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 399, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c01e5c8a-c5d3-47f1-ad51-f475c41fba31", "embedding": null, "metadata": {"issue_id": 181, "title": "The call.func parameter type of the local_python_executor.evaluate_call function might be ast.Subscript", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24265265-9d2b-4b38-8d3a-c0a8e70324c0", "node_type": "4", "metadata": {"issue_id": 181, "title": "The call.func parameter type of the local_python_executor.evaluate_call function might be ast.Subscript", "state": "closed", "labels": [], "type": "issue"}, "hash": "529816ac16c6a946c346591fcac7390d3ee7d38659aa0c4927571b63a065b44e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: The call.func parameter type of the local_python_executor.evaluate_call function might be ast.Subscript\n\nDescription: Hi While testing Python code execution, I discovered an issue where the call.func parameter type could potentially be ast.Subscript. Please consider supporting this case.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 310, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bea7cabb-a68d-43db-a96f-8e188b656c1b", "embedding": null, "metadata": {"issue_id": 180, "title": "Make torch dependency optional", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27c79e80-522f-4fc3-9a66-85a0170eee93", "node_type": "4", "metadata": {"issue_id": 180, "title": "Make torch dependency optional", "state": "closed", "labels": [], "type": "issue"}, "hash": "9533890a66ea25c9afd54a2614f4cf1779b86b42ffdaafbacbec7cd9f9036abd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Make torch dependency optional\n\nDescription: Related to 100 torch is a fairly heavy dependency to install and doesn't seem to be necessary when connecting to APIs instead (even local ones). It would be great to make at least torch an optional dependency. Ideally other dependencies could be reviewed too.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 326, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a44434ae-c4fe-41a5-b3a0-372c41baa460", "embedding": null, "metadata": {"issue_id": 176, "title": "Vision Language Support", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd6d3eab-5270-4e7c-bd3d-cfdaff91c39f", "node_type": "4", "metadata": {"issue_id": 176, "title": "Vision Language Support", "state": "closed", "labels": [], "type": "issue"}, "hash": "26b260daf8232136e0faa6584380b408cdd78d0181fb830860e90f7aa4335145", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Vision Language Support\n\nDescription: smolagents should have vision language support \ud83d\udc97 This can be in two ways: 1. Vision language models as tools to parse input to a structured output to pass to LLM agents - [ ] Having screenshot parsing tools like ShowUI or OmniParser - [ ] Having document parsing tools like GOT-OCR (which will soon be integrated to transformers) 2. Using VLMs as the main model in agent and directly passing input - [ ] this is a more image native approach, curious how it will work with smaller models though - [ ] the issue is we do not know at which step we input the images. for document use cases first step should be enough, for screenshot cases it could be every step or every step where a click happens. I wonder if we could let model decide this at some point, but for initial support I think this would be too complicated. This excites me a lot, so I am willing to tackle this one PR at a time.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 948, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "008572c2-1afd-40f5-ac46-02e955b8b6a2", "embedding": null, "metadata": {"issue_id": 169, "title": "The final_answer method does not have an answer variable because the prompt defines the answer variable,", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b89f90bd-9a79-4378-8db6-33812ace8680", "node_type": "4", "metadata": {"issue_id": 169, "title": "The final_answer method does not have an answer variable because the prompt defines the answer variable,", "state": "closed", "labels": [], "type": "issue"}, "hash": "2c4a9414f12291258bf4cc9ddf34ad7def92328a60468c1c5ffd579b97ace0e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: The final_answer method does not have an answer variable because the prompt defines the answer variable,\n\nDescription: The LLM generates the following code for execution. Output message of the LLM: Code: [CODE_BLOCK] \u2500 Executing this code: \u2500\u2500\u2500\u2500\u2500\u2500\u2500 final_answer(answer=response.text) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 evaluate_python_code.<locals>.final_answer() got an unexpected keyword argument 'answer'\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "36460e6c-393c-47ed-8d82-162702a98dcd", "embedding": null, "metadata": {"issue_id": 166, "title": "504 Gateway Timeout errors when running the Quick Demo", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad9a4108-949f-49bd-82c6-c7997eb4babb", "node_type": "4", "metadata": {"issue_id": 166, "title": "504 Gateway Timeout errors when running the Quick Demo", "state": "closed", "labels": [], "type": "issue"}, "hash": "86b0d28ee0f5fc90d5a225a78b7baa1cd16d4591a4f666629b3f98deec64b181", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: 504 Gateway Timeout errors when running the Quick Demo\n\nDescription: I am encountering 504 Gateway Timeout errors when trying to run the script shown in the Quick Demo section in the README of this repo **Error Messages**: 504 Server Error: Gateway Timeout for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions **Steps to Reproduce**: 1. Activate the virtual environment. 2. Run the command on the relevant Python script e.g: python agents.py. **Expected Behavior**: The script should run without timeout errors and return the expected model output. **Actual Behavior**: The script times out with the following error message: 504 Server Error: Gateway Timeout for url: https://api-inference.huggingface.co/models/Qwen/Qwen2.5-Coder-32B-Instruct/v1/chat/completions\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 837, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8b4de650-cf1a-4cf6-865a-f8cade04a362", "embedding": null, "metadata": {"issue_id": 162, "title": "Feature Request: Add support for ast.Pass in the interpreter", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c31d8bd4-790b-4e07-9118-248a52d1ed4e", "node_type": "4", "metadata": {"issue_id": 162, "title": "Feature Request: Add support for ast.Pass in the interpreter", "state": "closed", "labels": [], "type": "issue"}, "hash": "3330a626dcb57bff28edf57676ffc85f3b23b85fa4ad4f50b761eca8e8676aba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feature Request: Add support for ast.Pass in the interpreter\n\nDescription: I suggest including support for ast.Pass in the interpreter. Since pass is a no-op in Python, this could be implemented simply as return None. Thank you for creating such a great module!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f35d7046-7048-4fd5-b30f-29f745f91243", "embedding": null, "metadata": {"issue_id": 158, "title": "[Bug Report] `list index out of range` in ToolCallingAgent with OpenAIServerModel", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7afd5cb4-b178-45a7-974f-acdaefb5ebcf", "node_type": "4", "metadata": {"issue_id": 158, "title": "[Bug Report] `list index out of range` in ToolCallingAgent with OpenAIServerModel", "state": "closed", "labels": [], "type": "issue"}, "hash": "eb695d888082a48b266e1bd96f71d98cf43aa61b5b18e3df22ffb1d0586aaf8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [Bug Report] `list index out of range` in ToolCallingAgent with OpenAIServerModel\n\nDescription: **Bug Report: list index out of range in ToolCallingAgent with OpenAIServerModel** **1. Environment Details** - **OS:** Ubuntu 22.04 - **Dependencies:** - vllm==0.6.6.post1 - torch==2.5.1 - smolagents==1.2.2 **2. Steps to Reproduce** 1. Start a vllm service with one of the following commands: - **Qwen2.5-Coder-7B-Instruct** [CODE_BLOCK] - **Meta-Llama-3.1-8B-Instruct** [CODE_BLOCK] 2. Run the following test script: [CODE_BLOCK] **3. Observed Behavior or bug** The script produces the following error list index out of range : [CODE_BLOCK] **4. Traceback** The error originates from the step function in agents.py within the ToolCallingAgent class: [CODE_BLOCK] The tool_calls list is empty because the vllm service returns the tool_call data as a string within model_message.content instead of populating model_message.tool_calls. **Examples of model_message values returned by different models:** - **Qwen2.5-Coder-7B-Instruct:** [CODE_BLOCK] - **Llama-3.1-8B-Instruct:** [CODE_BLOCK] **5. Temporary Fix** A possible workaround is to modify the step function to parse tool_calls from model_message.content when model_message.tool_calls is empty: [CODE_BLOCK] With this bug-fix, the script executes normally: [CODE_BLOCK] **6. Note** This issue only occurs when using ToolCallingAgent with OpenAIServerModel. And does not occur when with TransformersModel or HfApiModel or LiteLLMModel.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9a03919-4916-4f22-bd0b-8786034acf86", "embedding": null, "metadata": {"issue_id": 154, "title": "Inconsistency in tool configuration requirements between `tool.py` and `tool_config.json`", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dfab1ab-3b16-43be-a055-984d48f3798f", "node_type": "4", "metadata": {"issue_id": 154, "title": "Inconsistency in tool configuration requirements between `tool.py` and `tool_config.json`", "state": "closed", "labels": [], "type": "issue"}, "hash": "c5581e3085a4dda13e4b50b2a80766f04c85bfad3ac139614d8330dc4fab0682", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Inconsistency in tool configuration requirements between `tool.py` and `tool_config.json`\n\nDescription: When attempting to use the ToolCollection as shown in the documentation, I encountered an error related to tool configuration files. Code Example [CODE_BLOCK] Error message [CODE_BLOCK] Because that repository has tool_config.json, I investigated the codebase and found several inconsistencies: The error message indicates that tool_config.json is required, but the actual tool saving and loading logic depends on tool.py: error message when **tool.py** is missing: https://github.com/huggingface/smolagents/blob/fec65e154a41d97d0d73613443514f86e220b1c3/src/smolagents/tools.pyL480-L483 seeking tool.py in from_hub method https://github.com/huggingface/smolagents/blob/fec65e154a41d97d0d73613443514f86e220b1c3/src/smolagents/tools.pyL456-L479 save method https://github.com/huggingface/smolagents/blob/fec65e154a41d97d0d73613443514f86e220b1c3/src/smolagents/tools.pyL253 https://github.com/huggingface/smolagents/blob/fec65e154a41d97d0d73613443514f86e220b1c3/src/smolagents/tools.pyL311-L312 So I initially thought tool_config.json is not mandatory, but soon i found that it is used for repository type checking: https://github.com/huggingface/smolagents/blob/fec65e154a41d97d0d73613443514f86e220b1c3/src/smolagents/tools.pyL64-L84 I think it has to be clarified which configuration file is actually required for tool functionality: - Is only tool.py necessary? - Is tool.py the primary option with tool_config.json as fallback? - Are both files required? Thank you for the amazing work and feel free to ask me if you need the additional info or detailed explanation!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23cf961e-bf51-45db-9107-aa786d6ceea3", "embedding": null, "metadata": {"issue_id": 153, "title": "Does smolagents offer structured outputs (constrained decoding not implicit)", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "179d5def-5c97-4c68-b3ac-3ba2a77e1a21", "node_type": "4", "metadata": {"issue_id": 153, "title": "Does smolagents offer structured outputs (constrained decoding not implicit)", "state": "open", "labels": [], "type": "issue"}, "hash": "feaf42f5f5d986a474c51d83f871656c41b377b02f4a66bd059e854d852a260d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Does smolagents offer structured outputs (constrained decoding not implicit)\n\nDescription: @aymeric-roucher I've been wondering what solutions HuggingFace offers with regards to structured outputs (SO) and have come short (the only thing I have found is dottxt/outlines, which is totally ok if HF intends to depend on them for SO). This is particularly relevant for smolagents where SO compound in value. I know this is not a very smolagents specific issue but nonetheless I think its highly consequential for smolagents. Do you know what HF strategy towards structured outputs is? How are you guys thinking about it?\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f2a8fd45-dd7a-49a2-abc0-369ef2f08e47", "embedding": null, "metadata": {"issue_id": 152, "title": "\u53ef\u4ee5\u505a\u4e9b\u63d2\u4ef6\u548cdjango\u5bf9\u63a5\u5417", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4d015e5-e6ae-4f62-a509-483700f348fa", "node_type": "4", "metadata": {"issue_id": 152, "title": "\u53ef\u4ee5\u505a\u4e9b\u63d2\u4ef6\u548cdjango\u5bf9\u63a5\u5417", "state": "closed", "labels": [], "type": "issue"}, "hash": "1219473c6d6055234dcb782f27b39df3e68e31afaad4816e166d2389bdaccd31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: \u53ef\u4ee5\u505a\u4e9b\u63d2\u4ef6\u548cdjango\u5bf9\u63a5\u5417\n\nDescription: \n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 53, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5e62b609-68b8-49d1-9211-f063bb9f6f12", "embedding": null, "metadata": {"issue_id": 149, "title": "Adding/removing a tool should also update the system prompt accordingly", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc7e7858-8fa6-4063-b597-025bf5b3e79f", "node_type": "4", "metadata": {"issue_id": 149, "title": "Adding/removing a tool should also update the system prompt accordingly", "state": "closed", "labels": [], "type": "issue"}, "hash": "7e6abfd41180c50b9927f0d70869f0fb01642b0dcb3d417275b5f39f1f7ca1bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Adding/removing a tool should also update the system prompt accordingly\n\nDescription: Currently, if the user updates agent.tools, say to add or delete a given tool, that update is not reflected in the agent.system_prompt. I recall there used to be dedicated functionality for managing a toolbox, but with the port to smolagents I think this got lost\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a7a55df2-62c6-4ed0-897f-e1a9374c930a", "embedding": null, "metadata": {"issue_id": 145, "title": "Making this library async", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f152ad8-02d3-4793-b1b9-72a1150b2c2f", "node_type": "4", "metadata": {"issue_id": 145, "title": "Making this library async", "state": "open", "labels": [], "type": "issue"}, "hash": "d88e7ad78045a9f4e43aa98f64ec3bf390df3f945d30c17cdab7b38a3add908d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Making this library async\n\nDescription: Most if not all of the functionality of this library is network bound. Would it make sense to make its API asynchronous?\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d5c316e-18d2-4297-b33f-80cba2d2dcaf", "embedding": null, "metadata": {"issue_id": 143, "title": "Add Portkey AI Gateway Support", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b2ca6ae-1396-4fb9-9904-d8165bd0f75b", "node_type": "4", "metadata": {"issue_id": 143, "title": "Add Portkey AI Gateway Support", "state": "closed", "labels": [], "type": "issue"}, "hash": "f84dced88372315602bce9002be14b6921fc8728264c5a8be0129a14b8bd0bf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add Portkey AI Gateway Support\n\nDescription: Overview We should add support for Portkey's AI Gateway to give users access to 250+ LLM models through a single interface, similar to our existing integration in models.py . Why? - Users need an easier way to get access to 250+ LLM providers - Built-in observability and monitoring with Portkey - fallback, load balancing, retries, and more with Portkey's reliability features - Better debugging and tracing capabilities in Portkey would help users\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 516, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69a5053f-f06a-4234-a800-e040d46a8196", "embedding": null, "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "74f7ca29-6756-4649-a89c-7a917075e7c8", "node_type": "4", "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "hash": "d61dbbde7d83325afb4873fe0fd033bd6f72f8a1944704f63f91fd3f5a89f025", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df30ab2b-0811-4236-ad3b-7077b62f0a59", "node_type": "1", "metadata": {}, "hash": "47254963dfba1a31e6d5ac8b4913ebfb6fd794c83dd95d48edeb65ac50e19eb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: pip install error. RuntimeError: uvloop does not support Windows at the moment\n\nDescription: my python version is 3.12.3 (.venv) PS C:\\...> pip install smolagents Collecting smolagents Downloading smolagents-1.1.0-py3-none-any.whl.metadata (7.5 kB) Requirement already satisfied: torch in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from smolagents) (2.4.1+cu124) Requirement already satisfied: torchaudio in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from smolagents) (2.4.1+cu124) Requirement already satisfied: torchvision in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from smolagents) (0.19.1+cu124) Requirement already satisfied: transformers>=4.0.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from smolagents) (4.36.2) Requirement already satisfied: requests>=2.32.3 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from smolagents) (2.32.3) Collecting rich>=13.9.4 (from smolagents) Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB) Collecting pandas>=2.2.3 (from smolagents) Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB) Requirement already satisfied: jinja2>=3.1.4 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from smolagents) (3.1.4) Collecting pillow>=11.0.0 (from smolagents) Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB) Collecting markdownify>=0.14.1 (from smolagents) Downloading markdownify-0.14.1-py3-none-any.whl.metadata (8.5 kB) Collecting gradio>=5.8.0 (from smolagents) Downloading gradio-5.11.0-py3-none-any.whl.metadata (16 kB) Collecting duckduckgo-search>=6.3.7 (from smolagents) Downloading duckduckgo_search-7.2.1-py3-none-any.whl.metadata (17 kB) Collecting python-dotenv>=1.0.1 (from smolagents) Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB) Collecting e2b-code-interpreter>=1.0.3 (from smolagents) Downloading e2b_code_interpreter-1.0.3-py3-none-any.whl.metadata (2.6 kB) Collecting litellm>=1.55.10 (from smolagents) Downloading litellm-1.57.5-py3-none-any.whl.metadata (36 kB) Requirement already satisfied: click>=8.1.7 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from duckduckgo-search>=6.3.7->smolagents) (8.1.7) Collecting primp>=0.10.0 (from duckduckgo-search>=6.3.7->smolagents) Using cached primp-0.10.0-cp38-abi3-win_amd64.whl.metadata (12 kB) Collecting lxml>=5.3.0 (from duckduckgo-search>=6.3.7->smolagents) Using cached lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB) Requirement already satisfied: attrs>=21.3.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from e2b-code-interpreter>=1.0.3->smolagents) (24.2.0) Collecting e2b<2.0.0,>=1.0.4 (from e2b-code-interpreter>=1.0.3->smolagents) Downloading e2b-1.0.5-py3-none-any.whl.metadata (2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "df30ab2b-0811-4236-ad3b-7077b62f0a59", "embedding": null, "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "74f7ca29-6756-4649-a89c-7a917075e7c8", "node_type": "4", "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "hash": "d61dbbde7d83325afb4873fe0fd033bd6f72f8a1944704f63f91fd3f5a89f025", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69a5053f-f06a-4234-a800-e040d46a8196", "node_type": "1", "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "hash": "4e771ae0b09374171c67998fe2d11fda90d850d7104ccac9635b2361436f69a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02f702fe-de7a-4b51-9b8c-edf77f266dc2", "node_type": "1", "metadata": {}, "hash": "bf1a068c45a19e75d3c053aa356cddcbc0420b43cd9b887469654c116b6fd033", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.0-cp38-abi3-win_amd64.whl.metadata (12 kB) Collecting lxml>=5.3.0 (from duckduckgo-search>=6.3.7->smolagents) Using cached lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB) Requirement already satisfied: attrs>=21.3.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from e2b-code-interpreter>=1.0.3->smolagents) (24.2.0) Collecting e2b<2.0.0,>=1.0.4 (from e2b-code-interpreter>=1.0.3->smolagents) Downloading e2b-1.0.5-py3-none-any.whl.metadata (2.6 kB) Collecting httpx<1.0.0,>=0.20.0 (from e2b-code-interpreter>=1.0.3->smolagents) Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB) Collecting aiofiles<24.0,>=22.0 (from gradio>=5.8.0->smolagents) Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB) Collecting anyio<5.0,>=3.0 (from gradio>=5.8.0->smolagents) Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB) Collecting fastapi<1.0,>=0.115.2 (from gradio>=5.8.0->smolagents) Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB) Collecting ffmpy (from gradio>=5.8.0->smolagents) Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB) Collecting gradio-client==1.5.3 (from gradio>=5.8.0->smolagents) Downloading gradio_client-1.5.3-py3-none-any.whl.metadata (7.1 kB) Requirement already satisfied: huggingface-hub>=0.25.1 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.25.2) Requirement already satisfied: markupsafe~=2.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (2.1.5) Requirement already satisfied: numpy<3.0,>=1.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (1.26.4) Collecting orjson~=3.0 (from gradio>=5.8.0->smolagents) Downloading orjson-3.10.14-cp312-cp312-win_amd64.whl.metadata (42 kB) Requirement already satisfied: packaging in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (24.1) Requirement already satisfied: pydantic>=2.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (2.8.2) Collecting pydub (from gradio>=5.8.0->smolagents) Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB) Collecting python-multipart>=0.0.18 (from gradio>=5.8.0->smolagents) Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB) Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (6.0.2) Collecting ruff>=0.2.", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 4632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "02f702fe-de7a-4b51-9b8c-edf77f266dc2", "embedding": null, "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "74f7ca29-6756-4649-a89c-7a917075e7c8", "node_type": "4", "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "hash": "d61dbbde7d83325afb4873fe0fd033bd6f72f8a1944704f63f91fd3f5a89f025", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df30ab2b-0811-4236-ad3b-7077b62f0a59", "node_type": "1", "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "hash": "d12804d132b662e7506a8d5475e975067375153085c71e38f5e3bff5564e0418", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdfb259e-9343-49ce-ac92-2aacc71c1a79", "node_type": "1", "metadata": {}, "hash": "bbeb9da6893dce4f3f1ab0e6117ce601fc4eb36b132463871054e644cce71390", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (2.8.2) Collecting pydub (from gradio>=5.8.0->smolagents) Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB) Collecting python-multipart>=0.0.18 (from gradio>=5.8.0->smolagents) Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB) Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (6.0.2) Collecting ruff>=0.2.2 (from gradio>=5.8.0->smolagents) Downloading ruff-0.9.0-py3-none-win_amd64.whl.metadata (26 kB) Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>=5.8.0->smolagents) Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB) Collecting semantic-version~=2.0 (from gradio>=5.8.0->smolagents) Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB) Collecting starlette<1.0,>=0.40.0 (from gradio>=5.8.0->smolagents) Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB) Collecting tomlkit<0.14.0,>=0.12.0 (from gradio>=5.8.0->smolagents) Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB) Requirement already satisfied: typer<1.0,>=0.12 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (0.12.3) Requirement already satisfied: typing-extensions~=4.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio>=5.8.0->smolagents) (4.12.2) Collecting uvicorn>=0.14.0 (from gradio>=5.8.0->smolagents) Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB) Requirement already satisfied: fsspec in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from gradio-client==1.5.3->gradio>=5.8.0->smolagents) (2024.6.1) Collecting websockets<15.0,>=10.0 (from gradio-client==1.5.3->gradio>=5.8.0->smolagents) Using cached websockets-14.1-cp312-cp312-win_amd64.whl.metadata (6.9 kB) Requirement already satisfied: aiohttp in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from litellm>=1.55.10->smolagents) (3.10.10) Collecting httpx<1.0.0,>=0.20.0 (from e2b-code-interpreter>=1.0.3->smolagents) Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB) Collecting importlib-metadata>=6.8.0 (from litellm>=1.55.10->smolagents) Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB) Collecting jsonschema<5.0.0,>=4.22.0 (from litellm>=1.55.10->smolagents) Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB) Collecting openai>=1.55.3 (from litellm>=1.55.", "mimetype": "text/plain", "start_char_idx": 4159, "end_char_idx": 6570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bdfb259e-9343-49ce-ac92-2aacc71c1a79", "embedding": null, "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "74f7ca29-6756-4649-a89c-7a917075e7c8", "node_type": "4", "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "hash": "d61dbbde7d83325afb4873fe0fd033bd6f72f8a1944704f63f91fd3f5a89f025", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02f702fe-de7a-4b51-9b8c-edf77f266dc2", "node_type": "1", "metadata": {"issue_id": 142, "title": "pip install error. RuntimeError: uvloop does not support Windows at the moment", "state": "closed", "labels": [], "type": "issue"}, "hash": "cdcff2bb0f83b2bc6dfac1cb4aa4eb02a3d945328dfb40097a5403aeca823626", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10.10) Collecting httpx<1.0.0,>=0.20.0 (from e2b-code-interpreter>=1.0.3->smolagents) Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB) Collecting importlib-metadata>=6.8.0 (from litellm>=1.55.10->smolagents) Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB) Collecting jsonschema<5.0.0,>=4.22.0 (from litellm>=1.55.10->smolagents) Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB) Collecting openai>=1.55.3 (from litellm>=1.55.10->smolagents) Downloading openai-1.59.6-py3-none-any.whl.metadata (27 kB) Requirement already satisfied: tiktoken>=0.7.0 in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from litellm>=1.55.10->smolagents) (0.8.0) Requirement already satisfied: tokenizers in c:\\coding\\gr0wth\\.venv\\lib\\site-packages (from litellm>=1.55.10->smolagents) (0.15.2) Collecting uvloop<0.22.0,>=0.21.0 (from litellm>=1.55.10->smolagents) Downloading uvloop-0.21.0.tar.gz (2.5 MB) \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.5/2.5 MB 8.9 MB/s eta 0:00:00 Installing build dependencies ... done Getting requirements to build wheel ... error error: subprocess-exited-with-error \u00d7 Getting requirements to build wheel did not run successfully. \u2502 exit code: 1 \u2570\u2500> [18 lines of output] Traceback (most recent call last): File \"c:\\Coding\\gr0wth\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module> main() File \"c:\\Coding\\gr0wth\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main json_out['return_val'] = hook(**hook_input['kwargs']) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"c:\\Coding\\gr0wth\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel return hook(config_settings) ^^^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\Oscar\\AppData\\Local\\Temp\\pip-build-env-p929xq8_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel return self._get_build_requires(config_settings, requirements=[]) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"C:\\Users\\Oscar\\AppData\\Local\\Temp\\pip-build-env-p929xq8_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires self.run_setup() File \"C:\\Users\\Oscar\\AppData\\Local\\Temp\\pip-build-env-p929xq8_\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup exec(code, locals()) File \"<string>\", line 8, in <module> RuntimeError: uvloop does not support Windows at the moment [end of output] note: This error originates from a subprocess, and is likely not a problem with pip.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 6092, "end_char_idx": 8713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0f3f39fc-22b7-435d-99b1-85f24aa09a68", "embedding": null, "metadata": {"issue_id": 141, "title": "[i18n-zh] Translating docs to Chinese", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dca0ff0-b153-4afc-bd58-3666e3f34d5c", "node_type": "4", "metadata": {"issue_id": 141, "title": "[i18n-zh] Translating docs to Chinese", "state": "closed", "labels": [], "type": "issue"}, "hash": "43f697e35491a0e49eafc45b221eb7b81e97f38668018b3abad39fd2e6bddf5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [i18n-zh] Translating docs to Chinese\n\nDescription: Hello Huggingface team, I've checked \"smolagents\" recently, and I'm interested in it, I would like to help translate docs.\ud83d\ude00 I'll be responsible to chapters below in next 2 weeks. [CODE_BLOCK] I also found reply from @aymeric-roucher under https://github.com/huggingface/smolagents/issues/80, I could also add this video to the \"Guided tour\" of zh translation docs.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9263019-52ba-4f1f-bbcc-30dcc0da33da", "embedding": null, "metadata": {"issue_id": 137, "title": "Provide another option for code execution - run in a docker container?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e8cded8-e487-4ed7-8974-5b343f761fe7", "node_type": "4", "metadata": {"issue_id": 137, "title": "Provide another option for code execution - run in a docker container?", "state": "closed", "labels": [], "type": "issue"}, "hash": "6a01e2f01ad938ceb6c43af41e6d8181a11330b1007c2ae784704083141e15b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Provide another option for code execution - run in a docker container?\n\nDescription: Hi, thanks for this library, it is very ergonomic! I was wondering if you might add an intermediate feature between local code execution and E2B : running inside a docker container (e.g. something like https://github.com/engineer-man/piston ). I have a legal need to keep my data and prompts safe, so it's harder for me to use providers like E2B until they allow me to self-host. Maybe I can reroute the local execution already and I just haven't seen it in the documentation?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d08151e-c199-4fcc-a2d7-2be6c893f83d", "embedding": null, "metadata": {"issue_id": 135, "title": "High Impact Security Issue - RCE vulnerability: server.py is inherently insecure and unsafe", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b72c7792-cb7f-4d33-ad99-82e7549bbd46", "node_type": "4", "metadata": {"issue_id": 135, "title": "High Impact Security Issue - RCE vulnerability: server.py is inherently insecure and unsafe", "state": "closed", "labels": [], "type": "issue"}, "hash": "5e35551cc771be64828a06c3d8326ad13826eed9d123f47c30d6bf18192d0ad7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: High Impact Security Issue - RCE vulnerability: server.py is inherently insecure and unsafe\n\nDescription: Hello, I was looking at the code and noticed a giant security hole. server.py line 29 [CODE_BLOCK] This is binding by default to all interfaces and allows any remote party that can connect to port 65432 to execute arbitrary python code from anywhere on the internet without restrictions. To fix this, the host value should be set to localhost \"127.0.0.1\" by default and if connections from the outside world are needed at all then there should be an IP whitelist perhaps as a parameter. Also make sure to manage this in your local firewall for the time being and block outside connections. I have a code fix PR in mind that I will likely submit once I understand why this file exists at all. I'm still trying to get a handle on the design of this library.,\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a74783c6-9d40-4ec9-ac46-aac794cd94d7", "embedding": null, "metadata": {"issue_id": 133, "title": "Getting error while installing smolagents depends on torch", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83a3055c-f69e-4046-83b7-5851f56668d0", "node_type": "4", "metadata": {"issue_id": 133, "title": "Getting error while installing smolagents depends on torch", "state": "closed", "labels": [], "type": "issue"}, "hash": "3c2a4ea3bba5dd7aa6372a86b593e6f2214b5485f9cb86d681f8ce18f5ff1e35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Getting error while installing smolagents depends on torch\n\nDescription: system specs: 1. macos - 15.1.1 2. M3 pro 18GB 3. python3 --version --> 3.13.0 4. pip --version --> 24.3.1 Commands i wrote [CODE_BLOCK] Error I'm getting: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab72e862-f11e-4f85-841c-0de280189251", "embedding": null, "metadata": {"issue_id": 131, "title": "Bug in long prompts for Tools from Spaces: Exception: File name too long", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1855b89-fce2-469d-bac2-562ed293d9de", "node_type": "4", "metadata": {"issue_id": 131, "title": "Bug in long prompts for Tools from Spaces: Exception: File name too long", "state": "closed", "labels": [], "type": "issue"}, "hash": "2958e36e87d30e3327b9e83396ce7efd28b17c6e14bd0bd778101a7c6b54cb65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Bug in long prompts for Tools from Spaces: Exception: File name too long\n\nDescription: Using the code from the example image_generation_tool code, I would expect to be able to use long prompts (within reason - at least not limited to valid file path length). Code: [CODE_BLOCK] Output: [CODE_BLOCK] OS: Mac OSX 15.0.1 Python version: Python 3.12.8 (uv 0.5.15) PS thanks for the cool library!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 413, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c6bf5b5-ae7c-40f4-b2ba-99024e216f4f", "embedding": null, "metadata": {"issue_id": 130, "title": "The end_action and end_code tags are not identical", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24b39c9d-138d-4f15-8fc0-ac2e3b507d8d", "node_type": "4", "metadata": {"issue_id": 130, "title": "The end_action and end_code tags are not identical", "state": "closed", "labels": [], "type": "issue"}, "hash": "5dd599f2e5e406f42dc9a421dfe7a27a26cd72a63af520c75d137c4cbb16fbf6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: The end_action and end_code tags are not identical\n\nDescription: Hi I have found a bug that may not affect the normal logic, but I believe it can be adjusted. The CODE_SYSTEM_PROMPT variable within the system prompts file provides instructions to the LLM that are terminated with <end_code>. However, in the Agents code files, end_action is used as the stop sequence, and the error messages used by the utils.parse_code_blob function also utilize <end_action>. This inconsistency between the system prompt and the code implementation may lead to certain issues.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b3df9a34-913f-4835-afd2-3a1ff309b1ec", "embedding": null, "metadata": {"issue_id": 127, "title": "How can I call a locally hosted LLM?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8b064182-5e06-4837-ba8f-f30c460d94f0", "node_type": "4", "metadata": {"issue_id": 127, "title": "How can I call a locally hosted LLM?", "state": "closed", "labels": [], "type": "issue"}, "hash": "fc0f5929a6b152ce5a66b714e71e4f15fa9b99fd7985d7be1d53b5aacf9cd9b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How can I call a locally hosted LLM?\n\nDescription: I didn't understand how to call a locally hosted LLM in LM Studio. Can someone help me with an example? Thanks\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 183, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1bfdd9d5-68d7-4a5b-880f-41de294ec219", "embedding": null, "metadata": {"issue_id": 125, "title": "Unable to import smolagents on my local jupyter", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e04b4289-c177-4b8e-ba1c-7ab3b492021e", "node_type": "4", "metadata": {"issue_id": 125, "title": "Unable to import smolagents on my local jupyter", "state": "closed", "labels": [], "type": "issue"}, "hash": "3a467d74a63e78d2c24503080194de6b9625f0d50499d2aa4316ca1787bfb3c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Unable to import smolagents on my local jupyter\n\nDescription: Code: [CODE_BLOCK] Ouput: [CODE_BLOCK] Environment: Hardware: Macos(Intel based) Python 3.9.13 Jupyter notebook\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 195, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e28cff4f-3e36-4f5b-a06d-556e59489fa5", "embedding": null, "metadata": {"issue_id": 124, "title": "Add support for additional remote code execution environments, e.g. Riza", "state": "open", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ee03e93-84ab-4388-bc4f-e0b65abb6041", "node_type": "4", "metadata": {"issue_id": 124, "title": "Add support for additional remote code execution environments, e.g. Riza", "state": "open", "labels": [], "type": "issue"}, "hash": "a8851b94e4b41230be61ecd6fc770bee45e74e380889346f0ab2f64220313e57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add support for additional remote code execution environments, e.g. Riza\n\nDescription: I'd like to play with this in some projects/demos for Riza (https://riza.io), which can remotely run Python/JS in isolation via an API. I'm happy to make an attempt at adding support myself, though most testing will be manual. > 1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it a feature related to something you need for a project? Is it something you worked on and think it could benefit the community? > > Whatever it is, we'd love to hear about it! I work on Riza so this is specifically useful to me and my team. But having multiple remote execution options would be generally helpful. > 2. Describe your requested feature in as much detail as possible. The more you can tell us about it, the better we'll be able to help you. The feature is just to add a class named RizaExecutor which the CodeAgent could use for remote code execution. See proposed code snippets below. > 3. Provide a *code snippet* that demonstrates the feature's usage. [CODE_BLOCK] > 4. If the feature is related to a paper, please include a link. N/A\n\nState: open", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2834c6e-182b-4c12-b660-c1c028d55926", "embedding": null, "metadata": {"issue_id": 122, "title": "Error in generating tool call with model", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb38fae5-0391-4370-93ba-310db49c48a3", "node_type": "4", "metadata": {"issue_id": 122, "title": "Error in generating tool call with model", "state": "closed", "labels": [], "type": "issue"}, "hash": "5dc635719c0936dc8afcbdf752766d84e1fc1516dfb2f80f86738e27249f4d42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Error in generating tool call with model\n\nDescription: I have very simple tool calling agent workflow and I am stuck with this error: [CODE_BLOCK] [CODE_BLOCK] Error that is seen [CODE_BLOCK] What should I do to get this working?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 251, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9026902e-24c5-4b4e-8caa-1ac72827cad2", "embedding": null, "metadata": {"issue_id": 120, "title": "Exclude examples from Ruff pre-commit hooks", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cf0fb00-f2f7-4c20-bd0d-34b8efb57441", "node_type": "4", "metadata": {"issue_id": 120, "title": "Exclude examples from Ruff pre-commit hooks", "state": "closed", "labels": [], "type": "issue"}, "hash": "0565c259855cd70c4b857dcf9bfba91b3a248ce8b5c2ee387f6eb6f14767403c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Exclude examples from Ruff pre-commit hooks\n\nDescription: Ruff pre-commit hooks raise errors for files located in the examples/ directory. It would be helpful to exclude examples/ from ruff to avoid unnecessary errors and enhance flexibility in examples/ .\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "332e82bf-a2fe-4d7b-80a8-42b731e9169a", "embedding": null, "metadata": {"issue_id": 118, "title": "Feature Request: use of XML as model generation structure", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9980b0f1-761f-4a31-9693-eaef0b963472", "node_type": "4", "metadata": {"issue_id": 118, "title": "Feature Request: use of XML as model generation structure", "state": "closed", "labels": [], "type": "issue"}, "hash": "ab702c129d1888d1d74eb440ede6495b668f4ed8ebcde10b35c919f60367bd7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feature Request: use of XML as model generation structure\n\nDescription: The current LLM models generate better quality results when tasked to generate XML than JSON see: https://medium.com/@isaiahdupree33/optimal-prompt-formats-for-llms-xml-vs-markdown-performance-insights-cef650b856db it make sense a XML is closely related to HTML which is the dominant form in the training datasets. This might particularly help weaker LLM. switching to XML for the model output will improve the performance of the agents (this can be then converted to JSON for better human readability and python usage.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 613, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e215c3dd-7e6b-4dfc-a8ec-78f4f2cd2a2f", "embedding": null, "metadata": {"issue_id": 117, "title": "Feature Request: Support for Multiple GPU Usage with `device_map=\"auto\"`", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "982dae58-a268-4e79-b669-bcb9be308d6d", "node_type": "4", "metadata": {"issue_id": 117, "title": "Feature Request: Support for Multiple GPU Usage with `device_map=\"auto\"`", "state": "closed", "labels": [], "type": "issue"}, "hash": "faea650e5ba103abfed0b626ab548a5685c18abf4ce1c45775f45bb01e217b21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feature Request: Support for Multiple GPU Usage with `device_map=\"auto\"`\n\nDescription: 1. Summary Currently, in the TransformersModel class of smolagents, the model is being allocated to a device using the .to(device) method. This approach restricts the usage to only one CUDA card. I would like to request the addition of an option to use device_map=\"auto\" to enable the utilization of multiple GPUs. 2. Problem Description 1. **Limited GPU Utilization**: With the current implementation, smolagents can only make use of a single CUDA device. This is a significant limitation, especially when dealing with large models that could benefit from parallel processing across multiple GPUs. For example, running a large language model for text generation tasks would be much faster if multiple GPUs could be used. 2. **Lack of Scalability**: As the size of models and the complexity of tasks grow, the ability to scale by using multiple GPUs becomes crucial. The current .to(device) method does not provide this scalability. There is the error I meet. Although I have many cudas, it only work on cuda:0. [CODE_BLOCK] 3. Proposed Solution 1. **Add device_map=\"auto\" Option**: In the TransformersModel class, when initializing the model, add an option to use device_map=\"auto\" instead of just .to(device). This would allow the model to automatically distribute across available GPUs, optimizing performance. For example, the code could be updated as follows: [CODE_BLOCK] 2. **Configuration for Users**: Provide a way for users to easily configure this option. This could be through a configuration file or an additional parameter when initializing the relevant classes in smolagents. 4. Benefits 1. **Performance Improvement**: Using multiple GPUs will significantly improve the inference speed of the models in smolagents. This will lead to faster response times, especially for computationally intensive tasks. 2. **Scalability**: It will make smolagents more suitable for large - scale projects and research, where the ability to scale the computing resources is essential. Thank you for considering this feature request. I believe it will greatly enhance the capabilities of smolagents.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1bf832dd-6781-4c1e-8128-ad93e9b47f02", "embedding": null, "metadata": {"issue_id": 115, "title": "Running Everything Offline", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77aa0109-a212-4e26-9792-4b652517eab2", "node_type": "4", "metadata": {"issue_id": 115, "title": "Running Everything Offline", "state": "closed", "labels": [], "type": "issue"}, "hash": "b6e8ade8359718994675b096200ef22b0b91b4b76fee50274d7d3cc6ff5fee55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Running Everything Offline\n\nDescription: Is it possible to run everything locally? What I can see in all the code examples is that the python code is using API calls all the time. For privacy concerns, I would like to be able to run everything locally. So, instead of sending data to a server that is running a language model, I would like to run the language model on my own computer. Here is an example code snippet of what it could look like: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 480, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99b2e3f6-0436-4878-813e-f6f6a9ac4549", "embedding": null, "metadata": {"issue_id": 114, "title": "parse_code_blob only parses first code blob in CodeAgent's Observation", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77171a2f-60a0-4588-a52a-cc1f1d59e303", "node_type": "4", "metadata": {"issue_id": 114, "title": "parse_code_blob only parses first code blob in CodeAgent's Observation", "state": "closed", "labels": [], "type": "issue"}, "hash": "9638f39c945477b2187c241a82dd023f93bd2256f2439bf91cee0e177ecbce16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: parse_code_blob only parses first code blob in CodeAgent's Observation\n\nDescription: https://github.com/huggingface/smolagents/blob/681758ae84a8075038dc676d8af7262077bd00c3/src/smolagents/utils.pyL108 Currently, when the LLM's observation contains multiple code blobs, only the first code blob ends up getting executed. I would expect that all code blobs written by the LLM should get executed? **Example:** Thought: Let's begin xxx... Code: [CODE_BLOCK] <end_code> Now, let's focus on zzz Code: [CODE_BLOCK] <end_code> **Output:** 1 **Expected output:** 1 2\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 580, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bee33763-fc7b-401c-830e-2aa002e0ce90", "embedding": null, "metadata": {"issue_id": 113, "title": "Fix broken Colab and Sagemaker notebook links", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9bc6ef5-8c6b-499e-aa74-c1abd4cfb4ee", "node_type": "4", "metadata": {"issue_id": 113, "title": "Fix broken Colab and Sagemaker notebook links", "state": "closed", "labels": [], "type": "issue"}, "hash": "393ecfa66df69b5ced58e9e2dee1392d08cc13deebfbcad445448bad156ae6c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Fix broken Colab and Sagemaker notebook links\n\nDescription: The documentation on huggingface <https://huggingface.co/docs/smolagents/guided_tour> seems to have broken links for colab and sagemaker. They both try opening the following link, which appears to be pointing to a non-existent file. - <https://github.com/huggingface/notebooks/blob/main/smolagents_doc/en/pytorch/guided_tour.ipynb> <img width=\"1507\" alt=\"smolagents_guided-tour_broken_colab-sagemaker_notebook\" src=\"https://github.com/user-attachments/assets/b971fc05-e5f7-4496-85b2-7f857a3635cd\" /> Time Accessed: 2025-01-08 00:47:09 EST\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "11f2549e-ba29-4e3e-80cb-8dfa30c7976c", "embedding": null, "metadata": {"issue_id": 112, "title": "Add support for PEP-561: type-hinting [`smolagents`]", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b998b02-05db-4a5c-8d66-f4bc41e9a056", "node_type": "4", "metadata": {"issue_id": 112, "title": "Add support for PEP-561: type-hinting [`smolagents`]", "state": "closed", "labels": [], "type": "issue"}, "hash": "6c43da4daa33b2ab092a91cf0dc9a4276334167b798ea7f6adf22e3b1b1e5591", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add support for PEP-561: type-hinting [`smolagents`]\n\nDescription: PEP-561 suggests how to add support for type-hinting to a python library. > Note: I have already pushed a PR for this. > - 111\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fb5beb6-d36b-4a01-82b2-45ec9a16f5a6", "embedding": null, "metadata": {"issue_id": 110, "title": "Using E2B with Tool Class", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0915a203-fa7a-4add-9723-b6e76f5f7f26", "node_type": "4", "metadata": {"issue_id": 110, "title": "Using E2B with Tool Class", "state": "closed", "labels": [], "type": "issue"}, "hash": "f2ca54386f4da6beb6b04e21469396091b36387cf254f5100bb6ffd4ff1c1614", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Using E2B with Tool Class\n\nDescription: When using E2B to run code remotely with a custom tool (inherited from Tool), I get errors like: Name 'os' is undefined. Name 'json' is undefined. - They are both standard python libs so not sure why I am getting this error. Any ideas? - Also is there a way to specify importing 'pandas as pd' etc? [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb3b6edc-58f9-4154-9a68-87e4b84fff06", "embedding": null, "metadata": {"issue_id": 108, "title": "Always getting the error: \"AssertionError exception: no description\" ", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adce918b-89e2-48f6-8a41-523dde252148", "node_type": "4", "metadata": {"issue_id": 108, "title": "Always getting the error: \"AssertionError exception: no description\" ", "state": "closed", "labels": [], "type": "issue"}, "hash": "d930e5c2087714a366dae2c35a2bbdb0cdbe27d31a7e12e96aeb68b0b83809c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Always getting the error: \"AssertionError exception: no description\" \n\nDescription: No matter what I do to modify the docstring I always get the same error as mentioned in the title. Here is a tool that I have created. I would like to know what within my docstrings is causing this. [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 317, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8dac19ef-f269-44bb-9416-9420b2ec3c3f", "embedding": null, "metadata": {"issue_id": 106, "title": "Agent from guided tour runs unrelated sample tasks on execution", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dd57e17-5502-40f2-be0d-554e822d4c7f", "node_type": "4", "metadata": {"issue_id": 106, "title": "Agent from guided tour runs unrelated sample tasks on execution", "state": "closed", "labels": [], "type": "issue"}, "hash": "881009609e05b554705dd90d7e1c963319c9b1808d97e789a4bd9c37ea0a7c6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Agent from guided tour runs unrelated sample tasks on execution\n\nDescription: I was working through the guided tour docs and ran the first example for getting a fibonacci number: [CODE_BLOCK] It'll usually get the right answer, but instead of returning it will often then try to answer very unrelated questions, such as how old is the pope or the population of various Chinese cities. On further review, it seems that both of these are from the task examples passed in as part of the system prompt (see here and here), so it seems that agent seems to be running them in addition to the user prompt. I've seen this happen with llama3.1, llama3.2, mistral and granite3.1-dense, so it doesn't seem to be model-specific. Here's the output of one such run where the agent return the pope's age instead of the fibonacci number (this was using llama3.2): [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "261863e2-9054-4a5c-b4e7-33aab01a46f1", "embedding": null, "metadata": {"issue_id": 105, "title": "It is not permitted to evaluate other functions than the provided tools or functions defined in previous code", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c6c2b7e7-a17b-46a5-b101-58a2d6137661", "node_type": "4", "metadata": {"issue_id": 105, "title": "It is not permitted to evaluate other functions than the provided tools or functions defined in previous code", "state": "closed", "labels": [], "type": "issue"}, "hash": "59bfb17ac58026028f5b394aafde2565a760ab295911ba79796311127056b95b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: It is not permitted to evaluate other functions than the provided tools or functions defined in previous code\n\nDescription: Hello, I am trying to run a CodeAgent that creates a csv output containing some sentiment analysis data. This is my agent definition: [CODE_BLOCK] But I am running into this error as attached in the screenshot below. Has anyone faced this? How to solve it? <img width=\"1388\" alt=\"Screenshot 2025-01-07 at 1 15 01 PM\" src=\"https://github.com/user-attachments/assets/7f040cb9-7738-4717-b448-2221885c01f8\" />\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d489ca5-d713-4b1e-af48-8a44b1b2ec3d", "embedding": null, "metadata": {"issue_id": 104, "title": "final_asnwer as Json", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c9b14dd-c5ab-4dc8-ba1f-5c357ef29049", "node_type": "4", "metadata": {"issue_id": 104, "title": "final_asnwer as Json", "state": "closed", "labels": [], "type": "issue"}, "hash": "a5480beedc0780a68cfefc5ef22c82294ab4aaf963dd98c1f9dfbbbf7836423d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: final_asnwer as Json\n\nDescription: I'm trying to get the agent's final answer in a JSON structure rather than a plain string. While defining the structure within the query works in most cases, it tends to break down in a multi-agent system. Does anyone have suggestions or best practices for handling this? Thanks!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "000751a1-eb2c-4a6d-94e4-38413557e922", "embedding": null, "metadata": {"issue_id": 103, "title": "[enhancement] Observability ", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53773306-772f-4b37-b531-a2bc9be2d7fe", "node_type": "4", "metadata": {"issue_id": 103, "title": "[enhancement] Observability ", "state": "closed", "labels": [], "type": "issue"}, "hash": "9f8481b206a461ff25d9ce1e9e58dba2f59e9ed7f81cc42b9160001282f1e588", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [enhancement] Observability \n\nDescription: It seems to me that a good thing would be to add integration with (open source) observability tools such as langfuse. I have not been able to run langfuse with litellm@smolagents, so I propose to do a similar integration.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7e219e9-0995-45fb-8c20-ac5cc8190266", "embedding": null, "metadata": {"issue_id": 102, "title": "[not bug] call LLM without agents", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c388cab-6ab8-4cff-941b-2444320e682d", "node_type": "4", "metadata": {"issue_id": 102, "title": "[not bug] call LLM without agents", "state": "closed", "labels": [], "type": "issue"}, "hash": "235de44e39a9e7e6e9f00c389ff704074bf52b9d4dd6ba036073cd5a028fa5e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [not bug] call LLM without agents\n\nDescription: Hi, maybe I missed it somewhere in the examples or docs, but is there a way to make a simple call to LLM with a prompt but without it being an agent call, so there are no tools or retries? A straight simple LLM prompt call without agents trying to figure anything out and do cycle after cycle of retries? (like ell or pydantic-ai)? Thanks\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 408, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e1fbb0c0-f575-4c2a-ac25-a826cd40f9c5", "embedding": null, "metadata": {"issue_id": 101, "title": "Code Agent -> max_iterations", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4f5884b-8ce8-4c3a-9b4b-d7abd6614607", "node_type": "4", "metadata": {"issue_id": 101, "title": "Code Agent -> max_iterations", "state": "closed", "labels": [], "type": "issue"}, "hash": "681e8a3bc9f38b3262c1933d30fb97cec9b5ed2b7f56df1f958c2feadcf2e778", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Code Agent -> max_iterations\n\nDescription: CodeAgent stops after 5 iterations. Any way to explicitely increase this limit ?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b16c1a1-b129-438e-a41e-41b30e05edd4", "embedding": null, "metadata": {"issue_id": 100, "title": "Importing smolagents Takes Excessive Time", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f804970-7366-4bd9-aeec-5db8572dea4f", "node_type": "4", "metadata": {"issue_id": 100, "title": "Importing smolagents Takes Excessive Time", "state": "closed", "labels": [], "type": "issue"}, "hash": "7e04b525e01f2c16976920b73d7eaabc4d3f930dc5e730dca52edefbfd1bf6af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Importing smolagents Takes Excessive Time\n\nDescription: Bug Report: Importing smolagents Takes Excessive Time **Description** Importing the smolagents module takes an unusually long time, which affects usability, especially for non-transformer-related tools. The long import time seems to be caused by the initialization of heavy dependencies like transformers and torch, even when they are not required for certain tools. **Steps to Reproduce** Run the following script to benchmark the import time: [CODE_BLOCK] **Observed Behavior** [CODE_BLOCK] **Expected Behavior** The smolagents module should import significantly faster, especially when non-transformer-related tools (e.g., DuckDuckGoSearchTool) are being used. The framework should avoid loading unnecessary dependencies during import or should defer their initialization using lazy loading. **Possible Cause** The long import time appears to be due to the initialization of dependencies like: - transformers - torch These dependencies are loaded even when they are not required for certain tools or agents. As an agent framework, smolagents should minimize the overhead of importing the module by: 1. Not loading dependencies unrelated to the tools in use. 2. Implementing lazy loading for modules like transformers and torch. **Environment** - smolagents version: 1.1.0 - Python version: 3.12 - Operating System: macOS/Linux/Windows **Proposed Solution** - Implement lazy loading for dependencies like transformers and torch. - Optimize the import path to avoid initializing unrelated components when using non-transformer-related tools. **Impact** The high import time reduces the performance and efficiency of the framework, especially in scenarios where only lightweight tools like DuckDuckGoSearchTool are required. **Additional Context** This issue significantly impacts workflows where smolagents is used as a lightweight agent framework for tools that do not require deep learning components.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1982, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83ca5208-d1ab-4942-a518-63e4091d3225", "embedding": null, "metadata": {"issue_id": 97, "title": "Error (?) in multi-agent example", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dadc905-e351-4256-8708-093184c97ae4", "node_type": "4", "metadata": {"issue_id": 97, "title": "Error (?) in multi-agent example", "state": "closed", "labels": [], "type": "issue"}, "hash": "38166fc260709c557c811cabb1dd93af0f2419b934a82257294d403c713ec48b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Error (?) in multi-agent example\n\nDescription: OS: Windows Python: 3.11 Smolagents version: v1.1.0 I was following this example listed here for multi-agents (https://huggingface.co/docs/smolagents/examples/multiagents). It seems when I run the code I always get this error: [CODE_BLOCK] I am not sure if this is exactly an error or its intended nature. This client error always happens with the Qwen/Qwen2.5-Coder-32B-Instruct model listed in the example. I tried running with mistralai/Mistral-7B-Instruct-v0.3, got different errors: [CODE_BLOCK] It does produce an answer however, once it reaches max steps. I would like to clarify how to resolve these errors. Heres the final answer: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a3bec7f7-c82c-4b1a-ad28-0efd875acd94", "embedding": null, "metadata": {"issue_id": 96, "title": "Add generic model interface using HTTP and/or OpenAI spec", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c65b3d3-5056-4f45-9a08-e00cc23333bf", "node_type": "4", "metadata": {"issue_id": 96, "title": "Add generic model interface using HTTP and/or OpenAI spec", "state": "closed", "labels": [], "type": "issue"}, "hash": "f0ce6697d8dd79909b855a22761c92af5425b66372ee84d9287364959ee8e219", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Add generic model interface using HTTP and/or OpenAI spec\n\nDescription: This is a feature request for a generic HTTP LLM interface or even simply an OpenAI API compatible interface. I was excited to use this _small_ library when I realized that to test anything with local models I first needed to also have a full instance of LiteLLM setup just to be able to run inference against a model I already have running on my machine. Cue me spending the next 20 minutes getting their docker container up and running just so it can sit as an unnecessary abstraction layer between smolagents and the inference engine serving my model, which already has an OpenAI compatible API. I'd love to use this framework, but _requiring_ this additional middleware without any option for a standard HTTP interface seems like a major oversight, practically every hosted and local provider is already offering an OpenAI compatible API.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 936, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45478d6c-b933-4dcd-b206-d453b6abe138", "embedding": null, "metadata": {"issue_id": 88, "title": "private API key or template ID in public source", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "094c8c63-65d9-43bd-9271-b7bcb0f1495b", "node_type": "4", "metadata": {"issue_id": 88, "title": "private API key or template ID in public source", "state": "closed", "labels": [], "type": "issue"}, "hash": "ec47de06183ddfc0fa4f13bc8ea092e5c10a7cd66d169990003cd58eb88d0661", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: private API key or template ID in public source\n\nDescription: https://github.com/huggingface/smolagents/blob/4fa825537754fca564674f58d9ff519a88a288ac/src/smolagents/e2b_executor.pyL36 Somebody should remove that comment.....\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 246, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bead7904-05c6-4cd7-b641-be3317e1b442", "embedding": null, "metadata": {"issue_id": 87, "title": "Invalid usage of tool by agent", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b74a810-fbe0-4436-b1a7-07d598af9f6b", "node_type": "4", "metadata": {"issue_id": 87, "title": "Invalid usage of tool by agent", "state": "closed", "labels": [], "type": "issue"}, "hash": "9c2f6e998e0accb48c9350dabc9275518aea8f782ffd2ac4786232473aad1a8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Invalid usage of tool by agent\n\nDescription: My code for tool is: [CODE_BLOCK] tool is used by agent: developer_web_agent = CodeAgent(tools=[folder_creator, file_creator], model=llm_model) [CODE_BLOCK] Then manager is handling: [CODE_BLOCK] What happens is that agent reads the schema kinda-ok, but then calls it improperly: [CODE_BLOCK] instead of expected: file_creator('folder_target': folder_target, 'file_name': file_name, 'file_contents': file_contents)\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9bb668fa-4c13-4f9f-bdec-ef23b870243b", "embedding": null, "metadata": {"issue_id": 86, "title": "Bug in built-in DuckDuckGoSearchTool", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80d594a7-b922-4861-b017-d18c135ccebb", "node_type": "4", "metadata": {"issue_id": 86, "title": "Bug in built-in DuckDuckGoSearchTool", "state": "closed", "labels": [], "type": "issue"}, "hash": "f448102c8189916384a1fe32e82c94943b9f25b15d4c1fdfaa5968b7c0bc61d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Bug in built-in DuckDuckGoSearchTool\n\nDescription: The description and output_type imply that the tool 'returns the top search results as a list of dict elements': [CODE_BLOCK] While the forward method simply returns a semi-structured string: [CODE_BLOCK] This leads to generated code expecting results to be a dict when it's really a string: [CODE_BLOCK] which causes this error: You're trying to subscript a string with a string index.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb10dccd-baa4-4266-b436-ca3cf7667a1c", "embedding": null, "metadata": {"issue_id": 83, "title": "How to save/extract executed code", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54e83808-934b-40ac-95c5-63eccb738eca", "node_type": "4", "metadata": {"issue_id": 83, "title": "How to save/extract executed code", "state": "closed", "labels": [], "type": "issue"}, "hash": "49c23c71031169f4efcc1f5878d2bbceae5b94ba5da3b5b02d19ba30466a28ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to save/extract executed code\n\nDescription: Is it possible to save the executed code? It's already in the log. It will be very useful. ex. ``` \u256d\u2500 Executing this code: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 1 attractions_list = [ \u2502 \u2502 2 [\"Attraction\", \"Description\"], \u2502 \u2502 3 [\"Sensoji Temple\", \"The oldest temple in Tokyo, offering beautiful architecture and a rich history.\"], \u2502 \u2502 4 [\"Nakamise Shopping Street\", \"A historic shopping street with souvenirs and traditional snacks.\"], \u2502 \u2502 5 [\"Kibi Dango\", \"A traditional rice cake snack available at Nakamise Street.\"], \u2502 \u2502 6 [\"Asakusa Jinja\", \"A historic Shinto shrine that survived the bombings during WWII.\"], \u2502 \u2502 7 [\"Kimono Experience\", \"Rent a kimono and walk around Asakusa.\"], \u2502 \u2502 8 [\"Asakusa Culture Tourist Information Center\", \"A building with unique architecture, great for photos.\"], \u2502 \u2502 9 [\"Tokyo Skytree\", \"The tallest structure in Tokyo, offering panoramic views.\"], \u2502 \u2502 10 [\"Hanayashiki\", \"Japan\u2019s oldest amusement park with nostalgic charm.\"], \u2502 \u2502 11 [\"Demboin Garden\", \"A serene Japanese garden adjacent to Sensoji Temple.\"], \u2502 \u2502 12 [\"Azuma-bashi Bridge\", \"An iconic bridge offering views of the Tokyo Skytree.\"] \u2502 \u2502 13 ] \u2502 \u2502 14 \u2502 \u2502 15 Convert the list to CSV format (string) \u2502 \u2502 16 csv_data = \"\\n\".join([\",\".join(row) for row in attractions_list]) \u2502 \u2502 17 \u2502 \u2502 18 Save the CSV data to file \u2502 \u2502 19 save_csv(data=csv_data, filename='asakusa_trip.csv') \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69018ced-b162-4acb-bc25-7342f9176da7", "embedding": null, "metadata": {"issue_id": 82, "title": "Arbitrary callable objects passed to the Executor", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69d3ce39-2bec-41ae-8e55-4d80b10f7825", "node_type": "4", "metadata": {"issue_id": 82, "title": "Arbitrary callable objects passed to the Executor", "state": "closed", "labels": [], "type": "issue"}, "hash": "42a07dfa3c390fed2af6e4f4593c4bd648aaf660ed13e20582441b16f6ecf664", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Arbitrary callable objects passed to the Executor\n\nDescription: Hey team, first off thanks for this great project, I'll definitely be spending more time with it! I was looking at the interpreter and found that you can pass in callable objects to the interpreter, which bypasses the restrictions imposed on importing modules. Here is a basic script that passes in a lambda that calls os.remove(): [CODE_BLOCK] This works even though os is not in the whitelist of allowed modules. I _think_ this would only be an issue if a user directly imports an unsafe module within the scope of where an instance of CodeAgent runs, i.e. the agent can pass in something like os.remove() into the interpreter without importing os since os is already in scope. I'm wondering if it would be worth it to add some additional checks. Let me know what you think\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f6ddfc99-3563-4246-8c13-d7af19d39e9c", "embedding": null, "metadata": {"issue_id": 81, "title": "Lack of Gemini Model Integration Without LiteLLM(vertexAI)", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80bfcf5e-59b5-452e-b9d8-757dacf7ca13", "node_type": "4", "metadata": {"issue_id": 81, "title": "Lack of Gemini Model Integration Without LiteLLM(vertexAI)", "state": "closed", "labels": [], "type": "issue"}, "hash": "7205369244ac4bc4b85a7fcb779644894f4b4049b662bddbd30d23a3c75a047e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Lack of Gemini Model Integration Without LiteLLM(vertexAI)\n\nDescription: Issue Summary: The current implementation of the smolagent does not support integration with the Gemini language model. This limitation hinders developers who wish to leverage the advanced capabilities of the Gemini model in their applications. Gemini's advanced features, such as configurable parameters (temperature, top_p, top_k), high token limits, and robust chat session management, make it a valuable addition for modern AI-driven workflows. Proposed Solution: The smolagent framework can integrate the Gemini model by defining a specialized GeminiLLM class and updating the existing infrastructure to support this new model. Below is an outline of the integration steps: GeminiLLM Class Definition: The GeminiLLM class encapsulates the configuration and interaction with the Gemini API. It includes methods for initializing the model, sending prompts, and handling stop sequences. Updating the Agent: Modify the smolagent's architecture to include Gemini as a supported model. This involves adding conditional logic or a plugin-based system to initialize the appropriate LLM based on user preference. **code snippet for GeminiLLM class** [CODE_BLOCK] If the community agrees, I can work on implementing this integration and submit a pull request. Feedback and suggestions on this proposal are welcome!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4ab1a82-c458-4072-85da-f8b8b0a387ce", "embedding": null, "metadata": {"issue_id": 80, "title": "\u672c\u5730\u90e8\u7f72\u89c6\u9891\u6559\u7a0b", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be20dc36-320d-4fcb-9e24-7c7614392b1d", "node_type": "4", "metadata": {"issue_id": 80, "title": "\u672c\u5730\u90e8\u7f72\u89c6\u9891\u6559\u7a0b", "state": "closed", "labels": [], "type": "issue"}, "hash": "4baa37dccc2eb91630e065477f51dcea8bbc1a68a7adb25b2fceaba983c90126", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: \u672c\u5730\u90e8\u7f72\u89c6\u9891\u6559\u7a0b\n\nDescription: https://youtu.be/wwN3oAugc4c\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 73, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fadefee5-ce9d-4d25-bf6f-6eb2fdcdd7d5", "embedding": null, "metadata": {"issue_id": 78, "title": "Support for \"complex\" in python executor", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df53f2b1-665f-4723-9ddf-e90ba2ea63dd", "node_type": "4", "metadata": {"issue_id": 78, "title": "Support for \"complex\" in python executor", "state": "closed", "labels": [], "type": "issue"}, "hash": "230cdd8ba974edbe4b89079026d28dc61e167c29a307e5b96bd5beed418feac6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Support for \"complex\" in python executor\n\nDescription: The following code produced by DeepSeek fails: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f2eab16-e834-44d6-ae70-eb083c80eb67", "embedding": null, "metadata": {"issue_id": 77, "title": "I need an Android version", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e939b06e-4366-4474-8fad-d199bc72c5f8", "node_type": "4", "metadata": {"issue_id": 77, "title": "I need an Android version", "state": "closed", "labels": [], "type": "issue"}, "hash": "cdac4b640c760b8a53cb6a0c41b43921de70f4c161c87dc54412d04467a43e7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: I need an Android version\n\nDescription: Run on android os ...\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 83, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69a0fd0c-3615-447b-aa0d-8b9573feedac", "embedding": null, "metadata": {"issue_id": 76, "title": "Simple Presentation Maker using smolagents", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df19cd34-e41e-4e16-b3de-8176fca524b5", "node_type": "4", "metadata": {"issue_id": 76, "title": "Simple Presentation Maker using smolagents", "state": "closed", "labels": [], "type": "issue"}, "hash": "e2e8a145a45b707c6c0f3d1120c7f204bb452aa009809aca2dcc5509b455a6d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Simple Presentation Maker using smolagents\n\nDescription: https://colab.research.google.com/drive/1UxQyXsMppbo7-vf9oXwmBn_rE1qXNCoA?usp=sharing\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 164, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a31cc2d8-9677-4345-bac3-9b336a7b076e", "embedding": null, "metadata": {"issue_id": 73, "title": "I was under the impression CodeAgent is an interface, but it is in fact a tool. Why?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7a61cb6-ad15-498f-8e17-00dbef63e603", "node_type": "4", "metadata": {"issue_id": 73, "title": "I was under the impression CodeAgent is an interface, but it is in fact a tool. Why?", "state": "closed", "labels": [], "type": "issue"}, "hash": "028da4dc4005e72a6881adb8dbca921ee8e4727b953271a0e0eca35ef3d010b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: I was under the impression CodeAgent is an interface, but it is in fact a tool. Why?\n\nDescription: @aymeric-roucher I was trying to setup an agent that utilized tools A and B. Where A had the ability to **code its own solutions** and B was able to search for popular models on HF. Once I ran it I realized that CodeAgent is not an interface but a tool itself that superseded A. Afterwords I tried to resolve that by looking for other less opinionated agents and ran into MultiStepAgent and ToolCallingAgent but I was not able to find any guidance on how to make them work (I was looking into them and I know you have to do something with step but **a guide would help a ton here**). A guide to use these less opinionated agents would also help in reflecting what the objective of smolagents is. I know I just happened to run into the one type of tool that may cause this error but nonetheless this is an intrinsic problem with smolagents. I should be able to call any and all tools from some smolagents interface whether that is CodeAgent or something else. Do you disagree? If so why? I think this is a good case study to solve and make this framework more less restrictive. So how can I make my agent work? Here is my code: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "347b9536-7700-40a5-8c6d-b90135777d2e", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c96f13f-1432-4f90-b5c2-33628c1b1ab1", "node_type": "1", "metadata": {}, "hash": "b0d4d92f95a0b1c4aa111b4bd9d2d105ad18c3dcd29af3affc017e100e4ce35e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.\n\nDescription: Hello there. I found it a bit strange, that even the most simple example, when the agent is given with the only mock tool (https://github.com/huggingface/smolagents/blob/main/examples/tool_calling_agent_ollama.py) is barely working for most of the small models I tested it with. Below is the code I was running. The only difference to original example is that I modified the model_id to make it compatible to work with ollama 0.4.7, as the code example in this repo did not work at all: [CODE_BLOCK] <details> <summary> With model_id=\"ollama/llama3.2-32k\" (made a copy of llama3.2 model in ollama to extend context size to 32k tokens) the agent did not recognize the output from the very first run of tool and keep executing it again and again. Also I can see tokens are spent like crazy - might be some kind of bug or inefficient LLM prompting. </summary> \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 What's the weather like in Paris? \u2502 \u2502 \u2502 \u2570\u2500 LiteLLMModel - ollama/llama3.2-32k \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 0 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'get_weather' with arguments: {'location': 'Paris', 'celsius': False} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Observations: The weather is UNGODLY with torrential rains and temperatures below -10\u00b0C [Step 0: Duration 0.26 seconds| Input tokens: 1,175 | Output tokens: 23] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'get_weather' with arguments: {'location': 'Paris', 'celsius': False} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Observations: The weather is UNGODLY with torrential rains and temperatures below -10\u00b0C [Step 1: Duration 0.19 seconds| Input tokens: 2,469 | Output tokens: 46] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'get_weather' with arguments: {'location': 'Paris', 'celsius': False} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Observations: The weather is UNGODLY with torrential rains and temperatures below -10\u00b0C [Step 2: Duration 0.19 seconds| Input tokens: 3,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c96f13f-1432-4f90-b5c2-33628c1b1ab1", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "347b9536-7700-40a5-8c6d-b90135777d2e", "node_type": "1", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "7388a894083e8bdfe014586ce4da09b80c10dfbc0964ec3b12eb36572522e0fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a0d2f9c-cfb6-4382-b967-de7151d769af", "node_type": "1", "metadata": {}, "hash": "9d688c6c5189af840cc85d766c929e88d07b95f34f36f7be4ffc1c3c966d379d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "19 seconds| Input tokens: 2,469 | Output tokens: 46] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'get_weather' with arguments: {'location': 'Paris', 'celsius': False} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Observations: The weather is UNGODLY with torrential rains and temperatures below -10\u00b0C [Step 2: Duration 0.19 seconds| Input tokens: 3,882 | Output tokens: 69] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 3 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'get_weather' with arguments: {'location': 'Paris', 'celsius': False} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Observations: The weather is UNGODLY with torrential rains and temperatures below -10\u00b0C [Step 3: Duration 0.18 seconds| Input tokens: 5,414 | Output tokens: 92] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'get_weather' with arguments: {'location': 'Paris', 'celsius': False} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Observations: The weather is UNGODLY with torrential rains and temperatures below -10\u00b0C [Step 4: Duration 0.26 seconds| Input tokens: 7,061 | Output tokens: 115] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'get_weather' with arguments: {'location': 'Paris', 'celsius': False} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Observations: The weather is UNGODLY with torrential rains and temperatures below -10\u00b0C [Step 5: Duration 0.16 seconds| Input tokens: 8,825 | Output tokens: 138] Reached max iterations. Final answer: It seems that the agent got stuck in a loop, continuously calling itself without resolving the issue. Since we can't make any new external calls, I'll do my best to provide a general answer based on the available information. Unfortunately, it appears that the current state of the weather in Paris is quite severe, with torrential rains and temperatures below -10\u00b0C. However, please note that this information might not be up-to-date or entirely accurate, as the agent's memory seems to have gotten stuck in an infinite loop. For the most accurate and recent weather updates, I recommend checking a reliable source such as the official Paris tourist board website, AccuWeather, or the National Weather Service. They will be able to provide you with the latest information on the current weather conditions in Paris. [Step 6: Duration 0.00 seconds| Input tokens: 9,637 | Output tokens: 297] It seems that the agent got stuck in a loop, continuously calling itself without resolving the issue. Since we can't make any new external calls, I'll do my best to provide a general answer based on the available information.", "mimetype": "text/plain", "start_char_idx": 2503, "end_char_idx": 6100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a0d2f9c-cfb6-4382-b967-de7151d769af", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c96f13f-1432-4f90-b5c2-33628c1b1ab1", "node_type": "1", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e1068e5815f98af1323f9e8d134b1d320900700f4a6be7a5bb7152c40febf4b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5efcc11-1515-4e36-8765-30d291d2f575", "node_type": "1", "metadata": {}, "hash": "daf993ff40277a4ca88dbb738205a7364d1414869c81ecef57489ccd1d454635", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unfortunately, it appears that the current state of the weather in Paris is quite severe, with torrential rains and temperatures below -10\u00b0C. However, please note that this information might not be up-to-date or entirely accurate, as the agent's memory seems to have gotten stuck in an infinite loop. For the most accurate and recent weather updates, I recommend checking a reliable source such as the official Paris tourist board website, AccuWeather, or the National Weather Service. They will be able to provide you with the latest information on the current weather conditions in Paris. [Step 6: Duration 0.00 seconds| Input tokens: 9,637 | Output tokens: 297] It seems that the agent got stuck in a loop, continuously calling itself without resolving the issue. Since we can't make any new external calls, I'll do my best to provide a general answer based on the available information. Unfortunately, it appears that the current state of the weather in Paris is quite severe, with torrential rains and temperatures below -10\u00b0C. However, please note that this information might not be up-to-date or entirely accurate, as the agent's memory seems to have gotten stuck in an infinite loop. For the most accurate and recent weather updates, I recommend checking a reliable source such as the official Paris tourist board website, AccuWeather, or the National Weather Service. They will be able to provide you with the latest information on the current weather conditions in Paris. </details> <details> <summary> With model_id=\"ollama/falcon3:10b\" the code fails somewhere in-between. Looks like the model doesn't \"want\" to generate patterns to call the function. </summary> \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 What's the weather like in Paris? \u2502 \u2502 \u2502 \u2570\u2500 LiteLLMModel - ollama/falcon3:10b \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 0 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 0: Duration 6.70 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'.", "mimetype": "text/plain", "start_char_idx": 5210, "end_char_idx": 8459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5efcc11-1515-4e36-8765-30d291d2f575", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a0d2f9c-cfb6-4382-b967-de7151d769af", "node_type": "1", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "2be2f6cfd61547b1a331bc591c93fb013216ed9cdab21e0787248369d04f9c0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "573ea607-148e-4edb-89a8-6544116a7eb6", "node_type": "1", "metadata": {}, "hash": "136024ee314867ed1d18f405252ba0fecef500645f86daa1d1f4ce1d2e5012ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 1: Duration 0.39 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 2: Duration 0.33 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 3 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 3: Duration 0.39 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'.", "mimetype": "text/plain", "start_char_idx": 8460, "end_char_idx": 11552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "573ea607-148e-4edb-89a8-6544116a7eb6", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5efcc11-1515-4e36-8765-30d291d2f575", "node_type": "1", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "17cf871868f293a229db3d3275628453b853d5d29e3b75f9194768ef53f2272c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b8cff96-cb20-4957-80a8-81419b1ee559", "node_type": "1", "metadata": {}, "hash": "295285534e23048e86fbcbf1df8559fd1fc40e856158f838b3b6e48a01d3f693", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 4: Duration 0.41 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 5: Duration 0.32 seconds] Reached max iterations. Final answer: I'm sorry, but I don't have real-time data access to current weather conditions. For the most accurate and up-to-date weather information in Paris, please check a reliable weather forecasting service or website. [Step 6: Duration 0.00 seconds| Input tokens: 98 | Output tokens: 47] I'm sorry, but I don't have real-time data access to current weather conditions. For the most accurate and up-to-date weather information in Paris, please check a reliable weather forecasting service or website. </details> <details> <summary> Same story goes with model_id=\"ollama/llama3.2-vision-16k\" which is 10B model. Out of 5 attempts none has called the function. </summary> \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 What's the weather like in Paris? \u2502 \u2502 \u2502 \u2570\u2500 LiteLLMModel - ollama/llama3.2-vision-16k \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 0 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'.", "mimetype": "text/plain", "start_char_idx": 11553, "end_char_idx": 14593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b8cff96-cb20-4957-80a8-81419b1ee559", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "573ea607-148e-4edb-89a8-6544116a7eb6", "node_type": "1", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "70e09893a703ca3e81cd48c9e8e3d92637309c2d5f71fa450307af39a8b68e3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8170287-8640-4e7f-8292-7712388af2df", "node_type": "1", "metadata": {}, "hash": "a07ed5c5efa1de127ec4ab7f4c21461fa22be448614a7815d85d135fbd80b7ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Out of 5 attempts none has called the function. </summary> \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 What's the weather like in Paris? \u2502 \u2502 \u2502 \u2570\u2500 LiteLLMModel - ollama/llama3.2-vision-16k \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 0 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 0: Duration 0.43 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 1: Duration 0.28 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'.", "mimetype": "text/plain", "start_char_idx": 13991, "end_char_idx": 16655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c8170287-8640-4e7f-8292-7712388af2df", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b8cff96-cb20-4957-80a8-81419b1ee559", "node_type": "1", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "ba342b9e22b42cd9b1830fd04d58ac32c364d97c44229d1820cd51c178f40e39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb048af3-f529-4ef3-8a26-1bf652c1c024", "node_type": "1", "metadata": {}, "hash": "6893a582bb147faf83729e671cd6950dbb79a6434b5de6feb98ff3e478c6e941", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 2: Duration 0.27 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 3 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 3: Duration 0.27 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 4: Duration 0.40 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'.", "mimetype": "text/plain", "start_char_idx": 16656, "end_char_idx": 19748, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb048af3-f529-4ef3-8a26-1bf652c1c024", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8170287-8640-4e7f-8292-7712388af2df", "node_type": "1", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "b35435d2eb38c2035949ee86298ccdce8d772ac01aebb1af5cb5903d9a5210ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58206a6f-83b2-4f8c-9579-d6cd3c5bac19", "node_type": "1", "metadata": {}, "hash": "d7fae8519b591e42210b21f3027a98b898ed7c9bc6f146cc94d725c31b5fe13b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 5: Duration 0.31 seconds] Reached max iterations. Final answer: I'd be happy to help answer the user's question! According to current weather reports, the weather in Paris is mostly cloudy with a high of 18\u00b0C (64\u00b0F) and a low of 12\u00b0C (54\u00b0F). There is a moderate chance of scattered showers throughout the day. [Step 6: Duration 0.00 seconds| Input tokens: 85 | Output tokens: 60] I'd be happy to help answer the user's question! According to current weather reports, the weather in Paris is mostly cloudy with a high of 18\u00b0C (64\u00b0F) and a low of 12\u00b0C (54\u00b0F). There is a moderate chance of scattered showers throughout the day. </details> <details> <summary> model_id=\"ollama/granite3.1-dense\" did it slightly better. The tool was called 2 times out of 4 attempts </summary> \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 What's the weather like in Paris? \u2502 \u2502 \u2502 \u2570\u2500 LiteLLMModel - ollama/granite3.1-dense:8b \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 0 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'get_weather' with arguments: {'location': 'Paris'} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Observations: The weather is UNGODLY with torrential rains and temperatures below -10\u00b0C [Step 0: Duration 4.99 seconds| Input tokens: 1,369 | Output tokens: 19] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'.", "mimetype": "text/plain", "start_char_idx": 19749, "end_char_idx": 22383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58206a6f-83b2-4f8c-9579-d6cd3c5bac19", "embedding": null, "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a48b26d-2e77-4fb3-a2cb-5a0dce80fec7", "node_type": "4", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "e376370a4131123fced32a6f0b284b55df70cb246cd9a28036c5ebd3e1f07da7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb048af3-f529-4ef3-8a26-1bf652c1c024", "node_type": "1", "metadata": {"issue_id": 71, "title": "The built-in example tool_calling_agent_ollama.py is very unsatisfactory with small models (llama 3.2, falcon3:10b, llama3.2-vision, granite3.1-dense). Tokens are overspent, functions are not being called.", "state": "closed", "labels": [], "type": "issue"}, "hash": "2bc6966c2ce71c430daed134b53bb51d423a57724314a7b2c7053ab215febf57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 1: Duration 1.02 seconds| Input tokens: 2,738 | Output tokens: 38] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: 'name' Traceback (most recent call last): File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/main.py\", line 2693, in completion response = base_llm_http_handler.completion( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 334, in completion return provider_config.transform_response( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/root/miniconda3/envs/jupyter/lib/python3.12/site-packages/litellm/llms/ollama/completion/transformation.py\", line 263, in transform_response \"name\": function_call[\"name\"], ~~~~~~~~~~~~~^^^^^^^^ KeyError: 'name' [Step 2: Duration 0.99 seconds| Input tokens: 4,107 | Output tokens: 57] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 3 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Calling tool: 'final_answer' with arguments: {'answer': 'The weather in Paris is ungodly with torrential rains \u2502 \u2502 and temperatures below -10\u00b0C.'} \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Final answer: The weather in Paris is ungodly with torrential rains and temperatures below -10\u00b0C. [Step 3: Duration 0.85 seconds| Input tokens: 5,620 | Output tokens: 147] The weather in Paris is ungodly with torrential rains and temperatures below -10\u00b0C. </details> I see a big potential of closing the gap in terms of tools usage - to use them as code, but I'm unsure if the approach that was selected in this framework is reliable one. I'm especially worried about huge tokens wastage - like up to 2k for every step. Not sure if this economical.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 22384, "end_char_idx": 25324, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a2c41d4d-2293-4dbd-a07b-464b38bcb60d", "embedding": null, "metadata": {"issue_id": 70, "title": "Assistant on this github repo", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b35e620e-a41e-4231-8d36-bd10277b2b00", "node_type": "4", "metadata": {"issue_id": 70, "title": "Assistant on this github repo", "state": "closed", "labels": [], "type": "issue"}, "hash": "81e5d089c41fe6e5d275e58dc150c6bf5278cd97eb5f4c5986b0b22c7d8699e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Assistant on this github repo\n\nDescription: https://hf.co/chat/assistant/67799333831c2edbbfd91c81\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 119, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "669c30c6-1643-4a76-b45e-5984518938b8", "embedding": null, "metadata": {"issue_id": 69, "title": "Import a Space as a tool: Exception: Tool's 'forward' method should take 'self' as its first argument, ...", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "330833a5-697f-4f65-9375-d3f88b953554", "node_type": "4", "metadata": {"issue_id": 69, "title": "Import a Space as a tool: Exception: Tool's 'forward' method should take 'self' as its first argument, ...", "state": "closed", "labels": [], "type": "issue"}, "hash": "83e401f75cc6e839a3e03be487dcbd99fa418c03f1e56e9ae44f9c8343161fd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Import a Space as a tool: Exception: Tool's 'forward' method should take 'self' as its first argument, ...\n\nDescription: Hi, Going through the smolagents documentation :-) :-) The Import a Space as a tool example is not running as expected. Code: [CODE_BLOCK] Output: [CODE_BLOCK] Environment: - smolagents==1.0.0 - torch==2.5.1 - macOS 15.2 (M processor)\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 377, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dcb9413c-29c3-4fd5-865e-0d5afb7c189a", "embedding": null, "metadata": {"issue_id": 67, "title": "Modifications to the system prompts", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c68071f-3ea9-4d94-bae3-baa01ab65d75", "node_type": "4", "metadata": {"issue_id": 67, "title": "Modifications to the system prompts", "state": "closed", "labels": [], "type": "issue"}, "hash": "f96308d0f45c62f1360af06725b8b82aade2668c1834609b52c9713ed4eb7fe2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Modifications to the system prompts\n\nDescription: In the guided tour you have provided this code for modifying system prompts from smolagents import ToolCallingAgent, PythonInterpreterTool, TOOL_CALLING_SYSTEM_PROMPT modified_prompt = TOOL_CALLING_SYSTEM_PROMPT agent = ToolCallingAgent(tools=[PythonInterpreterTool()], model=model, system_prompt=modified_prompt) I don't get it which part of the code will I be modifying the code?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 453, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "853da8be-c909-4f8d-bef2-830c5a92912c", "embedding": null, "metadata": {"issue_id": 66, "title": "Gradio code is throwing error", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b84e939b-e493-4b34-8528-41f52d2620f7", "node_type": "4", "metadata": {"issue_id": 66, "title": "Gradio code is throwing error", "state": "closed", "labels": [], "type": "issue"}, "hash": "3dd813254649a973789f4ee91e7cc2cbba4aba627ac71d6cd8d7f22fe2072a4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Gradio code is throwing error\n\nDescription: Gradio code provided in getting started is not working .. **Error** : return Tool.from_hub( task_or_repo_id, model_repo_id=model_repo_id, token=token, trust_remote_code=trust_remote_code, **kwargs, ) UnboundLocalError: cannot access local variable 'tool_class' where it is not associated with a value **code:** from smolagents import ( load_tool, CodeAgent, HfApiModel, GradioUI ) image_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True) model = HfApiModel(model_id) agent = CodeAgent(tools=[image_generation_tool], model=model) GradioUI(agent).launch()\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "df5579f6-d47f-48fc-b9c5-fb9ca781fda7", "embedding": null, "metadata": {"issue_id": 63, "title": "\u89c6\u9891\u6559\u7a0b/Tutorial", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b2c7a15-2257-4565-84bd-91cb798c575c", "node_type": "4", "metadata": {"issue_id": 63, "title": "\u89c6\u9891\u6559\u7a0b/Tutorial", "state": "closed", "labels": [], "type": "issue"}, "hash": "0b3ffe730aeee57ab251190c7d973c0c57069cf22c638e8395b4c5aec69f96ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: \u89c6\u9891\u6559\u7a0b/Tutorial\n\nDescription: https://youtu.be/wwN3oAugc4c\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 78, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "629245eb-8fd6-45c1-ae63-4db25e35e746", "embedding": null, "metadata": {"issue_id": 62, "title": "Installation issue over macos ( installation of smolagents via `uv`) ", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d38fed5-7e8f-45f1-977d-3b1e1b63770d", "node_type": "4", "metadata": {"issue_id": 62, "title": "Installation issue over macos ( installation of smolagents via `uv`) ", "state": "closed", "labels": [], "type": "issue"}, "hash": "b3a0d73d7936a555d77d4dbcce6bdc5461e44e77e441d7523d37aa2e30999ab2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Installation issue over macos ( installation of smolagents via `uv`) \n\nDescription: Command I ran [CODE_BLOCK] Sys info: [CODE_BLOCK] error [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "caea2fb9-2aae-4226-b008-7b57f22c6655", "embedding": null, "metadata": {"issue_id": 61, "title": "model = HfApiModel(model_id=model_id, timeout=300) - Timeout parameter seems ineffective", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a81b2b4c-b31b-4f03-8e3e-5bc90bf03bed", "node_type": "4", "metadata": {"issue_id": 61, "title": "model = HfApiModel(model_id=model_id, timeout=300) - Timeout parameter seems ineffective", "state": "closed", "labels": [], "type": "issue"}, "hash": "ec4787d4e664865db20392d2570bda513a525da465f9e5e2c65505fdcb056ddd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: model = HfApiModel(model_id=model_id, timeout=300) - Timeout parameter seems ineffective\n\nDescription: Hello, First of all, congrats for your work. model = HfApiModel(model_id=model_id, timeout=300) When creating a new model, the timeout setting seems ineffective. When running the agent, I frequently get: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5bf451e5-34f2-49b8-a4ef-50eccb595f29", "embedding": null, "metadata": {"issue_id": 60, "title": "Question : any plan for model context protocol integration ?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8ebca0b4-3530-4a71-a02a-869199cd577d", "node_type": "4", "metadata": {"issue_id": 60, "title": "Question : any plan for model context protocol integration ?", "state": "closed", "labels": [], "type": "issue"}, "hash": "d6b196a33f897460094b7c838b84a580d6a6c18a06a7af2cf9bda7b6d3b71582", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Question : any plan for model context protocol integration ?\n\nDescription: It will be very good to use it ?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f0d05006-cf39-4cb6-bdc5-b81ca27817ae", "embedding": null, "metadata": {"issue_id": 59, "title": "example for multiple tools calling on demend?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc122b48-67f0-4176-ab43-ce344df07605", "node_type": "4", "metadata": {"issue_id": 59, "title": "example for multiple tools calling on demend?", "state": "closed", "labels": [], "type": "issue"}, "hash": "3b394219e297f8c1043e2652c3ac6aaa685a577233051e4ccd2132b60fb51cba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: example for multiple tools calling on demend?\n\nDescription: For instance, we create multiple tools, but it's not necessary to use them all at the same time. Instead, an agent should use different tools at different stages. Can you provide a code example for this scenario?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60e1dc00-de72-4026-b93b-c4d8b7e5d173", "embedding": null, "metadata": {"issue_id": 58, "title": "is there a plan for persisting agent memory", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c61d4796-06ab-4417-95f9-83c5e8ff9e1f", "node_type": "4", "metadata": {"issue_id": 58, "title": "is there a plan for persisting agent memory", "state": "closed", "labels": [], "type": "issue"}, "hash": "ca28c2bb01d7cf09d6add7d2c19b2ac5dd8dc9559c5236f309532cfc30450d73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: is there a plan for persisting agent memory\n\nDescription: Currently agents have an in-memory memory to use in steps. It would be nice to have persistent agent memory for tasks, so repetitive tasks will get the final answer quickly.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6aa9c8d-67c3-45f3-a2c6-58ea6db4e186", "embedding": null, "metadata": {"issue_id": 57, "title": "Getting the current step to use the entire dataframe from the previous step", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f2d442a-60e9-4fd6-ad62-7437d168c097", "node_type": "4", "metadata": {"issue_id": 57, "title": "Getting the current step to use the entire dataframe from the previous step", "state": "closed", "labels": [], "type": "issue"}, "hash": "309109006a5108522490533d04907043d97e373a176714d2ff272e9af3099c3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Getting the current step to use the entire dataframe from the previous step\n\nDescription: First - loving this new project. Kudos to the team Question - I am pulling data from supabase and passing pandas, json and numpy as authorized imports. [CODE_BLOCK] The function returns a dataframe: !image In the second step though the code agent assumes a small subset of the dataframe and goes with it. !image How do i get it to use the entire dataframe from previous step? This must a common thing that I must just be missing something. Thanks.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0fbd4165-c203-4308-a026-20bd8a6f2816", "embedding": null, "metadata": {"issue_id": 55, "title": "Improvement to the LiteLLM support", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcbea6dd-7d1a-4d73-a40f-4d943646cb77", "node_type": "4", "metadata": {"issue_id": 55, "title": "Improvement to the LiteLLM support", "state": "closed", "labels": [], "type": "issue"}, "hash": "73cf50da89ea6af7afe29efbb539db1435dcf0eb1a94e4ba3ba13453bd49acfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Improvement to the LiteLLM support\n\nDescription: The integration of litellm is faily static and limits the usability at least for some users. https://github.com/huggingface/smolagents/blob/e57f4f55ef506948d2e17b320ddc2a98b282eacf/src/smolagents/models.pyL434 Litellm allows some more parameters for the completion invokation like temperature, top_k, and llm provider specific extra args. It would be usefull to be able to pass kwargs to both __call__ and get_tool_call, or a config using the constructor.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 526, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62b93b12-5586-4e3b-98ab-49498177b472", "embedding": null, "metadata": {"issue_id": 52, "title": "How to implement human in the loop?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e3c4aa38-7021-4daa-a9f1-9a91dd17f7c3", "node_type": "4", "metadata": {"issue_id": 52, "title": "How to implement human in the loop?", "state": "closed", "labels": [], "type": "issue"}, "hash": "cc81f77075b426c28073976575b88752d3feda938a0fb0c560a8f733e3bae4e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to implement human in the loop?\n\nDescription: How to implement human in the loop? There are two scenarios: one where more information and input from the user are required, and another where the user's consent is needed to perform a certain action.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 273, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "03b463e6-006e-495c-88a4-898a14875b80", "embedding": null, "metadata": {"issue_id": 48, "title": "Fstring Error in agents.py ", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7bca2c9c-7a65-4656-86f7-b46ff701acd2", "node_type": "4", "metadata": {"issue_id": 48, "title": "Fstring Error in agents.py ", "state": "closed", "labels": [], "type": "issue"}, "hash": "6c76ec8566cc5250fab11f7d480e4fdf3ea3b5441fba45a7108614a51648de04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Fstring Error in agents.py \n\nDescription: https://github.com/huggingface/smolagents/blob/e57f4f55ef506948d2e17b320ddc2a98b282eacf/src/smolagents/agents.pyL476 ^^^^^^^^ SyntaxError: f-string: unmatched '(' RuntimeError: Failed to import smolagents.agents because of the following error (look up to see its traceback): f-string: unmatched '(' (agents.py, line 476) The issues are: Unnecessary parentheses around the ternary expression Using double quotes inside an f-string that's already using double quotes\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 528, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed49e5a8-0e2a-4ae3-8751-15d5e4636db8", "embedding": null, "metadata": {"issue_id": 46, "title": "Feature Request: max_tokens as a parameter", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ed9276d-c863-4c3f-b2a1-80b434b25acf", "node_type": "4", "metadata": {"issue_id": 46, "title": "Feature Request: max_tokens as a parameter", "state": "closed", "labels": [], "type": "issue"}, "hash": "35dedc43ad5c9a811afe4f2cd35bda3577b8336c4cc732843fe7e720f8c78b3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Feature Request: max_tokens as a parameter\n\nDescription: Hello, Congrats for your work! It would be fantastic if we could define max_tokens. The default value 1500 is too small for me. Live long and prosper.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 229, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c33153f-ebca-4757-a903-b0a95b5ea822", "embedding": null, "metadata": {"issue_id": 45, "title": "Error occurred when using a third-party model", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9bc2575-60c0-4725-ba58-5f8d3e200a89", "node_type": "4", "metadata": {"issue_id": 45, "title": "Error occurred when using a third-party model", "state": "closed", "labels": [], "type": "issue"}, "hash": "1129174c89b605fd56ca3191dcec92a31db24f83488a6aff8933abed579bcda8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Error occurred when using a third-party model\n\nDescription: [CODE_BLOCK] \u2570\u2500 LiteLLMModel - deepseek-chat \u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 0 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Provider List: https://docs.litellm.ai/docs/providers Error in generating tool call with model: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=deepseek-chat Pass model as E.g. For 'Huggingface' inference endpoints pass in completion(model='huggingface/star coder',..) Learn more: https://docs.litellm.ai/docs/provid ers\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4a53402-3167-4902-9623-acc0f52f3896", "embedding": null, "metadata": {"issue_id": 44, "title": "LLM using wrong function to send a request to an agent", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b90a3a3f-896c-4fbb-9617-64eee5de9e8d", "node_type": "4", "metadata": {"issue_id": 44, "title": "LLM using wrong function to send a request to an agent", "state": "closed", "labels": [], "type": "issue"}, "hash": "4b97df77f79d5225aa25dac02e930e1d7d3fd536d41c41d9b8df471cedf099ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: LLM using wrong function to send a request to an agent\n\nDescription: Notice in Step 0, it tried to call home_automation.request, gets an error, then calls the correct function home_automation() [CODE_BLOCK] Here's my code: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73bc6e37-2294-4bea-b9cb-70e09c16f083", "embedding": null, "metadata": {"issue_id": 42, "title": "Ollama usage", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c11c63a6-adbe-4f74-bf83-2e06aebdc5d4", "node_type": "4", "metadata": {"issue_id": 42, "title": "Ollama usage", "state": "closed", "labels": [], "type": "issue"}, "hash": "7b98356790f1d2fd223e058d7f930e80fb1fcd55642659cb4a51eea3aefe48ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Ollama usage\n\nDescription: How can I use it with the Ollama local inference server?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3601b65e-3a4e-40e5-b268-222a15793c58", "embedding": null, "metadata": {"issue_id": 39, "title": "can we implement rate limiting?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3e79daf-0d01-4a47-bc0c-f2181f5ba479", "node_type": "4", "metadata": {"issue_id": 39, "title": "can we implement rate limiting?", "state": "closed", "labels": [], "type": "issue"}, "hash": "3ac5b3ac77361a38fe9230128fbdedf150c82ea4467bbdfb4285b7a5daf9e7f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: can we implement rate limiting?\n\nDescription: Hi, I have a problem using gemini model via litellm. I am getting rate limited very frequently. What about to add some waiting time between calls so it does not happen?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "717946c8-512f-4185-b41e-119d78e5ac2f", "embedding": null, "metadata": {"issue_id": 38, "title": "i have an issue to use with svelte 5 Framework", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10413dc3-d003-45d1-9a92-16501be66cd5", "node_type": "4", "metadata": {"issue_id": 38, "title": "i have an issue to use with svelte 5 Framework", "state": "closed", "labels": [], "type": "issue"}, "hash": "0c22c83da272a7cc05327da162af5071368451b87439bebb59330dfeacb4405f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: i have an issue to use with svelte 5 Framework\n\nDescription: Hey all videos on the internet using Python, any showcase with svelte 5 Framework ? will be helpful. in VSCode how i can run it to suggest code and run the code in action ?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10f76fdd-890f-4385-9afc-137526f44590", "embedding": null, "metadata": {"issue_id": 36, "title": "Example tool_calling_agent_ollama.py causes error", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "998ad849-574d-4112-a933-552e4aec2377", "node_type": "4", "metadata": {"issue_id": 36, "title": "Example tool_calling_agent_ollama.py causes error", "state": "closed", "labels": [], "type": "issue"}, "hash": "e49957c400c85c9c11a723a86dc7a5aaa2f4462c3889ef9808129cff6a9cc1d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Example tool_calling_agent_ollama.py causes error\n\nDescription: I tweaked example code using my **external Ollama server URL (on LightningAI)** and **llama3.2:latest**, using this code: [CODE_BLOCK] but it caused error. **The error is:** [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "12db9613-c8cd-4634-b90d-907e07f13963", "embedding": null, "metadata": {"issue_id": 35, "title": "UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7ac25ed-e5a2-4139-97d7-a3c34dc06b0f", "node_type": "4", "metadata": {"issue_id": 35, "title": "UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)", "state": "closed", "labels": [], "type": "issue"}, "hash": "d663e3aede471b937c6c794208206dcca8ac83427ca0bb48b6a3990caf1d0ecf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "890c30eb-7e5e-46b0-9f07-dcba50d021cf", "node_type": "1", "metadata": {}, "hash": "8c242be4b814d2266a5223f6828c6dbfe1ba97d4d2d672820ac1a4b5630f362d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)\n\nDescription: [CODE_BLOCK] (.venv) (base) \u279c test-smolagents git:(master) \u2717 python app-ollama.py /Users/charlesqin/PycharmProjects/test-smolagents/.venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2: * 'fields' has been removed warnings.warn(message, UserWarning) \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 New run \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 What's the weather like in Paris? \u2502 \u2502 \u2502 \u2570\u2500 LiteLLMModel - ollama_chat/llama3.2 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 0 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404 [Step 0: Duration 0.05 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404 [Step 1: Duration 0.02 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2751, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "890c30eb-7e5e-46b0-9f07-dcba50d021cf", "embedding": null, "metadata": {"issue_id": 35, "title": "UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7ac25ed-e5a2-4139-97d7-a3c34dc06b0f", "node_type": "4", "metadata": {"issue_id": 35, "title": "UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)", "state": "closed", "labels": [], "type": "issue"}, "hash": "d663e3aede471b937c6c794208206dcca8ac83427ca0bb48b6a3990caf1d0ecf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12db9613-c8cd-4634-b90d-907e07f13963", "node_type": "1", "metadata": {"issue_id": 35, "title": "UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)", "state": "closed", "labels": [], "type": "issue"}, "hash": "9afbb1a5047add95b4ecbceba2316e65211f72c38dc7f94188a9d68c9b2ff3c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84f8816b-6ea1-4aa0-bb95-850df84b7f35", "node_type": "1", "metadata": {}, "hash": "9f0f20500a15cbb72e989d2a4b158446cdba219e4a799d7f605166000b8e20c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404 [Step 2: Duration 0.02 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 3 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404 [Step 3: Duration 0.02 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 4 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404 [Step 4: Duration 0.02 seconds] \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 5 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use litellm.set_verbose=True'. Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404 [Step 5: Duration 0.02 seconds] Reached max iterations. Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.", "mimetype": "text/plain", "start_char_idx": 2752, "end_char_idx": 5263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84f8816b-6ea1-4aa0-bb95-850df84b7f35", "embedding": null, "metadata": {"issue_id": 35, "title": "UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7ac25ed-e5a2-4139-97d7-a3c34dc06b0f", "node_type": "4", "metadata": {"issue_id": 35, "title": "UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)", "state": "closed", "labels": [], "type": "issue"}, "hash": "d663e3aede471b937c6c794208206dcca8ac83427ca0bb48b6a3990caf1d0ecf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "890c30eb-7e5e-46b0-9f07-dcba50d021cf", "node_type": "1", "metadata": {"issue_id": 35, "title": "UserWarning: Valid config keys have changed in V2: * 'fields' has been removed   warnings.warn(message, UserWarning)", "state": "closed", "labels": [], "type": "issue"}, "hash": "f7e70f3b9ebd8d046320948065f6aed2dc67cb7b0cae9072141a49828339e031", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Error in generating tool call with model: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404 [Step 5: Duration 0.02 seconds] Reached max iterations. Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'. Final answer: Error in generating final LLM output: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404 [Step 6: Duration 0.00 seconds] Error in generating final LLM output: litellm.APIConnectionError: Ollama_chatException - Client error '404 Not Found' for url 'http://localhost:11434/v1/api/chat' For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 4801, "end_char_idx": 5828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dcccdc8b-2b6b-4836-a3e6-9818002622ef", "embedding": null, "metadata": {"issue_id": 34, "title": "have a warnings", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56897068-8003-4a9a-9d52-f69bfc09a3b1", "node_type": "4", "metadata": {"issue_id": 34, "title": "have a warnings", "state": "closed", "labels": [], "type": "issue"}, "hash": "25e3726a963ab2273f85f2846ea97c8408fbfafd831b2a94da3f8e34c0ac3e7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: have a warnings\n\nDescription: [CODE_BLOCK] when use LiteLLMModel [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 99, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3691d0d0-41cd-41b8-aade-f88cb08bbc43", "embedding": null, "metadata": {"issue_id": 32, "title": "CodeAgent pandas import", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5bca16e-a63a-433f-b89d-b833a97ec677", "node_type": "4", "metadata": {"issue_id": 32, "title": "CodeAgent pandas import", "state": "closed", "labels": [], "type": "issue"}, "hash": "8c3165a6564e227c80beab2ba515c65e38b49f802a91d4d30c9de257285fa13e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: CodeAgent pandas import\n\nDescription: Say my function that I'm using as a tool is returning a pandas dataframe, now the CodeAgent tried to build a code to fetch a particular data I requested from it and when it tried to import pandas in the code it generates there was an, Error: Code execution failed: Code execution failed at line 'import pandas as pd' because of the following error: Import of pandas is not allowed. Authorized imports are: ['random', 'queue', 'datetime', 'statistics', 'unicodedata', 'collections', 'math', 'itertools', 'stat', 're', 'time'] Now the data in that dataframe is too big it's around 3000 rows and 15 to 16 columns, it tried a few different things it didn't work, I tried returning my data as a json data from my tool but then again it's such a huge data, it couldn't run it properly so like what would be the way to tackle this problem?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "602e8124-8629-480d-b4e5-7d2f72bd4cdf", "embedding": null, "metadata": {"issue_id": 31, "title": "LangChain Interoperability?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c80eeb9-059f-44bb-96c1-182c1ff9e703", "node_type": "4", "metadata": {"issue_id": 31, "title": "LangChain Interoperability?", "state": "closed", "labels": [], "type": "issue"}, "hash": "3ef13473ec46f56a4a7bb93de5869cbe5aecbc91799ed7fcddaa42b7a025fad8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: LangChain Interoperability?\n\nDescription: I was wondering if smolagents are interoperable with LangChain chains? If not, can we please make that as a feature?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4634c70f-500f-4ab2-b7d1-3fa4ad5690d5", "embedding": null, "metadata": {"issue_id": 30, "title": "Expose the agents using an ChatCompletions standard API interface", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2042761-cb5d-4481-947b-ccc5b3e160a3", "node_type": "4", "metadata": {"issue_id": 30, "title": "Expose the agents using an ChatCompletions standard API interface", "state": "closed", "labels": [], "type": "issue"}, "hash": "32d4ba162cdd300dabe5ff7dc25780cdf27de4eed8d7a8f22f204a16e5d5a1e1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Expose the agents using an ChatCompletions standard API interface\n\nDescription: \n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "66af3140-f9e3-448f-b040-f10749a75317", "embedding": null, "metadata": {"issue_id": 28, "title": "Trying to build a NLP -> API Call Agent", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0149ee81-2f4b-48d5-a829-eeb00cd22475", "node_type": "4", "metadata": {"issue_id": 28, "title": "Trying to build a NLP -> API Call Agent", "state": "closed", "labels": [], "type": "issue"}, "hash": "7374f0175a5fc326dca7eabbf89efa9cef89e4fec68d8efdc5af635edda6f087", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Trying to build a NLP -> API Call Agent\n\nDescription: Hi Guys, I'm trying to make a little agent to make API call to the Netbox API. <img width=\"1118\" alt=\"Screenshot 2025-01-01 at 4 57 18 PM\" src=\"https://github.com/user-attachments/assets/5bf17b87-217b-4534-9af2-752cf50efee5\" /> <img width=\"1098\" alt=\"Screenshot 2025-01-01 at 4 57 05 PM\" src=\"https://github.com/user-attachments/assets/1c9704fe-d4fa-41f7-b4cd-9db75515ce13\" /> and even though calling the function directly works fine. The Agent continues iterating What I'm doing wrong? this is the code: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "180b3fd8-1d6d-4a87-ba77-6c3de4691c24", "embedding": null, "metadata": {"issue_id": 27, "title": "Loading Models from disk", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57ac5687-a1c7-47aa-aa3d-efcd758f62e3", "node_type": "4", "metadata": {"issue_id": 27, "title": "Loading Models from disk", "state": "closed", "labels": [], "type": "issue"}, "hash": "81b5a6535c7b5d9e3e56837ffb478f7891baf71b332f4313c8d6427462662359", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Loading Models from disk\n\nDescription: Is there any way to load the models without using the Hugginface model id? I have several Llama models on my drive and would like to use those.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 204, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57234315-273e-4aab-9b64-3bce5646e4c5", "embedding": null, "metadata": {"issue_id": 25, "title": "CodeAgent relies on e2b Code Interpreter, no self-hosting support", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19dc5acb-8914-4dba-a8ce-270a690aafaa", "node_type": "4", "metadata": {"issue_id": 25, "title": "CodeAgent relies on e2b Code Interpreter, no self-hosting support", "state": "closed", "labels": [], "type": "issue"}, "hash": "b7f6963faea10afd4e622dbdd697212aacda945d5dfe3dadfee8a833c1873f0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: CodeAgent relies on e2b Code Interpreter, no self-hosting support\n\nDescription: **Description** Right now, CodeAgent in smolagents depends on e2b\u2019s Code Interpreter, which doesn\u2019t support self-hosting. This means smolagents has to rely on e2b\u2019s online service, which can be a problem for use cases that need privacy or offline capabilities. **Issues** 1. It doesn\u2019t work in offline environments, which limits independence. 2. Relying on an online service raises privacy and security concerns, especially for sensitive data. 3. Long-term dependence on an external service could be risky if the service goes down or gets deprecated. **Questions** 1. Are there plans to move to a self-hosted or custom solution in the future? 2. Or will smolagents stick with e2b\u2019s Code Interpreter?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ef4b225-59da-4463-b169-12bc58add7bb", "embedding": null, "metadata": {"issue_id": 24, "title": "Conversational agent", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37845304-76a2-4189-946e-d499ecf5dac1", "node_type": "4", "metadata": {"issue_id": 24, "title": "Conversational agent", "state": "closed", "labels": [], "type": "issue"}, "hash": "f49e614abdd55182cc9b3d7a0528d9caec94472b880e98c02e0710ada9b4e241", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Conversational agent\n\nDescription: Can I create make a conversational agent using smol agents? For example I created a loop so that the response generation and user input can be continued and it's conversational, while True: user_input = input(\"You: \") if user_input.lower() == \"exit\": print(\"Goodbye!\") break try: response = agent.run(user_input) print(f\"Assistant: {response}\") except Exception as e: print(f\"An error occurred: {e}\") But what happened was my initial prompt got an answer which was almost accurate but as soon as I said in my next prompt \"The answer you generated was almost correct refer to the tool again and try to get me the correct answer\" It completely hallucinated it went completely sideways\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1dd9eaf-472e-4fc9-a796-b147f0ce950c", "embedding": null, "metadata": {"issue_id": 23, "title": "Tool calling error", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1cfaf48e-ab9f-44e6-aa5d-234e0095c51d", "node_type": "4", "metadata": {"issue_id": 23, "title": "Tool calling error", "state": "closed", "labels": [], "type": "issue"}, "hash": "98975aae5175a14c3d9a072551ddaa11a903cbdb59e22883e921677dc560cef3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Tool calling error\n\nDescription: I decorated a python function properly with @tool, no arguments are to be passed into the function, and in the docstring section of the function I even added a line saying Args: No arguments This is my function: def fetch_database_schema() There are no arguments required while calling it, but still for some reason the agent keeps calling it with different functions like this: Calling tool: 'fetch_database_schema' with arguments: {'content': 'fetch_database_schema'} Calling tool: 'fetch_database_schema' with arguments: {'content': '{}'} Calling tool: 'fetch_database_schema' with arguments: {'answer': ''} Like this and when it will do that obviously an error will occur saying got an unexpected keyword, why is that happening?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cdf17d88-9f06-4880-b416-0244164d8274", "embedding": null, "metadata": {"issue_id": 21, "title": "How can we set num_ctx when calling Ollama?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84d352d1-47df-4fa4-87db-3e88911bb9d3", "node_type": "4", "metadata": {"issue_id": 21, "title": "How can we set num_ctx when calling Ollama?", "state": "closed", "labels": [], "type": "issue"}, "hash": "f8fceccfc481e92e65db288c1f95018a1ead20cd27bb08acd176e0548b707a9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How can we set num_ctx when calling Ollama?\n\nDescription: Ollama by default limits all calls to 2048 tokens - normally we can add an options dicts to the call like {'options': {'num_ctx':16384}} to override this. How can we do this using litellm? I checked their docs and there doesn't seem to be any way to do this.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7700d234-42f9-4157-96b7-a107b87cd548", "embedding": null, "metadata": {"issue_id": 20, "title": "LiteLLMModel cannot set api_base", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1640bcc-f158-44bb-a513-7e092c8c957c", "node_type": "4", "metadata": {"issue_id": 20, "title": "LiteLLMModel cannot set api_base", "state": "closed", "labels": [], "type": "issue"}, "hash": "88b890133ab54f94dd00bb3731762eff41137f2a940bc38dcb22f30d408272d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: LiteLLMModel cannot set api_base\n\nDescription: [CODE_BLOCK] I got TypeError: LiteLLMModel.__init__() got an unexpected keyword argument 'api_base' The doc on LiteLLMModel suggests api_base and api_key are both valid arguments.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "33e4b433-955d-41db-bd66-620805bf84ab", "embedding": null, "metadata": {"issue_id": 19, "title": "Can't use local model using Ollama in the tool_calling_agent_ollama.py example", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6748251-4cce-4e7b-8da2-a5f4f4230001", "node_type": "4", "metadata": {"issue_id": 19, "title": "Can't use local model using Ollama in the tool_calling_agent_ollama.py example", "state": "closed", "labels": [], "type": "issue"}, "hash": "6b6fb0502295594b402f0e4780a48ffbb2034cb1e1b18ca0a12727c675631502", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Can't use local model using Ollama in the tool_calling_agent_ollama.py example\n\nDescription: Hello, this is my first meaningful (i hope) OSS contribution. In tool_calling_agent_ollama.py, we get the following error when trying to run: [CODE_BLOCK] Here is the relevant code: [CODE_BLOCK] To fix it, we need to change \"openai/llama3.2\" to \"**ollama**/llama3.2\" PR here: https://github.com/huggingface/smolagents/pull/18\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c52fa04-d801-4d1f-af87-ed5ceb97052f", "embedding": null, "metadata": {"issue_id": 17, "title": "NOT AN ISSUE", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e73c4b52-98ca-4887-9390-f580987dc89a", "node_type": "4", "metadata": {"issue_id": 17, "title": "NOT AN ISSUE", "state": "closed", "labels": [], "type": "issue"}, "hash": "aeaf70ad0f7e9f0747d46a6f8d30b23caecd6d608fa07085c7c58bf20122e1e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: NOT AN ISSUE\n\nDescription: This is amazing! just yesterday i had the concept in mind and made a pow to to get python code in response of prompts and run in eval for some of my devsecops tasks and today google suggest me this article referencing smolagents love to read code asap and contribute!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ecd62ea2-53fd-4873-bd5a-e709b5977e98", "embedding": null, "metadata": {"issue_id": 15, "title": "Python function as a tool", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "770e18d1-6f19-4ee3-8305-0318d2292d89", "node_type": "4", "metadata": {"issue_id": 15, "title": "Python function as a tool", "state": "closed", "labels": [], "type": "issue"}, "hash": "cb9ff49541c4d23cfebb211bb936e684d58f5fb85165a45a68e072f3e4a9ac3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Python function as a tool\n\nDescription: If I create a python function to do a particular task, can I use that python function as a tool? If so how? I created a python function that creates a dataframe of information that I want to pass it in as a tool, I tried that but I'm getting a attribute error AttributeError Traceback (most recent call last) <ipython-input-20-349bcbef1fc9> in <cell line: 62>() 60 61 Create and run agent ---> 62 agent = CodeAgent(tools=[fetch_database_schema_tool], model=HfApiModel()) /usr/local/lib/python3.10/dist-packages/smolagents/tools.py in <dictcomp>(.0) 955 956 def __init__(self, tools: List[Tool], add_base_tools: bool = False): --> 957 self._tools = {tool.name: tool for tool in tools} 958 if add_base_tools: 959 self.add_base_tools() Is there a list of tools or some fixed number of tools I can use or is there a way I can do this?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42453393-d0e8-4f4e-b938-1d7da3d94780", "embedding": null, "metadata": {"issue_id": 14, "title": "I just added an [example of multi-agent orchestration](https://github.com/huggingface/smolagents/blob/main/docs/source/examples/multiagents.md) @MonolithFoundation @whisper-bye!", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "621763ee-a948-45ef-9ad0-88d27baaf5f0", "node_type": "4", "metadata": {"issue_id": 14, "title": "I just added an [example of multi-agent orchestration](https://github.com/huggingface/smolagents/blob/main/docs/source/examples/multiagents.md) @MonolithFoundation @whisper-bye!", "state": "closed", "labels": [], "type": "issue"}, "hash": "2db891a2a1abf34d3615edb9f1b27fcbb8ba77e36116da5077670c97beabca71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: I just added an [example of multi-agent orchestration](https://github.com/huggingface/smolagents/blob/main/docs/source/examples/multiagents.md) @MonolithFoundation @whisper-bye!\n\nDescription: I just added an example of multi-agent orchestration @MonolithFoundation @whisper-bye! Tell me what you think! _Originally posted by @aymeric-roucher in https://github.com/huggingface/smolagents/issues/3issuecomment-2566327823_ I could not find it, it gives a 404 error\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6c7dd92d-b659-4512-b727-484efc3a4dc2", "embedding": null, "metadata": {"issue_id": 13, "title": "Example demonstrating multi-agent system using smolagents", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e38bcdd-2c0c-4871-a634-ffdab3e57c91", "node_type": "4", "metadata": {"issue_id": 13, "title": "Example demonstrating multi-agent system using smolagents", "state": "closed", "labels": [], "type": "issue"}, "hash": "2018e97f32a6dd548550df68909d0c35996801a46c70e9db7fe889c135196c46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Example demonstrating multi-agent system using smolagents\n\nDescription: Hi again @aymeric-roucher , I just wrote an example notebook about how to let multiple smolagents write a manuscript together and I'm curious about two things: - Would this example notebook be in scope for your repository as example? I could clean it a bit up (depending in your feedback) and send a PR if this makes sense. - Is there room for improvement? I'm in particular curious if the implementation of the scheduler agent makes sense. Feedback welcome! Best, Robert\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "118aa815-835e-45a0-9816-d2fd0639fb7c", "embedding": null, "metadata": {"issue_id": 11, "title": "Some Questions and Suggestions", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91c97655-53d0-4d07-a1a3-77cb2a2ede4e", "node_type": "4", "metadata": {"issue_id": 11, "title": "Some Questions and Suggestions", "state": "closed", "labels": [], "type": "issue"}, "hash": "f91b7d62022505a3802e06c67af19654dcb5226c373433b92de33a668e2bbf9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Some Questions and Suggestions\n\nDescription: I think this project has great potential, and I commend you on your work. Below are some questions I have, and while my suggestions may add complexity, they are indeed issues that an agent framework needs to address. 1\u3001I have a question regarding how the planner is embodied and what distinguishes it from the ManagedAgent. 2\u3001Impressive examples: It is recommended to include the capability to load and analyze data, and to implement a data analysis task where the results can be presented graphically, such as generating bar charts and other visual representations. 3\u3001Are there plans to introduce a simple memory mechanism? 4\u3001Are there plans to add orchestration features for multiple agents? 5\u3001Are there plans to introduce more types of agents, such as a ReactCodeAgent?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8fe43b19-ca95-4895-960f-01704f10eacd", "embedding": null, "metadata": {"issue_id": 6, "title": "broken link ", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "079cbe12-d6d7-41b0-8767-3840db61d3da", "node_type": "4", "metadata": {"issue_id": 6, "title": "broken link ", "state": "closed", "labels": [], "type": "issue"}, "hash": "7084a4533cd82ec3996d2b538da824ce65389a69b435686e2bea836b990a8568", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: broken link \n\nDescription: readme broken link here: Head to our high-level intro to agents to learn more on that.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 135, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b85604b8-cd1c-468f-917c-4efb190ea0c0", "embedding": null, "metadata": {"issue_id": 5, "title": "Questions Regarding the Similarities and Differences Between smolagents and transformers Agent Frameworks", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4c196a1-7bc2-4911-b9f4-112eee123cc7", "node_type": "4", "metadata": {"issue_id": 5, "title": "Questions Regarding the Similarities and Differences Between smolagents and transformers Agent Frameworks", "state": "closed", "labels": [], "type": "issue"}, "hash": "229f0fb405b3b0aa83ed9afa362fe8b4091108303ec169fa8b0d7352b50e3765", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Questions Regarding the Similarities and Differences Between smolagents and transformers Agent Frameworks\n\nDescription: Hello, I noticed that both huggingface/smolagents and huggingface/transformers offer agent frameworks, and the technical principles behind both seem quite similar. Given that transformers already includes agent-related functionalities, I would like to ask for some clarification on the following points: Overlap in Functionality: What are the key differences in terms of functionality between the agent frameworks in smolagents and transformers? Is there a specific use case or scenario where one is preferred over the other? Technological Differences: Are there significant differences in the underlying architecture or technology between the two frameworks? For example, are they based on the same agent design principles, or is there a fundamental difference in how they operate? Future Development: Are both frameworks going to be maintained separately, or is there any plan for merging or aligning the features from these two projects moving forward? Understanding the relationship between these two frameworks would help clarify which one is more suitable for different types of agent-based applications. Thanks in advance!\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1271, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ec46a22-0728-4284-891b-3ea79e96f616", "embedding": null, "metadata": {"issue_id": 4, "title": "[bug] 0.1.2 f-string issue", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "74ac7806-e0bc-4378-b00f-bdcf9602b9de", "node_type": "4", "metadata": {"issue_id": 4, "title": "[bug] 0.1.2 f-string issue", "state": "closed", "labels": [], "type": "issue"}, "hash": "d9f48f3fe9e1d8210d9ed231c86e8a895c241590a759238fdd20617c79f98981", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: [bug] 0.1.2 f-string issue\n\nDescription: - version 0.1.2 will crash on a simple import as from smolagents import CodeAgent. - The error log is: [CODE_BLOCK] - Seems like like 476 has a double quote in the f-string: [CODE_BLOCK] Should be replaced as: [CODE_BLOCK]\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49787323-d083-40ec-a6ed-f9feec8768cf", "embedding": null, "metadata": {"issue_id": 3, "title": "Provide more agents examples", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bbec38d7-0413-443d-917a-2c5c418374b2", "node_type": "4", "metadata": {"issue_id": 3, "title": "Provide more agents examples", "state": "closed", "labels": [], "type": "issue"}, "hash": "c087a0d331eaad05182e0385d800c051cb7fc8579039106a104f31ece7da0145", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: Provide more agents examples\n\nDescription: Currently, everyone is talking about agents. However, most only offer examples like \"search today's temperature\". While this is not entirely useless, it is not overly useful either. Some more general and advanced agent examples would be more convincing.\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5ba5912f-3824-4f7c-b431-5c33752930ec", "embedding": null, "metadata": {"issue_id": 2, "title": "How to call OpenAI-like models through an API?", "state": "closed", "labels": [], "type": "issue"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "720ea5db-597c-4ee8-aa87-ebfd53144361", "node_type": "4", "metadata": {"issue_id": 2, "title": "How to call OpenAI-like models through an API?", "state": "closed", "labels": [], "type": "issue"}, "hash": "d8eae603e6c21977468fc8542fa820a3ecf68c7f0a8600f9dc7a46baab76972c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Title: How to call OpenAI-like models through an API?\n\nDescription: How to call OpenAI-like models through an API?\n\nState: closed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "26a936c9-9270-49ef-939b-19b370a932a8", "embedding": null, "metadata": {"issue_id": 1190, "pr_number": 1191, "type": "patch", "files": ["src/smolagents/local_python_executor.py", "tests/test_local_python_executor.py"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4841b9e3-5383-4f98-842a-5780cb199782", "node_type": "4", "metadata": {"issue_id": 1190, "pr_number": 1191, "type": "patch", "files": ["src/smolagents/local_python_executor.py", "tests/test_local_python_executor.py"]}, "hash": "027f306f21a3522b81ab9d2862cfbe25fb6601bf7ec92510cfc100ea7837cbd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d78be0e2-30ce-4967-8e66-edf7f7240d8e", "node_type": "1", "metadata": {}, "hash": "932320e82ea13ab90bbe6ddc40ee29538d9d6f63f0d9507ed70cf28f58485b99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #1190 (PR #1191):\nPatch summary for PR (max 8000 chars):\nFiles changed: local_python_executor.py, test_local_python_executor.py\n--- Changes ---\n\n--- src/smolagents/local_python_executor.py ---\n@@ -585,17 +585,18 @@ def evaluate_boolop(\n     static_tools: Dict[str, Callable],\n     custom_tools: Dict[str, Callable],\n     authorized_imports: List[str],\n-) -> bool:\n-    if isinstance(node.op, ast.And):\n-        for value in node.values:\n-            if not evaluate_ast(value, state, static_tools, custom_tools, authorized_imports):\n-                return False\n-        return True\n-    elif isinstance(node.op, ast.Or):\n-        for value in node.values:\n-            if evaluate_ast(value, state, static_tools, custom_tools, authorized_imports):\n-                return True\n-        return False\n+) -> Any:\n+    # Determine which value should trigger short-circuit based on operation type:\n+    # - 'and' returns the first falsy value encountered (or the last value if all are truthy)\n+    # - 'or' returns the first truthy value encountered (or the last value if all are falsy)\n+    is_short_circuit_value = (lambda x: not x) if isinstance(node.op, ast.And) else (lambda x: bool(x))\n+    for value in node.values:\n+        result = evaluate_ast(value, state, static_tools, custom_tools, authorized_imports)\n+        # Short-circuit: return immediately if the condition is met\n+        if is_short_circuit_value(result):\n+            return result\n+    # If no short-circuit occurred, return the last evaluated value\n+    return result\n def evaluate_binop(\n\n--- tests/test_local_python_executor.py ---\n@@ -32,6 +32,7 @@\n     LocalPythonExecutor,\n     PrintContainer,\n     check_import_authorized,\n+    evaluate_boolop,\n     evaluate_condition,\n     evaluate_delete,\n     evaluate_python_code,\n\n--- tests/test_local_python_executor.py ---\n@@ -478,6 +479,22 @@ def test_boolops(self):\n         result, _ = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5})\n         assert result == \"Sacramento\"\n+        # Short-circuit evaluation:\n+        # (T and 0) or (T and T) => 0 or True => True\n+        code = \"result = (x > 3 and y) or (z == 10 and not y)\\nresult\"\n+        result, _ = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={\"x\": 5, \"y\": 0, \"z\": 10})\n+        assert result\n+\n+        # (None or \"\") or \"Found\" => \"\" or \"Found\" => \"Found\"\n+        code = \"result = (a or c) or b\\nresult\"\n+        result, _ = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={\"a\": None, \"b\": \"Found\", \"c\": \"\"})\n+        assert result == \"Found\"\n+\n+        # (\"First\" and \"\") or \"Third\" => \"\" or \"Third\" -> \"Third\"\n+        code = \"result = (a and b) or c\\nresult\"\n+        result, _ = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={\"a\": \"First\", \"b\": \"\", \"c\": \"Third\"})\n+        assert result == \"Third\"\n+\n     def test_if_conditions(self):\n         code = \"\"\"char='a'\n if char.isalpha():\n\n--- tests/test_local_python_executor.py ---\n@@ -1288,6 +1305,26 @@ def test_evaluate_python_code_with_evaluate_delete(code, expected_error_message)\n     assert expected_error_message in str(exception_info.value)\n+@pytest.mark.parametrize(\"a\", [1, 0])\n+@pytest.mark.parametrize(\"b\", [2, 0])\n+@pytest.mark.parametrize(\"c\", [3, 0])\n+def test_evaluate_boolop_and(a, b, c):\n+    boolop_ast = ast.parse(\"a and b and c\").body[0].value\n+    state = {\"a\": a, \"b\": b, \"c\": c}\n+    result = evaluate_boolop(boolop_ast, state, {}, {},", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3475, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d78be0e2-30ce-4967-8e66-edf7f7240d8e", "embedding": null, "metadata": {"issue_id": 1190, "pr_number": 1191, "type": "patch", "files": ["src/smolagents/local_python_executor.py", "tests/test_local_python_executor.py"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4841b9e3-5383-4f98-842a-5780cb199782", "node_type": "4", "metadata": {"issue_id": 1190, "pr_number": 1191, "type": "patch", "files": ["src/smolagents/local_python_executor.py", "tests/test_local_python_executor.py"]}, "hash": "027f306f21a3522b81ab9d2862cfbe25fb6601bf7ec92510cfc100ea7837cbd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26a936c9-9270-49ef-939b-19b370a932a8", "node_type": "1", "metadata": {"issue_id": 1190, "pr_number": 1191, "type": "patch", "files": ["src/smolagents/local_python_executor.py", "tests/test_local_python_executor.py"]}, "hash": "f70094205bd745351a0ea5b4495dc04d15f383b04eba955a0f21baf6aabbff38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "isalpha():\n\n--- tests/test_local_python_executor.py ---\n@@ -1288,6 +1305,26 @@ def test_evaluate_python_code_with_evaluate_delete(code, expected_error_message)\n     assert expected_error_message in str(exception_info.value)\n+@pytest.mark.parametrize(\"a\", [1, 0])\n+@pytest.mark.parametrize(\"b\", [2, 0])\n+@pytest.mark.parametrize(\"c\", [3, 0])\n+def test_evaluate_boolop_and(a, b, c):\n+    boolop_ast = ast.parse(\"a and b and c\").body[0].value\n+    state = {\"a\": a, \"b\": b, \"c\": c}\n+    result = evaluate_boolop(boolop_ast, state, {}, {}, [])\n+    assert result == (a and b and c)\n+\n+\n+@pytest.mark.parametrize(\"a\", [1, 0])\n+@pytest.mark.parametrize(\"b\", [2, 0])\n+@pytest.mark.parametrize(\"c\", [3, 0])\n+def test_evaluate_boolop_or(a, b, c):\n+    boolop_ast = ast.parse(\"a or b or c\").body[0].value\n+    state = {\"a\": a, \"b\": b, \"c\": c}\n+    result = evaluate_boolop(boolop_ast, state, {}, {}, [])\n+    assert result == (a or b or c)\n+\n+\n @pytest.mark.parametrize(\n     \"code, state, expectation\",\n     [", "mimetype": "text/plain", "start_char_idx": 2941, "end_char_idx": 3940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "610828e0-38af-4639-a4a0-22ac1b3f02dd", "embedding": null, "metadata": {"issue_id": 919, "pr_number": 983, "type": "patch", "files": ["docs/source/en/tutorials/secure_code_execution.mdx"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8671b7dc-a77c-4861-977f-0c096f141cd4", "node_type": "4", "metadata": {"issue_id": 919, "pr_number": 983, "type": "patch", "files": ["docs/source/en/tutorials/secure_code_execution.mdx"]}, "hash": "75ef8d640b9990c1603b2d3306b05219e8ea77b38e10ae70a6611108b8991b8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef0e0977-ad3d-4683-b293-6a33efd4b3ae", "node_type": "1", "metadata": {}, "hash": "8f00a29ad1a7e6960ab30e6e5871b0ec65c93ce2feac4daecadc39c2d48c89e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #919 (PR #983):\nPatch summary for PR (max 8000 chars):\nFiles changed: secure_code_execution.mdx\n--- Changes ---\n\n--- docs/source/en/tutorials/secure_code_execution.mdx ---\n@@ -115,43 +115,28 @@ run_capture_exception(harmful_command)\n These safeguards make out interpreter is safer.\n We have used it on a diversity of use cases, without ever observing any damage to the environment.\n-However, this solution is certainly not watertight, as no local python sandbox can really be: one could imagine occasions where LLMs fine-tuned for malignant actions could still hurt your environment.\n-\n-\n-For instance, if you have allowed an innocuous package like `Pillow` to process images, the LLM could generate thousands of image saves to bloat your hard drive.\n-\n-```py\n-custom_executor = LocalPythonExecutor([\"PIL\"])\n-\n-harmful_command=\"\"\"\n-from PIL import Image\n-\n-img = Image.new('RGB', (100, 100), color='blue')\n-\n-i=0\n-while i < 10000:\n-    img.save('simple_image_{i}.png')\n-    i += 1\n-\"\"\"\n-# Let's not execute this but it would not error out, and it would bloat your system with images.\n-```\n+> [!WARNING]\n+> It's important to understand that no local python sandbox can ever be completely secure. While our interpreter provides significant safety improvements over the standard Python interpreter, it is still possible for a determined attacker or a fine-tuned malicious LLM to find vulnerabilities and potentially harm your environment. \n+> \n+> For example, if you've allowed packages like `Pillow` to process images, the LLM could generate code that creates thousands of large image files to fill your hard drive. Other advanced escape techniques might exploit deeper vulnerabilities in authorized packages.\n+> \n+> Running LLM-generated code in your local environment always carries some inherent risk. The only way to run LLM-generated code with truly robust security isolation is to use remote execution options like E2B or Docker, as detailed below.\n-Other examples of attacks can be found [here](https://gynvael.coldwind.pl/n/python_sandbox_escape).\n+The risk of a malicious attack is low when using well-known LLMs from trusted inference providers, but it is not zero.\n+For high-security applications or when using less trusted models, you should consider using a remote execution sandbox.\n-Running these targeted malicious code snippet require a supply chain attack, meaning the LLM you use has been intoxicated.\n+## Sandbox approaches for secure code execution\n-The likelihood of this happening is low when using well-known LLMs from trusted inference providers, but it is still non-zero.\n+When working with AI agents that execute code, security is paramount. There are two main approaches to sandboxing code execution in smolagents, each with different security properties and capabilities:\n-> [!WARNING]\n-> The only way to run LLM-generated code securely is to isolate the execution from your local environment.\n-So if you want to exercise caution, you should use a remote execution sandbox.\n+![Sandbox approaches comparison](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/remote_execution.png)\n-Here are examples of how to do it.\n+1. **Running individual code snippets in a sandbox**: This approach (left side of diagram) only executes the agent-generated Python code snippets in a sandbox while keeping the rest of the agentic system in your local environment. It's simpler to set up using `executor_type=\"e2b\"` or `executor_type=\"docker\"`, but it doesn't support multi-agents and still requires passing state data between your environment and the sandbox.\n-## Sandbox setup for secure code execution\n+2. **Running the entire agentic system in a sandbox**: This approach (right side of diagram) runs the entire agentic system, including the agent, model, and tools, within a sandbox environment. This provides better isolation but requires more manual setup and may require passing sensitive credentials (like API keys) to the sandbox environment.\n-When working with AI agents that execute code, security is paramount. This guide describes how to set up and use secure sandboxes for your agent applications using either E2B cloud sandboxes or local Docker containers.\n+This guide describes how to set up and use both types of sandbox approaches for your agent applications.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ef0e0977-ad3d-4683-b293-6a33efd4b3ae", "embedding": null, "metadata": {"issue_id": 919, "pr_number": 983, "type": "patch", "files": ["docs/source/en/tutorials/secure_code_execution.mdx"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8671b7dc-a77c-4861-977f-0c096f141cd4", "node_type": "4", "metadata": {"issue_id": 919, "pr_number": 983, "type": "patch", "files": ["docs/source/en/tutorials/secure_code_execution.mdx"]}, "hash": "75ef8d640b9990c1603b2d3306b05219e8ea77b38e10ae70a6611108b8991b8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "610828e0-38af-4639-a4a0-22ac1b3f02dd", "node_type": "1", "metadata": {"issue_id": 919, "pr_number": 983, "type": "patch", "files": ["docs/source/en/tutorials/secure_code_execution.mdx"]}, "hash": "5775eecb6c59c0e4f66678084bd3e1b2f209026ef4444efa2646ca7a16e21ac0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It's simpler to set up using `executor_type=\"e2b\"` or `executor_type=\"docker\"`, but it doesn't support multi-agents and still requires passing state data between your environment and the sandbox.\n-## Sandbox setup for secure code execution\n+2. **Running the entire agentic system in a sandbox**: This approach (right side of diagram) runs the entire agentic system, including the agent, model, and tools, within a sandbox environment. This provides better isolation but requires more manual setup and may require passing sensitive credentials (like API keys) to the sandbox environment.\n-When working with AI agents that execute code, security is paramount. This guide describes how to set up and use secure sandboxes for your agent applications using either E2B cloud sandboxes or local Docker containers.\n+This guide describes how to set up and use both types of sandbox approaches for your agent applications.\n ### E2B setup\n\n--- docs/source/en/tutorials/secure_code_execution.mdx ---\n@@ -414,4 +399,30 @@ These key practices apply to both E2B and Docker sandboxes:\n - Cleanup\n   - Always ensure proper cleanup of resources, especially for Docker containers, to avoid having dangling containers eating up resources.\n-\u2728 By following these practices and implementing proper cleanup procedures, you can ensure your agent runs safely and efficiently in a sandboxed environment.\n+\u2728 By following these practices and implementing proper cleanup procedures, you can ensure your agent runs safely and efficiently in a sandboxed environment.\n+\n+## Comparing security approaches\n+\n+As illustrated in the diagram earlier, both sandboxing approaches have different security implications:\n+\n+### Approach 1: Running just the code snippets in a sandbox\n+- **Pros**: \n+  - Easier to set up with a simple parameter (`executor_type=\"e2b\"` or `executor_type=\"docker\"`)\n+  - No need to transfer API keys to the sandbox\n+  - Better protection for your local environment\n+- **Cons**:\n+  - Doesn't support multi-agents (managed agents)\n+  - Still requires transferring state between your environment and the sandbox\n+  - Limited to specific code execution\n+\n+### Approach 2: Running the entire agentic system in a sandbox\n+- **Pros**:\n+  - Supports multi-agents\n+  - Complete isolation of the entire agent system\n+  - More flexible for complex agent architectures\n+- **Cons**:\n+  - Requires more manual setup\n+  - May require transferring sensitive API keys to the sandbox\n+  - Potentially higher latency due to more complex operations\n+\n+Choose the approach that best balances your security needs with your application's requirements. For most applications with simpler agent architectures, Approach 1 provides a good balance of security and ease of use. For more complex multi-agent systems where you need full isolation, Approach 2, while more involved to set up, offers better security guarantees.", "mimetype": "text/plain", "start_char_idx": 3430, "end_char_idx": 6311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "720b1911-8860-4f03-8045-786a3e16125a", "embedding": null, "metadata": {"issue_id": 770, "pr_number": 771, "type": "patch", "files": ["docs/source/en/_toctree.yml"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "822a346c-6e1b-4950-bf98-92d183048dfd", "node_type": "4", "metadata": {"issue_id": 770, "pr_number": 771, "type": "patch", "files": ["docs/source/en/_toctree.yml"]}, "hash": "e885476db73d661bc3da284a90f9e61bd913f8e5fd744c90d7319d0e2c6913c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #770 (PR #771):\nPatch summary for PR (max 8000 chars):\nFiles changed: _toctree.yml\n--- Changes ---\n\n--- docs/source/en/_toctree.yml ---\n@@ -13,7 +13,7 @@\n   - local: tutorials/tools\n     title: \ud83d\udee0\ufe0f Tools - in-depth guide\n   - local: tutorials/secure_code_execution\n-    title: \ud83d\udee1\ufe0f Secure your code execution with E2B\n+    title: \ud83d\udee1\ufe0f Secure code execution\n   - local: tutorials/memory\n     title: \ud83d\udcda Manage your agent's memory\n - title: Conceptual guides", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 465, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30aee0e1-7d6e-4e47-bce6-df96617a5a24", "embedding": null, "metadata": {"issue_id": 635, "pr_number": 636, "type": "patch", "files": ["examples/open_deep_research/README.md"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7cff9922-f483-41eb-bb1f-2f0d728f75fb", "node_type": "4", "metadata": {"issue_id": 635, "pr_number": 636, "type": "patch", "files": ["examples/open_deep_research/README.md"]}, "hash": "80023cc12d04cb718df9710c5470c7b7084bac9701cd0acc608d318d614a1ffb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #635 (PR #636):\nPatch summary for PR (max 8000 chars):\nFiles changed: README.md\n--- Changes ---\n\n--- examples/open_deep_research/README.md ---\n@@ -13,7 +13,7 @@ pip install -r requirements.txt\n And install smolagents dev version\n ```bash\n-pip install -e smolagents[dev]\n+pip install smolagents[dev]\n ```\n Then you're good to go! Run the run.py script, as in:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "df5fa7e7-3084-494d-8b3a-10110caacc16", "embedding": null, "metadata": {"issue_id": 575, "pr_number": 576, "type": "patch", "files": ["src/smolagents/agents.py", "tests/test_agents.py"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b1b38ee-0097-4322-b2ca-fdbbb2698926", "node_type": "4", "metadata": {"issue_id": 575, "pr_number": 576, "type": "patch", "files": ["src/smolagents/agents.py", "tests/test_agents.py"]}, "hash": "1fe5d97848715f511c7388ab36ee353d86fa2a94c04b56f14d2f194e5b1d855e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #575 (PR #576):\nPatch summary for PR (max 8000 chars):\nFiles changed: agents.py, test_agents.py\n--- Changes ---\n\n--- src/smolagents/agents.py ---\n@@ -459,7 +459,20 @@ def planning_step(self, task, is_first_step: bool, step: int) -> None:\n                 \"role\": MessageRole.SYSTEM,\n                 \"content\": [{\"type\": \"text\", \"text\": self.prompt_templates[\"planning\"][\"initial_facts\"]}],\n             }\n-            input_messages = [message_prompt_facts]\n+            message_prompt_task = {\n+                \"role\": MessageRole.USER,\n+                \"content\": [\n+                    {\n+                        \"type\": \"text\",\n+                        \"text\": f\"\"\"Here is the task:\n+```\n+{task}\n+```\n+Now begin!\"\"\",\n+                    }\n+                ],\n+            }\n+            input_messages = [message_prompt_facts, message_prompt_task]\n             chat_message_facts: ChatMessage = self.model(input_messages)\n             answer_facts = chat_message_facts.content\n\n--- tests/test_agents.py ---\n@@ -703,12 +703,14 @@ def test_planning_step_first_step(self):\n         assert isinstance(planning_step, PlanningStep)\n         messages = planning_step.model_input_messages\n         assert isinstance(messages, list)\n-        assert len(messages) == 1\n-        for message in messages:\n+        assert len(messages) == 2\n+        expected_roles = [MessageRole.SYSTEM, MessageRole.USER]\n+        for i, message in enumerate(messages):\n             assert isinstance(message, dict)\n             assert \"role\" in message\n             assert \"content\" in message\n             assert isinstance(message[\"role\"], MessageRole)\n+            assert message[\"role\"] == expected_roles[i]\n             assert isinstance(message[\"content\"], list)\n             assert len(message[\"content\"]) == 1\n             for content in message[\"content\"]:\n\n--- tests/test_agents.py ---\n@@ -721,7 +723,7 @@ def test_planning_step_first_step(self):\n             assert len(call_args.args) == 1\n             messages = call_args.args[0]\n             assert isinstance(messages, list)\n-            assert len(messages) == 1\n+            # assert len(messages) == 1  # TODO\n             for message in messages:\n                 assert isinstance(message, dict)\n                 assert \"role\" in message", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "29230bfd-0eaf-4f06-a627-2469c5456a33", "embedding": null, "metadata": {"issue_id": 34, "pr_number": 488, "type": "patch", "files": ["pyproject.toml"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8881ebb-f85a-4ffa-ba56-47d73e14431c", "node_type": "4", "metadata": {"issue_id": 34, "pr_number": 488, "type": "patch", "files": ["pyproject.toml"]}, "hash": "853687c1b3f61c3ee9cd4b4a56968e4505eb134cd9e5ed649fefb7eb8835cb42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #34 (PR #488):\nPatch summary for PR (max 8000 chars):\nFiles changed: pyproject.toml\n--- Changes ---\n\n--- pyproject.toml ---\n@@ -45,7 +45,7 @@ gradio = [\n   \"gradio>=5.13.2\",\n ]\n litellm = [\n-  \"litellm>=1.55.10\",\n+  \"litellm>=1.60.2\",\n ]\n mcp = [\n   \"mcpadapt>=0.0.6\",", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d35c483-a16a-4971-a8a9-2475496372f1", "embedding": null, "metadata": {"issue_id": 34, "pr_number": 488, "type": "patch", "files": ["pyproject.toml"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8818974c-ea7a-4023-a2c3-ff8548ab899b", "node_type": "4", "metadata": {"issue_id": 34, "pr_number": 488, "type": "patch", "files": ["pyproject.toml"]}, "hash": "853687c1b3f61c3ee9cd4b4a56968e4505eb134cd9e5ed649fefb7eb8835cb42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #34 (PR #488):\nPatch summary for PR (max 8000 chars):\nFiles changed: pyproject.toml\n--- Changes ---\n\n--- pyproject.toml ---\n@@ -45,7 +45,7 @@ gradio = [\n   \"gradio>=5.13.2\",\n ]\n litellm = [\n-  \"litellm>=1.55.10\",\n+  \"litellm>=1.60.2\",\n ]\n mcp = [\n   \"mcpadapt>=0.0.6\",", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "adbd82de-532f-4f09-b966-e1c946279b18", "embedding": null, "metadata": {"issue_id": 34, "pr_number": 488, "type": "patch", "files": ["pyproject.toml"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "00d72211-87d5-473f-b1be-6316dfb48a47", "node_type": "4", "metadata": {"issue_id": 34, "pr_number": 488, "type": "patch", "files": ["pyproject.toml"]}, "hash": "853687c1b3f61c3ee9cd4b4a56968e4505eb134cd9e5ed649fefb7eb8835cb42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #34 (PR #488):\nPatch summary for PR (max 8000 chars):\nFiles changed: pyproject.toml\n--- Changes ---\n\n--- pyproject.toml ---\n@@ -45,7 +45,7 @@ gradio = [\n   \"gradio>=5.13.2\",\n ]\n litellm = [\n-  \"litellm>=1.55.10\",\n+  \"litellm>=1.60.2\",\n ]\n mcp = [\n   \"mcpadapt>=0.0.6\",", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 284, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6528efe8-6f31-463c-8092-e20c6e3b9ab5", "embedding": null, "metadata": {"issue_id": 258, "pr_number": 259, "type": "patch", "files": ["src/smolagents/agents.py"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a579022a-5291-4d5a-90aa-755404cee725", "node_type": "4", "metadata": {"issue_id": 258, "pr_number": 259, "type": "patch", "files": ["src/smolagents/agents.py"]}, "hash": "5154750535d5dfe5853f7d033c58f9a1ccf54e1cc9dac05dbb5610b9e34cd84d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #258 (PR #259):\nPatch summary for PR (max 8000 chars):\nFiles changed: agents.py\n--- Changes ---\n\n--- src/smolagents/agents.py ---\n@@ -907,13 +907,6 @@ def __init__(\n             raise AgentError(\n                 \"Tag '{{authorized_imports}}' should be provided in the prompt.\"\n             )\n-\n-        if \"*\" in self.additional_authorized_imports:\n-            self.logger.log(\n-                \"Caution: you set an authorization for all imports, meaning your agent can decide to import any package it deems necessary. This might raise issues if the package is not installed in your environment.\",\n-                0,\n-            )\n-\n         super().__init__(\n             tools=tools,\n             model=model,\n\n--- src/smolagents/agents.py ---\n@@ -922,6 +915,12 @@ def __init__(\n             planning_interval=planning_interval,\n             **kwargs,\n         )\n+        if \"*\" in self.additional_authorized_imports:\n+            self.logger.log(\n+                \"Caution: you set an authorization for all imports, meaning your agent can decide to import any package it deems necessary. This might raise issues if the package is not installed in your environment.\",\n+                0,\n+            )\n+\n         if use_e2b_executor and len(self.managed_agents) > 0:\n             raise Exception(\n                 f\"You passed both {use_e2b_executor=} and some managed agents. Managed agents is not yet supported with remote code execution.\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1464, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f12e057-bf7e-42b4-a0db-6c6d66fb609d", "embedding": null, "metadata": {"issue_id": 117, "pr_number": 139, "type": "patch", "files": ["src/smolagents/models.py"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0481093-1235-48e9-a62e-a38c71e516fc", "node_type": "4", "metadata": {"issue_id": 117, "pr_number": 139, "type": "patch", "files": ["src/smolagents/models.py"]}, "hash": "d624f179142125c77ae0a022a7ebdf669b645abd007aa5778816cfe707284af1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #117 (PR #139):\nPatch summary for PR (max 8000 chars):\nFiles changed: models.py\n--- Changes ---\n\n--- src/smolagents/models.py ---\n@@ -318,15 +318,13 @@ def __init__(self, model_id: Optional[str] = None, device: Optional[str] = None)\n         logger.info(f\"Using device: {self.device}\")\n         try:\n             self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n-            self.model = AutoModelForCausalLM.from_pretrained(model_id).to(self.device)\n+            self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=self.device)\n         except Exception as e:\n             logger.warning(\n                 f\"Failed to load tokenizer and model for {model_id=}: {e}. Loading default tokenizer and model instead from {model_id=}.\"\n             )\n             self.tokenizer = AutoTokenizer.from_pretrained(default_model_id)\n-            self.model = AutoModelForCausalLM.from_pretrained(default_model_id).to(\n-                self.device\n-            )\n+            self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=self.device)\n     def make_stopping_criteria(self, stop_sequences: List[str]) -> StoppingCriteriaList:\n         class StopOnStrings(StoppingCriteria):", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1228, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40cf2d10-e60a-4883-9c34-b70324ce301d", "embedding": null, "metadata": {"issue_id": 69, "pr_number": 75, "type": "patch", "files": ["src/smolagents/tools.py"]}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a836c471-8805-4261-be36-be769ab37f40", "node_type": "4", "metadata": {"issue_id": 69, "pr_number": 75, "type": "patch", "files": ["src/smolagents/tools.py"]}, "hash": "b66561091d41863f81ed5e5a772ef0afa18874855ed06af4a3d5368ca2fe8c01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Patch for Issue #69 (PR #75):\nPatch summary for PR (max 8000 chars):\nFiles changed: tools.py\n--- Changes ---\n\n--- src/smolagents/tools.py ---\n@@ -206,10 +206,10 @@ def validate_arguments(self):\n         assert getattr(self, \"output_type\", None) in AUTHORIZED_TYPES\n-        # Validate forward function signature, except for PipelineTool\n+        # Validate forward function signature, except for Tools that use a \"generic\" signature (PipelineTool, SpaceToolWrapper)\n         if not (\n-            hasattr(self, \"is_pipeline_tool\")\n-            and getattr(self, \"is_pipeline_tool\") is True\n+            hasattr(self, \"skip_forward_signature_validation\")\n+            and getattr(self, \"skip_forward_signature_validation\") is True\n         ):\n             signature = inspect.signature(self.forward)\n\n--- src/smolagents/tools.py ---\n@@ -575,6 +575,9 @@ def from_space(\n         from gradio_client import Client, handle_file\n         class SpaceToolWrapper(Tool):\n+            \n+            skip_forward_signature_validation = True\n+\n             def __init__(\n                 self,\n                 space_id: str,\n\n--- src/smolagents/tools.py ---\n@@ -1098,7 +1101,7 @@ class PipelineTool(Tool):\n     name = \"pipeline\"\n     inputs = {\"prompt\": str}\n     output_type = str\n-    is_pipeline_tool = True\n+    skip_forward_signature_validation = True\n     def __init__(\n         self,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1379, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]